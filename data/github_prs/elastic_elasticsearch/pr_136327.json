{
  "id": 136327,
  "repository": "elastic/elasticsearch",
  "title": "ESQL: Enable new data types with created version",
  "body": "Enables `dense_vector` and `aggregate_metric_double` if all nodes in all of the target clusters support it.\r\n\r\nRelates to #135193\r\n",
  "state": "closed",
  "merged": true,
  "merged_at": "2025-10-23T20:23:10+00:00",
  "created_at": "2025-10-09T20:45:52+00:00",
  "updated_at": "2025-10-23T22:15:22+00:00",
  "author": "nik9000",
  "reviewers": [
    "alex-spies",
    "nik9000"
  ],
  "base_sha": "440faee03ee04b4ab8dbf2e41afcc3cc6f0a26c0",
  "head_sha": "b86a0963774cbc1ccdf27e4fd19e5e312d736e33",
  "review_comments": [
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T22:57:45+00:00"
    },
    {
      "user": "alex-spies",
      "state": "APPROVED",
      "body": "Nice, thanks @nik9000 ! I have only minor comments, and a suggestion for a follow-up (more tests!) that we can tackle separately.",
      "submitted_at": "2025-10-22T17:52:22+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-22T18:00:34+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-22T18:05:06+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-22T18:05:42+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-22T18:06:58+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T15:13:12+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T15:14:44+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T15:29:59+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T15:30:07+00:00"
    },
    {
      "user": "alex-spies",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T15:32:34+00:00"
    },
    {
      "user": "alex-spies",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T15:35:47+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T17:04:31+00:00"
    }
  ],
  "pr_comments": [
    {
      "user": "github-actions[bot]",
      "body": "## ‚ÑπÔ∏è Important: Docs version tagging\n\nüëã Thanks for updating the docs! Just a friendly reminder that our docs are now **cumulative**. This means all 9.x versions are documented on the same page and published off of the main branch, instead of creating separate pages for each minor version.\n\nWe use [applies_to tags](https://elastic.github.io/docs-builder/syntax/applies) to mark version-specific features and changes.\n\n<details>\n<summary>Expand for a quick overview</summary>\n\n### When to use applies_to tags:\n‚úÖ At the page level to indicate which products/deployments the content applies to (mandatory)\n‚úÖ When features change state (e.g. preview, ga) in a specific version\n‚úÖ When availability differs across deployments and environments\n\n### What NOT to do:\n‚ùå Don't remove or replace information that applies to an older version\n‚ùå Don't add new information that applies to a specific version without an applies_to tag\n‚ùå Don't forget that applies_to tags can be used at the page, section, and inline level\n</details>\n\n### ü§î Need help?\n- Check out the [cumulative docs guidelines](https://elastic.github.io/docs-builder/contribute/cumulative-docs/)\n- Reach out in the [#docs](https://elastic.slack.com/archives/C0JF80CJZ) Slack channel",
      "created_at": "2025-10-09T20:47:28+00:00"
    },
    {
      "user": "elasticsearchmachine",
      "body": "Pinging @elastic/es-analytical-engine (Team:Analytics)",
      "created_at": "2025-10-21T22:58:14+00:00"
    },
    {
      "user": "elasticsearchmachine",
      "body": "Hi @nik9000, I've created a changelog YAML for you.",
      "created_at": "2025-10-22T12:57:44+00:00"
    },
    {
      "user": "nik9000",
      "body": "Follow ups we need:\r\n* [ ] Let's add a test case for using all kinds of types as lookup/enrich fields, right? (Not match fields, we have separate tests for that.) \r\n* [ ] We could use a similar test for aggregate metric double, no? https://github.com/elastic/elasticsearch/pull/137070\r\n* [ ] Integrate with https://github.com/elastic/elasticsearch/pull/136887\r\n* [ ] Create a pragma to enable under construction types (sounds kind of difficult, but probably worth it)\r\n",
      "created_at": "2025-10-23T15:12:20+00:00"
    },
    {
      "user": "elasticsearchmachine",
      "body": "## üíî Backport failed\n| Status | Branch | Result |\n|:------:|:------:|:------:|\n| ‚ùå |  9.2  | Commit could not be cherrypicked due to conflicts |\n\nYou can use [sqren/backport](https://github.com/sqren/backport) to manually backport by running `backport --upstream elastic/elasticsearch --pr 136327`",
      "created_at": "2025-10-23T20:24:27+00:00"
    },
    {
      "user": "nik9000",
      "body": "Backport: https://github.com/elastic/elasticsearch/pull/137073",
      "created_at": "2025-10-23T22:15:22+00:00"
    }
  ],
  "files_changed": [
    {
      "filename": "docs/changelog/136327.yaml",
      "status": "added",
      "additions": 5,
      "deletions": 0,
      "changes": 5,
      "patch": "@@ -0,0 +1,5 @@\n+pr: 136327\n+summary: Enable new data types with created version\n+area: ES|QL\n+type: enhancement\n+issues: []"
    },
    {
      "filename": "x-pack/plugin/build.gradle",
      "status": "modified",
      "additions": 0,
      "deletions": 13,
      "changes": 13,
      "patch": "@@ -143,19 +143,6 @@ tasks.named(\"yamlRestCompatTestTransform\").configure({ task ->\n   task.skipTest(\"ml/sparse_vector_search/Search on a sparse_vector field with dots in the field names\", \"Vectors are no longer returned by default\")\n   task.skipTest(\"ml/sparse_vector_search/Search on a nested sparse_vector field with dots in the field names and conflicting child fields\", \"Vectors are no longer returned by default\")\n   task.skipTest(\"esql/190_lookup_join/lookup-no-key-only-key\", \"Requires the fix\")\n-  task.skipTest(\"esql/40_tsdb/aggregate_metric_double unsortable\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/avg of aggregate_metric_double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/grouping stats on aggregate_metric_double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/render aggregate_metric_double when missing min and max\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/render aggregate_metric_double when missing value\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/sorting with aggregate_metric_double with partial submetrics\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/stats on aggregate_metric_double missing min and max\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/to_string aggregate_metric_double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/stats on aggregate_metric_double with partial submetrics\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/MV_EXPAND on non-MV aggregate metric double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/Query stats on downsampled index\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/Render stats from downsampled index\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/Sort from multiple indices one with aggregate metric double\", \"Extra function required to enable the field type\")\n })\n \n tasks.named('yamlRestCompatTest').configure {"
    },
    {
      "filename": "x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/DataType.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "patch": "@@ -7,6 +7,7 @@\n package org.elasticsearch.xpack.esql.core.type;\n \n import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.Build;\n import org.elasticsearch.TransportVersion;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n@@ -748,7 +749,7 @@ public DataType counter() {\n \n     @Override\n     public void writeTo(StreamOutput out) throws IOException {\n-        if (supportedVersion.supportedOn(out.getTransportVersion()) == false) {\n+        if (supportedVersion.supportedOn(out.getTransportVersion(), Build.current().isSnapshot()) == false) {\n             /*\n              * TODO when we implement version aware planning flip this to an IllegalStateException\n              * so we throw a 500 error. It'll be our bug then. Right now it's a sign that the user"
    },
    {
      "filename": "x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/SupportedVersion.java",
      "status": "modified",
      "additions": 7,
      "deletions": 7,
      "changes": 14,
      "patch": "@@ -11,15 +11,15 @@\n import org.elasticsearch.TransportVersion;\n \n public interface SupportedVersion {\n-    boolean supportedOn(TransportVersion version);\n+    boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot);\n \n     default boolean supportedLocally() {\n-        return supportedOn(TransportVersion.current());\n+        return supportedOn(TransportVersion.current(), Build.current().isSnapshot());\n     }\n \n     SupportedVersion SUPPORTED_ON_ALL_NODES = new SupportedVersion() {\n         @Override\n-        public boolean supportedOn(TransportVersion version) {\n+        public boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot) {\n             return true;\n         }\n \n@@ -56,8 +56,8 @@ public String toString() {\n     // Check usage of this constant to be sure.\n     SupportedVersion UNDER_CONSTRUCTION = new SupportedVersion() {\n         @Override\n-        public boolean supportedOn(TransportVersion version) {\n-            return Build.current().isSnapshot();\n+        public boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot) {\n+            return currentBuildIsSnapshot;\n         }\n \n         @Override\n@@ -76,8 +76,8 @@ public String toString() {\n     static SupportedVersion supportedSince(TransportVersion supportedVersion) {\n         return new SupportedVersion() {\n             @Override\n-            public boolean supportedOn(TransportVersion version) {\n-                return version.supports(supportedVersion) || Build.current().isSnapshot();\n+            public boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot) {\n+                return version.supports(supportedVersion) || currentBuildIsSnapshot;\n             }\n \n             @Override"
    },
    {
      "filename": "x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java",
      "status": "modified",
      "additions": 89,
      "deletions": 25,
      "changes": 114,
      "patch": "@@ -32,6 +32,7 @@\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Comparator;\n import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n@@ -43,6 +44,7 @@\n import static org.elasticsearch.test.ListMatcher.matchesList;\n import static org.elasticsearch.test.MapMatcher.assertMap;\n import static org.elasticsearch.test.MapMatcher.matchesMap;\n+import static org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse.RESOLVE_FIELDS_RESPONSE_CREATED_TV;\n import static org.hamcrest.Matchers.any;\n import static org.hamcrest.Matchers.anyOf;\n import static org.hamcrest.Matchers.containsString;\n@@ -76,11 +78,6 @@ public class AllSupportedFieldsTestCase extends ESRestTestCase {\n \n     @ParametersFactory(argumentFormatting = \"pref=%s mode=%s\")\n     public static List<Object[]> args() {\n-        if (Build.current().isSnapshot()) {\n-            // We only test behavior in release builds. Snapshot builds will have data types enabled that are still under construction.\n-            return List.of();\n-        }\n-\n         List<Object[]> args = new ArrayList<>();\n         for (MappedFieldType.FieldExtractPreference extractPreference : Arrays.asList(\n             null,\n@@ -102,7 +99,7 @@ protected AllSupportedFieldsTestCase(MappedFieldType.FieldExtractPreference extr\n         this.indexMode = indexMode;\n     }\n \n-    protected record NodeInfo(String cluster, String id, TransportVersion version, Set<String> roles) {}\n+    protected record NodeInfo(String cluster, String id, boolean snapshot, TransportVersion version, Set<String> roles) {}\n \n     private static Map<String, NodeInfo> nodeToInfo;\n \n@@ -126,6 +123,19 @@ protected boolean fetchDenseVectorAggMetricDoubleIfFns() throws IOException {\n         return clusterHasCapability(\"GET\", \"/_query\", List.of(), List.of(\"DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_FNS\")).orElse(false);\n     }\n \n+    private static Boolean denseVectorAggMetricDoubleIfVersion;\n+\n+    private boolean denseVectorAggMetricDoubleIfVersion() throws IOException {\n+        if (denseVectorAggMetricDoubleIfVersion == null) {\n+            denseVectorAggMetricDoubleIfVersion = fetchDenseVectorAggMetricDoubleIfVersion();\n+        }\n+        return denseVectorAggMetricDoubleIfVersion;\n+    }\n+\n+    protected boolean fetchDenseVectorAggMetricDoubleIfVersion() throws IOException {\n+        return clusterHasCapability(\"GET\", \"/_query\", List.of(), List.of(\"DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_VERSION\")).orElse(false);\n+    }\n+\n     private static Boolean supportsNodeAssignment;\n \n     protected boolean supportsNodeAssignment() throws IOException {\n@@ -153,11 +163,21 @@ protected static Map<String, NodeInfo> fetchNodeToInfo(RestClient client, String\n             String id = (String) n.getKey();\n             Map<?, ?> nodeInfo = (Map<?, ?>) n.getValue();\n             String nodeName = (String) extractValue(nodeInfo, \"name\");\n+\n+            /*\n+             * Figuring out is a node is a snapshot is kind of tricky. The main version\n+             * doesn't include -SNAPSHOT. But ${VERSION}-SNAPSHOT is in the node info\n+             * *somewhere*. So we do this silly toString here.\n+             */\n+            String version = (String) extractValue(nodeInfo, \"version\");\n+            boolean snapshot = nodeInfo.toString().contains(version + \"-SNAPSHOT\");\n+\n             TransportVersion transportVersion = TransportVersion.fromId((Integer) extractValue(nodeInfo, \"transport_version\"));\n             List<?> roles = (List<?>) nodeInfo.get(\"roles\");\n+\n             nodeToInfo.put(\n                 nodeName,\n-                new NodeInfo(cluster, id, transportVersion, roles.stream().map(Object::toString).collect(Collectors.toSet()))\n+                new NodeInfo(cluster, id, snapshot, transportVersion, roles.stream().map(Object::toString).collect(Collectors.toSet()))\n             );\n         }\n \n@@ -175,6 +195,22 @@ public void createIndices() throws IOException {\n         }\n     }\n \n+    /**\n+     * Make sure the test doesn't run on snapshot builds. Release builds only.\n+     * <p>\n+     *     {@link Build#isSnapshot()} checks if the version under test is a snapshot.\n+     *     But! This run test runs against many versions and if *any* are snapshots\n+     *     then this will fail. So we check the versions of each node in the cluster too.\n+     * </p>\n+     */\n+    @Before\n+    public void skipSnapshots() throws IOException {\n+        assumeFalse(\"Only supported on production builds\", Build.current().isSnapshot());\n+        for (NodeInfo n : allNodeToInfo().values()) {\n+            assumeFalse(\"Only supported on production builds\", n.snapshot());\n+        }\n+    }\n+\n     // TODO: Also add a test for _tsid once we can determine the minimum transport version of all nodes.\n     public final void testFetchAll() throws IOException {\n         Map<String, Object> response = esql(\"\"\"\n@@ -212,7 +248,7 @@ public final void testFetchAll() throws IOException {\n                 if (supportedInIndex(type) == false) {\n                     continue;\n                 }\n-                expectedValues = expectedValues.entry(fieldName(type), expectedValue(type));\n+                expectedValues = expectedValues.entry(fieldName(type), expectedValue(type, nodeInfo));\n             }\n             expectedValues = expectedValues.entry(\"_id\", any(String.class))\n                 .entry(\"_ignored\", nullValue())\n@@ -227,15 +263,23 @@ public final void testFetchAll() throws IOException {\n         profileLogger.clearProfile();\n     }\n \n-    // Tests a workaround and will become obsolete once we can determine the actual minimum transport version of all nodes.\n+    /**\n+     * Tests fetching {@code dense_vector} if possible. Uses the {@code dense_vector_agg_metric_double_if_fns}\n+     * work around if required.\n+     */\n     public final void testFetchDenseVector() throws IOException {\n         Map<String, Object> response;\n         try {\n-            response = esql(\"\"\"\n-                | EVAL k = v_l2_norm(f_dense_vector, [1])  // workaround to enable fetching dense_vector\n+            String request = \"\"\"\n                 | KEEP _index, f_dense_vector\n                 | LIMIT 1000\n-                \"\"\");\n+                \"\"\";\n+            if (denseVectorAggMetricDoubleIfVersion() == false) {\n+                request = \"\"\"\n+                    | EVAL k = v_l2_norm(f_dense_vector, [1])  // workaround to enable fetching dense_vector\n+                    \"\"\" + request;\n+            }\n+            response = esql(request);\n             if ((Boolean) response.get(\"is_partial\")) {\n                 Map<?, ?> clusters = (Map<?, ?>) response.get(\"_clusters\");\n                 Map<?, ?> details = (Map<?, ?>) clusters.get(\"details\");\n@@ -410,7 +454,7 @@ private void createAllTypesDoc(RestClient client, String indexName) throws IOExc\n     }\n \n     // This will become dependent on the minimum transport version of all nodes once we can determine that.\n-    private Matcher<?> expectedValue(DataType type) {\n+    private Matcher<?> expectedValue(DataType type, NodeInfo nodeInfo) throws IOException {\n         return switch (type) {\n             case BOOLEAN -> equalTo(true);\n             case COUNTER_LONG, LONG, COUNTER_INTEGER, INTEGER, UNSIGNED_LONG, SHORT, BYTE -> equalTo(1);\n@@ -429,14 +473,24 @@ private Matcher<?> expectedValue(DataType type) {\n             case GEO_SHAPE -> equalTo(\"POINT (-71.34 41.12)\");\n             case NULL -> nullValue();\n             case AGGREGATE_METRIC_DOUBLE -> {\n-                // Currently, we cannot tell if all nodes support it or not so we treat it as unsupported.\n-                // TODO: Fix this once we know the node versions.\n-                yield nullValue();\n+                /*\n+                 * We need both AGGREGATE_METRIC_DOUBLE_CREATED and RESOLVE_FIELDS_RESPONSE_CREATED_TV\n+                 * but RESOLVE_FIELDS_RESPONSE_CREATED_TV came last so it's enough to check just it.\n+                 */\n+                if (minVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) == false) {\n+                    yield nullValue();\n+                }\n+                yield equalTo(\"{\\\"min\\\":-302.5,\\\"max\\\":702.3,\\\"sum\\\":200.0,\\\"value_count\\\":25}\");\n             }\n             case DENSE_VECTOR -> {\n-                // Currently, we cannot tell if all nodes support it or not so we treat it as unsupported.\n-                // TODO: Fix this once we know the node versions.\n-                yield nullValue();\n+                /*\n+                 * We need both DENSE_VECTOR_CREATED and RESOLVE_FIELDS_RESPONSE_CREATED_TV\n+                 * but RESOLVE_FIELDS_RESPONSE_CREATED_TV came last so it's enough to check just it.\n+                 */\n+                if (minVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) == false) {\n+                    yield nullValue();\n+                }\n+                yield equalTo(List.of(0.5, 10.0, 5.9999995));\n             }\n             default -> throw new AssertionError(\"unsupported field type [\" + type + \"]\");\n         };\n@@ -507,7 +561,7 @@ private Map<String, Object> nameToValue(List<String> names, List<?> values) {\n     }\n \n     // This will become dependent on the minimum transport version of all nodes once we can determine that.\n-    private Matcher<String> expectedType(DataType type) {\n+    private Matcher<String> expectedType(DataType type) throws IOException {\n         return switch (type) {\n             case COUNTER_DOUBLE, COUNTER_LONG, COUNTER_INTEGER -> {\n                 if (indexMode == IndexMode.TIME_SERIES) {\n@@ -518,10 +572,16 @@ private Matcher<String> expectedType(DataType type) {\n             case BYTE, SHORT -> equalTo(\"integer\");\n             case HALF_FLOAT, SCALED_FLOAT, FLOAT -> equalTo(\"double\");\n             case NULL -> equalTo(\"keyword\");\n-            // Currently unsupported without TS command or KNN function\n-            case AGGREGATE_METRIC_DOUBLE, DENSE_VECTOR ->\n-                // TODO: Fix this once we know the node versions.\n-                equalTo(\"unsupported\");\n+            case AGGREGATE_METRIC_DOUBLE, DENSE_VECTOR -> {\n+                /*\n+                 * We need both <type_name>_CREATED and RESOLVE_FIELDS_RESPONSE_CREATED_TV\n+                 * but RESOLVE_FIELDS_RESPONSE_CREATED_TV came last so it's enough to check just it.\n+                 */\n+                if (minVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) == false) {\n+                    yield equalTo(\"unsupported\");\n+                }\n+                yield equalTo(type.esType());\n+            }\n             default -> equalTo(type.esType());\n         };\n     }\n@@ -555,9 +615,13 @@ private Map<String, NodeInfo> expectedIndices() throws IOException {\n                     name = e.getValue().cluster + \":\" + name;\n                 }\n                 // We should only end up with one per cluster\n-                result.put(name, new NodeInfo(e.getValue().cluster, null, e.getValue().version(), null));\n+                result.put(name, new NodeInfo(e.getValue().cluster, null, e.getValue().snapshot(), e.getValue().version(), null));\n             }\n         }\n         return result;\n     }\n+\n+    protected TransportVersion minVersion() throws IOException {\n+        return allNodeToInfo().values().stream().map(NodeInfo::version).min(Comparator.naturalOrder()).get();\n+    }\n }"
    },
    {
      "filename": "x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-bit.csv-spec",
      "status": "modified",
      "additions": 5,
      "deletions": 7,
      "changes": 12,
      "patch": "@@ -1,9 +1,8 @@\n retrieveBitVectorData\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL k = v_l2_norm(bit_vector, [1,2])  // workaround to enable fetching dense_vector\n | KEEP id, bit_vector\n | SORT id\n ;\n@@ -16,11 +15,11 @@ id:l | bit_vector:dense_vector\n ;\n \n denseBitVectorWithEval\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL v = bit_vector, k = v_l2_norm(bit_vector, [1,2])  // workaround to enable fetching dense_vector\n+| EVAL v = bit_vector\n | KEEP id, v\n | SORT id\n ;\n@@ -33,14 +32,13 @@ id:l | v:dense_vector\n ;\n \n denseBitVectorWithRenameAndDrop\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n | EVAL v = bit_vector\n-| EVAL k = v_l2_norm(bit_vector, [1,2])  // workaround to enable fetching dense_vector\n | RENAME v AS new_vector\n-| DROP float_vector, byte_vector, bit_vector, k\n+| DROP float_vector, byte_vector, bit_vector\n | SORT id\n ;\n "
    },
    {
      "filename": "x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-byte.csv-spec",
      "status": "modified",
      "additions": 4,
      "deletions": 7,
      "changes": 11,
      "patch": "@@ -1,9 +1,8 @@\n retrieveByteVectorData\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL k = v_l2_norm(byte_vector, [1,2,3]) // workaround to enable fetching dense_vector\n | KEEP id, byte_vector\n | SORT id\n ;\n@@ -16,12 +15,11 @@ id:l | byte_vector:dense_vector\n ;\n \n denseByteVectorWithEval\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n | EVAL v = byte_vector\n-| EVAL k = v_l2_norm(byte_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | KEEP id, v\n | SORT id\n ;\n@@ -34,14 +32,13 @@ id:l | v:dense_vector\n ;\n \n denseByteVectorWithRenameAndDrop\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector \n | EVAL v = byte_vector\n-| EVAL k = v_l2_norm(byte_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | RENAME v AS new_vector \n-| DROP float_vector, byte_vector, bit_vector, k\n+| DROP float_vector, byte_vector, bit_vector\n | SORT id\n ;\n "
    },
    {
      "filename": "x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector.csv-spec",
      "status": "modified",
      "additions": 4,
      "deletions": 7,
      "changes": 11,
      "patch": "@@ -1,10 +1,9 @@\n retrieveDenseVectorData\n required_capability: dense_vector_field_type_released\n-required_capability: dense_vector_agg_metric_double_if_fns\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL k = v_l2_norm(float_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | KEEP id, float_vector\n | SORT id\n ;\n@@ -17,12 +16,11 @@ id:l | float_vector:dense_vector\n ;\n \n denseVectorWithEval\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n | EVAL v = float_vector\n-| EVAL k = v_l2_norm(float_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | KEEP id, v\n | SORT id\n ;\n@@ -35,14 +33,13 @@ id:l | v:dense_vector\n ;\n \n denseVectorWithRenameAndDrop\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector \n | EVAL v = float_vector \n-| EVAL k = v_l2_norm(float_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | RENAME v AS new_vector \n-| DROP float_vector, byte_vector, bit_vector, k\n+| DROP float_vector, byte_vector, bit_vector\n | SORT id\n ;\n "
    },
    {
      "filename": "x-pack/plugin/esql/qa/testFixtures/src/main/resources/inlinestats.csv-spec",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "patch": "@@ -4125,8 +4125,8 @@ from employees\n inlineStatsOnAggregateMetricDouble\n required_capability: inline_stats\n required_capability: aggregate_metric_double_v0\n+required_capability: dense_vector_agg_metric_double_if_version\n FROM k8s-downsampled\n-| EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)     // Temporary workaround to enable aggregate_metric_double\n | INLINE STATS tx_max = MAX(network.eth0.tx) BY pod\n | SORT @timestamp, cluster, pod\n | KEEP @timestamp, cluster, pod, network.eth0.tx, tx_max"
    },
    {
      "filename": "x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/DenseVectorFieldTypeIT.java",
      "status": "modified",
      "additions": 0,
      "deletions": 4,
      "changes": 4,
      "patch": "@@ -95,8 +95,6 @@ public void testRetrieveFieldType() {\n \n         var query = \"\"\"\n             FROM test\n-            | EVAL k = v_l2_norm(vector, [1])  // workaround to enable fetching dense_vector\n-            | DROP k\n             \"\"\";\n \n         try (var resp = run(query)) {\n@@ -111,7 +109,6 @@ public void testRetrieveTopNDenseVectorFieldData() {\n \n         var query = \"\"\"\n                 FROM test\n-                | EVAL k = v_l2_norm(vector, [1])  // workaround to enable fetching dense_vector\n                 | KEEP id, vector\n                 | SORT id ASC\n             \"\"\";\n@@ -141,7 +138,6 @@ public void testRetrieveDenseVectorFieldData() {\n \n         var query = \"\"\"\n             FROM test\n-            | EVAL k = v_l2_norm(vector, [1])  // workaround to enable fetching dense_vector\n             | KEEP id, vector\n             \"\"\";\n "
    },
    {
      "filename": "x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/plugin/KnnFunctionIT.java",
      "status": "modified",
      "additions": 4,
      "deletions": 6,
      "changes": 10,
      "patch": "@@ -219,12 +219,10 @@ public void testKnnWithLookupJoin() {\n         var error = expectThrows(VerificationException.class, () -> run(query));\n         assertThat(\n             error.getMessage(),\n-            // TODO revert this when we have proper versioned type resolutions\n-            // containsString(\n-            // \"line 3:13: [KNN] function cannot operate on [lookup_vector], supplied by an index [test_lookup] in non-STANDARD \"\n-            // + \"mode [lookup]\"\n-            // )\n-            containsString(\"line 3:13: Cannot use field [lookup_vector] with unsupported type [dense_vector]\")\n+            containsString(\n+                \"line 3:13: [KNN] function cannot operate on [lookup_vector], supplied by an index [test_lookup] in non-STANDARD \"\n+                    + \"mode [lookup]\"\n+            )\n         );\n     }\n "
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlCapabilities.java",
      "status": "modified",
      "additions": 2,
      "deletions": 0,
      "changes": 2,
      "patch": "@@ -1479,6 +1479,8 @@ public enum Cap {\n \n         DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_FNS,\n \n+        DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_VERSION,\n+\n         /**\n          * FUSE L2_NORM score normalization support\n          */"
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "patch": "@@ -17,7 +17,7 @@\n import java.io.IOException;\n \n public class EsqlResolveFieldsResponse extends ActionResponse {\n-    private static final TransportVersion RESOLVE_FIELDS_RESPONSE_CREATED_TV = TransportVersion.fromName(\n+    public static final TransportVersion RESOLVE_FIELDS_RESPONSE_CREATED_TV = TransportVersion.fromName(\n         \"esql_resolve_fields_response_created\"\n     );\n "
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/analysis/PreAnalyzer.java",
      "status": "modified",
      "additions": 21,
      "deletions": 12,
      "changes": 33,
      "patch": "@@ -29,8 +29,8 @@ public record PreAnalysis(\n         Map<IndexPattern, IndexMode> indexes,\n         List<Enrich> enriches,\n         List<IndexPattern> lookupIndices,\n-        boolean supportsAggregateMetricDouble,\n-        boolean supportsDenseVector\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported\n     ) {\n         public static final PreAnalysis EMPTY = new PreAnalysis(Map.of(), List.of(), List.of(), false, false);\n     }\n@@ -64,19 +64,22 @@ protected PreAnalysis doPreAnalyze(LogicalPlan plan) {\n         plan.forEachUp(Enrich.class, unresolvedEnriches::add);\n \n         /*\n-         * Enable aggregate_metric_double and dense_vector when we see certain function\n-         * or the TS command. This allows us to release these when not all nodes understand\n+         * Enable aggregate_metric_double and dense_vector when we see certain functions\n+         * or the TS command. This allowed us to release these when not all nodes understand\n          * these types. These functions are only supported on newer nodes, so we use them\n          * as a signal that the query is only for nodes that support these types.\n          *\n-         * This work around is temporary until we flow the minimum transport version\n-         * back through a cross cluster search field caps call.\n+         * This was a workaround that was required to enable these in 9.2.0. These days\n+         * we enable these field types if all nodes in all clusters support them. But this\n+         * work around persists to support force-enabling them on queries that might touch\n+         * nodes that don't have 9.2.1 or 9.3.0. If all nodes in the cluster have 9.2.1 or 9.3.0\n+         * this code doesn't do anything.\n          */\n-        Holder<Boolean> supportsAggregateMetricDouble = new Holder<>(false);\n-        Holder<Boolean> supportsDenseVector = new Holder<>(false);\n+        Holder<Boolean> useAggregateMetricDoubleWhenNotSupported = new Holder<>(false);\n+        Holder<Boolean> useDenseVectorWhenNotSupported = new Holder<>(false);\n         indexes.forEach((ip, mode) -> {\n             if (mode == IndexMode.TIME_SERIES) {\n-                supportsAggregateMetricDouble.set(true);\n+                useAggregateMetricDoubleWhenNotSupported.set(true);\n             }\n         });\n         plan.forEachDown(p -> p.forEachExpression(UnresolvedFunction.class, fn -> {\n@@ -88,16 +91,22 @@ protected PreAnalysis doPreAnalyze(LogicalPlan plan) {\n                 || fn.name().equalsIgnoreCase(\"v_l2_norm\")\n                 || fn.name().equalsIgnoreCase(\"v_dot_product\")\n                 || fn.name().equalsIgnoreCase(\"v_magnitude\")) {\n-                supportsDenseVector.set(true);\n+                useDenseVectorWhenNotSupported.set(true);\n             }\n             if (fn.name().equalsIgnoreCase(\"to_aggregate_metric_double\")) {\n-                supportsAggregateMetricDouble.set(true);\n+                useAggregateMetricDoubleWhenNotSupported.set(true);\n             }\n         }));\n \n         // mark plan as preAnalyzed (if it were marked, there would be no analysis)\n         plan.forEachUp(LogicalPlan::setPreAnalyzed);\n \n-        return new PreAnalysis(indexes, unresolvedEnriches, lookupIndices, supportsAggregateMetricDouble.get(), supportsDenseVector.get());\n+        return new PreAnalysis(\n+            indexes,\n+            unresolvedEnriches,\n+            lookupIndices,\n+            useAggregateMetricDoubleWhenNotSupported.get(),\n+            useDenseVectorWhenNotSupported.get()\n+        );\n     }\n }"
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "patch": "@@ -856,8 +856,8 @@ private void preAnalyzeMainIndices(\n                     default -> requestFilter;\n                 },\n                 indexMode == IndexMode.TIME_SERIES,\n-                preAnalysis.supportsAggregateMetricDouble(),\n-                preAnalysis.supportsDenseVector(),\n+                preAnalysis.useAggregateMetricDoubleWhenNotSupported(),\n+                preAnalysis.useDenseVectorWhenNotSupported(),\n                 listener.delegateFailureAndWrap((l, indexResolution) -> {\n                     EsqlCCSUtils.updateExecutionInfoWithUnavailableClusters(executionInfo, indexResolution.inner().failures());\n                     l.onResponse("
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java",
      "status": "modified",
      "additions": 82,
      "deletions": 24,
      "changes": 106,
      "patch": "@@ -6,6 +6,7 @@\n  */\n package org.elasticsearch.xpack.esql.session;\n \n+import org.elasticsearch.Build;\n import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n@@ -16,19 +17,22 @@\n import org.elasticsearch.client.internal.Client;\n import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.util.Maps;\n+import org.elasticsearch.core.Nullable;\n import org.elasticsearch.index.IndexMode;\n import org.elasticsearch.index.mapper.TimeSeriesParams;\n import org.elasticsearch.index.query.QueryBuilder;\n import org.elasticsearch.logging.LogManager;\n import org.elasticsearch.logging.Logger;\n import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsAction;\n+import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse;\n import org.elasticsearch.xpack.esql.core.expression.MetadataAttribute;\n import org.elasticsearch.xpack.esql.core.type.DataType;\n import org.elasticsearch.xpack.esql.core.type.DateEsField;\n import org.elasticsearch.xpack.esql.core.type.EsField;\n import org.elasticsearch.xpack.esql.core.type.InvalidMappedField;\n import org.elasticsearch.xpack.esql.core.type.KeywordEsField;\n+import org.elasticsearch.xpack.esql.core.type.SupportedVersion;\n import org.elasticsearch.xpack.esql.core.type.TextEsField;\n import org.elasticsearch.xpack.esql.core.type.UnsupportedEsField;\n import org.elasticsearch.xpack.esql.index.EsIndex;\n@@ -45,9 +49,7 @@\n import java.util.TreeMap;\n import java.util.TreeSet;\n \n-import static org.elasticsearch.xpack.esql.core.type.DataType.AGGREGATE_METRIC_DOUBLE;\n import static org.elasticsearch.xpack.esql.core.type.DataType.DATETIME;\n-import static org.elasticsearch.xpack.esql.core.type.DataType.DENSE_VECTOR;\n import static org.elasticsearch.xpack.esql.core.type.DataType.KEYWORD;\n import static org.elasticsearch.xpack.esql.core.type.DataType.OBJECT;\n import static org.elasticsearch.xpack.esql.core.type.DataType.TEXT;\n@@ -89,8 +91,8 @@ public void resolveAsMergedMapping(\n         Set<String> fieldNames,\n         QueryBuilder requestFilter,\n         boolean includeAllDimensions,\n-        boolean supportsAggregateMetricDouble,\n-        boolean supportsDenseVector,\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported,\n         ActionListener<IndexResolution> listener\n     ) {\n         ActionListener<Versioned<IndexResolution>> ignoreVersion = listener.delegateFailureAndWrap(\n@@ -102,8 +104,8 @@ public void resolveAsMergedMapping(\n             fieldNames,\n             requestFilter,\n             includeAllDimensions,\n-            supportsAggregateMetricDouble,\n-            supportsDenseVector,\n+            useAggregateMetricDoubleWhenNotSupported,\n+            useDenseVectorWhenNotSupported,\n             ignoreVersion\n         );\n     }\n@@ -117,30 +119,81 @@ public void resolveAsMergedMappingAndRetrieveMinimumVersion(\n         Set<String> fieldNames,\n         QueryBuilder requestFilter,\n         boolean includeAllDimensions,\n-        boolean supportsAggregateMetricDouble,\n-        boolean supportsDenseVector,\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported,\n         ActionListener<Versioned<IndexResolution>> listener\n     ) {\n         client.execute(\n             EsqlResolveFieldsAction.TYPE,\n             createFieldCapsRequest(indexWildcard, fieldNames, requestFilter, includeAllDimensions),\n             listener.delegateFailureAndWrap((l, response) -> {\n-                TransportVersion minimumVersion = response.minTransportVersion();\n-\n-                LOGGER.debug(\"minimum transport version {}\", minimumVersion);\n-                l.onResponse(\n-                    new Versioned<>(\n-                        mergedMappings(indexWildcard, new FieldsInfo(response.caps(), supportsAggregateMetricDouble, supportsDenseVector)),\n-                        // The minimum transport version was added to the field caps response in 9.2.1; in clusters with older nodes,\n-                        // we don't have that information and need to assume the oldest supported version.\n-                        minimumVersion == null ? TransportVersion.minimumCompatible() : minimumVersion\n-                    )\n+                FieldsInfo info = new FieldsInfo(\n+                    response.caps(),\n+                    response.minTransportVersion(),\n+                    Build.current().isSnapshot(),\n+                    useAggregateMetricDoubleWhenNotSupported,\n+                    useDenseVectorWhenNotSupported\n                 );\n+                LOGGER.debug(\"minimum transport version {} {}\", response.minTransportVersion(), info.effectiveMinTransportVersion());\n+                l.onResponse(new Versioned<>(mergedMappings(indexWildcard, info), info.effectiveMinTransportVersion()));\n             })\n         );\n     }\n \n-    public record FieldsInfo(FieldCapabilitiesResponse caps, boolean supportAggregateMetricDouble, boolean supportDenseVector) {}\n+    /**\n+     * Information for resolving a field.\n+     * @param caps {@link FieldCapabilitiesResponse} from all indices involved in the query\n+     * @param minTransportVersion The minimum {@link TransportVersion} of any node that <strong>might</strong> receive the request.\n+     *                            More precisely, it's the minimum transport version of ALL nodes in ALL the clusters that the query\n+     *                            is targeting. It doesn't matter if the node is a data node or an ML node or a unicorn, it's transport\n+     *                            version counts. BUT if the query doesn't dispatch to that cluster AT ALL, we don't count the versions\n+     *                            of any nodes in that cluster.\n+     * @param currentBuildIsSnapshot is the current build a snapshot? Note: This is always {@code Build.current().isSnapshot()} in\n+     *                               production but tests need more control\n+     * @param useAggregateMetricDoubleWhenNotSupported does the query itself force us to use {@code aggregate_metric_double} fields\n+     *                                                 even if the remotes don't report that they support the type? This exists because\n+     *                                                 some remotes <strong>do</strong> support {@code aggregate_metric_double} without\n+     *                                                 reporting that they do. And, for a while, we used the query itself to opt into\n+     *                                                 reading these fields.\n+     * @param useDenseVectorWhenNotSupported does the query itself force us to use {@code dense_vector} fields even if the remotes don't\n+     *                                       report that they support the type? This exists because some remotes <strong>do</strong>\n+     *                                       support {@code dense_vector} without reporting that they do. And, for a while, we used the\n+     *                                       query itself to opt into reading these fields.\n+     */\n+    public record FieldsInfo(\n+        FieldCapabilitiesResponse caps,\n+        @Nullable TransportVersion minTransportVersion,\n+        boolean currentBuildIsSnapshot,\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported\n+    ) {\n+        /**\n+         * The {@link #minTransportVersion}, but if any remote didn't tell us the version we assume\n+         * that it's very, very old. This effectively disables any fields that were created \"recently\".\n+         * Which is appropriate because those fields are not supported on *almost* all versions that\n+         * don't return the transport version in the response.\n+         * <p>\n+         *     \"Very, very old\" above means that there are versions of Elasticsearch that we're wire\n+         *     compatible that with that don't support sending the version back. That's anything\n+         *     from {@code 8.19.FIRST} to {@code 9.2.0}. \"Recently\" means any field types we\n+         *     added support for after the initial release of ESQL. These fields use\n+         *     {@link SupportedVersion#supportedOn} rather than {@link SupportedVersion#SUPPORTED_ON_ALL_NODES}.\n+         *     Except for DATE_NANOS. For DATE_NANOS we got lucky/made a mistake. It wasn't widely\n+         *     used before ESQL added support for it and we weren't careful about enabling it. So\n+         *     queries on mixed version clusters that touch DATE_NANOS will fail. All the types\n+         *     added after that, like DENSE_VECTOR, will gracefully disable themselves when talking\n+         *     to older nodes.\n+         * </p>\n+         * <p>\n+         *     Note: Once {@link EsqlResolveFieldsResponse}'s CREATED version is live everywhere\n+         *     we can remove this and make sure {@link #minTransportVersion} is non-null. That'll\n+         *     be 10.0-ish.\n+         * </p>\n+         */\n+        TransportVersion effectiveMinTransportVersion() {\n+            return minTransportVersion != null ? minTransportVersion : TransportVersion.minimumCompatible();\n+        }\n+    }\n \n     // public for testing only\n     public static IndexResolution mergedMappings(String indexPattern, FieldsInfo fieldsInfo) {\n@@ -275,11 +328,16 @@ private static EsField createField(\n         IndexFieldCapabilities first = fcs.get(0);\n         List<IndexFieldCapabilities> rest = fcs.subList(1, fcs.size());\n         DataType type = EsqlDataTypeRegistry.INSTANCE.fromEs(first.type(), first.metricType());\n-        type = switch (type) {\n-            case AGGREGATE_METRIC_DOUBLE -> fieldsInfo.supportAggregateMetricDouble ? AGGREGATE_METRIC_DOUBLE : UNSUPPORTED;\n-            case DENSE_VECTOR -> fieldsInfo.supportDenseVector ? DENSE_VECTOR : UNSUPPORTED;\n-            default -> type;\n-        };\n+        boolean typeSupported = type.supportedVersion()\n+            .supportedOn(fieldsInfo.effectiveMinTransportVersion(), fieldsInfo.currentBuildIsSnapshot)\n+            || switch (type) {\n+                case AGGREGATE_METRIC_DOUBLE -> fieldsInfo.useAggregateMetricDoubleWhenNotSupported;\n+                case DENSE_VECTOR -> fieldsInfo.useDenseVectorWhenNotSupported;\n+                default -> false;\n+            };\n+        if (false == typeSupported) {\n+            type = UNSUPPORTED;\n+        }\n         boolean aggregatable = first.isAggregatable();\n         EsField.TimeSeriesFieldType timeSeriesFieldType = EsField.TimeSeriesFieldType.fromIndexFieldCapabilities(first);\n         if (rest.isEmpty() == false) {"
    },
    {
      "filename": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/analysis/AnalyzerTests.java",
      "status": "modified",
      "additions": 34,
      "deletions": 29,
      "changes": 63,
      "patch": "@@ -8,6 +8,7 @@\n package org.elasticsearch.xpack.esql.analysis;\n \n import org.elasticsearch.Build;\n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n import org.elasticsearch.action.fieldcaps.IndexFieldCapabilities;\n@@ -3152,16 +3153,14 @@ public void testResolveInsist_multiIndexFieldPartiallyMappedWithSingleKeywordTyp\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"keyword\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n \n@@ -3179,16 +3178,14 @@ public void testResolveInsist_multiIndexFieldExistsWithSingleTypeButIsNotKeyword\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3207,17 +3204,15 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesNoKeyw\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", messageResponseMap(\"date\")),\n                         fieldCapabilitiesIndexResponse(\"bazz\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3235,16 +3230,14 @@ public void testResolveInsist_multiIndexSameMapping_fieldIsMapped() {\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", messageResponseMap(\"long\"))\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3260,7 +3253,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesWithKe\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n@@ -3269,9 +3262,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesWithKe\n                         fieldCapabilitiesIndexResponse(\"qux\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3289,17 +3280,15 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesWithCa\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", messageResponseMap(\"date\")),\n                         fieldCapabilitiesIndexResponse(\"bazz\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         VerificationException e = expectThrows(\n@@ -3319,13 +3308,19 @@ public void testResolveDenseVector() {\n             List.of()\n         );\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, true, true));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, true, true)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(plan.output().getFirst().dataType(), equalTo(DENSE_VECTOR));\n         }\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, true, false));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, true, false)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(plan.output().getFirst().dataType(), equalTo(UNSUPPORTED));\n@@ -3343,7 +3338,10 @@ public void testResolveAggregateMetricDouble() {\n             List.of()\n         );\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, true, true));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, true, true)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(\n@@ -3352,7 +3350,10 @@ public void testResolveAggregateMetricDouble() {\n             );\n         }\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, false, true));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, false, true)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(plan.output().getFirst().dataType(), equalTo(UNSUPPORTED));\n@@ -3802,7 +3803,7 @@ private static LogicalPlan analyzeWithEmptyFieldCapsResponse(String query) throw\n         List<FieldCapabilitiesIndexResponse> idxResponses = List.of(\n             new FieldCapabilitiesIndexResponse(\"idx\", \"idx\", Map.of(), true, IndexMode.STANDARD)\n         );\n-        IndexResolver.FieldsInfo caps = new IndexResolver.FieldsInfo(new FieldCapabilitiesResponse(idxResponses, List.of()), true, true);\n+        IndexResolver.FieldsInfo caps = fieldsInfoOnCurrentVersion(new FieldCapabilitiesResponse(idxResponses, List.of()));\n         IndexResolution resolution = IndexResolver.mergedMappings(\"test*\", caps);\n         var analyzer = analyzer(indexResolutions(resolution), TEST_VERIFIER, configuration(query));\n         return analyze(query, analyzer);\n@@ -4692,4 +4693,8 @@ static Literal string(String value) {\n     static Literal literal(int value) {\n         return new Literal(EMPTY, value, INTEGER);\n     }\n+\n+    static IndexResolver.FieldsInfo fieldsInfoOnCurrentVersion(FieldCapabilitiesResponse caps) {\n+        return new IndexResolver.FieldsInfo(caps, TransportVersion.current(), false, false, false);\n+    }\n }"
    },
    {
      "filename": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/type/EsqlDataTypeRegistryTests.java",
      "status": "modified",
      "additions": 5,
      "deletions": 2,
      "changes": 7,
      "patch": "@@ -6,6 +6,7 @@\n  */\n package org.elasticsearch.xpack.esql.type;\n \n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n import org.elasticsearch.action.fieldcaps.IndexFieldCapabilitiesBuilder;\n@@ -28,7 +29,6 @@ public void testCounter() {\n         resolve(\"long\", TimeSeriesParams.MetricType.COUNTER, DataType.COUNTER_LONG);\n         resolve(\"integer\", TimeSeriesParams.MetricType.COUNTER, DataType.COUNTER_INTEGER);\n         resolve(\"double\", TimeSeriesParams.MetricType.COUNTER, DataType.COUNTER_DOUBLE);\n-\n     }\n \n     public void testGauge() {\n@@ -54,7 +54,10 @@ private void resolve(String esTypeName, TimeSeriesParams.MetricType metricType,\n \n         FieldCapabilitiesResponse caps = new FieldCapabilitiesResponse(idxResponses, List.of());\n         // IndexResolver uses EsqlDataTypeRegistry directly\n-        IndexResolution resolution = IndexResolver.mergedMappings(\"idx-*\", new IndexResolver.FieldsInfo(caps, true, true));\n+        IndexResolution resolution = IndexResolver.mergedMappings(\n+            \"idx-*\",\n+            new IndexResolver.FieldsInfo(caps, TransportVersion.current(), false, false, false)\n+        );\n         EsField f = resolution.get().mapping().get(field);\n         assertThat(f.getDataType(), equalTo(expected));\n     }"
    },
    {
      "filename": "x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_tsdb.yml",
      "status": "modified",
      "additions": 33,
      "deletions": 81,
      "changes": 114,
      "patch": "@@ -231,10 +231,7 @@ filter on counter without cast:\n       catch: bad_request\n       esql.query:\n         body:\n-          query: |\n-            FROM test\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | WHERE k8s.pod.network.tx == 1434577921\n+          query: 'from test | where k8s.pod.network.tx == 1434577921'\n \n ---\n cast counter then filter:\n@@ -244,8 +241,9 @@ cast counter then filter:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n-      reason: \"Uses TO_AGGREGATE_METRIC_DOUBLE\"\n+          capabilities: [ dense_vector_agg_metric_double_if_version ]\n+      reason: \"uses aggregate_metric_double\"\n+\n   - do:\n       esql.query:\n         body:\n@@ -268,18 +266,13 @@ sort on counter without cast:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [sorting_on_source_and_counters_forbidden, aggregate_metric_double_v0]\n+          capabilities: [sorting_on_source_and_counters_forbidden, dense_vector_agg_metric_double_if_version]\n       reason: \"Sorting on counters shouldn't have been possible\"\n   - do:\n       catch: /cannot sort on counter_long/\n       esql.query:\n         body:\n-          query: |\n-            FROM test\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | KEEP k8s.pod.network.tx\n-            | SORT k8s.pod.network.tx\n-            | LIMIT 1\n+          query: 'from test |  KEEP k8s.pod.network.tx | sort k8s.pod.network.tx | limit 1'\n \n ---\n cast then sort on counter:\n@@ -298,17 +291,14 @@ from doc with aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n+          query: 'from test2'\n \n   - match: {columns.0.name: \"@timestamp\"}\n   - match: {columns.0.type: \"date\"}\n@@ -330,17 +320,14 @@ stats on aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)\n+          query: 'FROM test2 | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)'\n   - length: {values: 1}\n   - length: {values.0: 4}\n   - match: {columns.0.name: \"max(agg_metric)\"}\n@@ -364,18 +351,16 @@ grouping stats on aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY dim\n-            | SORT dim\n+          query: \"FROM test2\n+          | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY dim\n+          | SORT dim\"\n   - length: {values: 2}\n   - length: {values.0: 5}\n   - match: {columns.0.name: \"max(agg_metric)\"}\n@@ -407,18 +392,14 @@ sorting with aggregate_metric_double with partial submetrics:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_sorting, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for sorting when aggregate_metric_double present\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test3\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | SORT @timestamp\n-            | KEEP @timestamp, agg_metric\n+          query: 'FROM test3 | SORT @timestamp | KEEP @timestamp, agg_metric'\n \n   - length: {values: 4}\n   - length: {values.0: 2}\n@@ -443,17 +424,13 @@ aggregate_metric_double unsortable:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_sorting, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for sorting when aggregate_metric_double present\"\n   - do:\n       catch: /cannot sort on aggregate_metric_double/\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | SORT agg_metric\n+          query: 'FROM test2 | sort agg_metric'\n \n ---\n stats on aggregate_metric_double with partial submetrics:\n@@ -463,18 +440,14 @@ stats on aggregate_metric_double with partial submetrics:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_partial_submetrics, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for partial submetrics in aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test3\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY k8s.pod.uid\n-            | SORT k8s.pod.uid\n+          query: 'FROM test3 | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY k8s.pod.uid | SORT k8s.pod.uid'\n \n   - length: {values: 2}\n   - length: {values.0: 5}\n@@ -507,17 +480,14 @@ stats on aggregate_metric_double missing min and max:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_partial_submetrics, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for partial submetrics in aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test4\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)\n+          query: 'FROM test4 | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)'\n \n   - length: {values: 1}\n   - length: {values.0: 4}\n@@ -542,17 +512,14 @@ render aggregate_metric_double when missing min and max:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test4\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | KEEP agg_metric\n+          query: 'FROM test4 | KEEP agg_metric'\n \n   - length: {values: 1}\n   - length: {values.0: 1}\n@@ -569,18 +536,14 @@ render aggregate_metric_double when missing value:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test3\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | WHERE @timestamp == \"2021-04-28T19:51:04.467Z\"\n-            | KEEP agg_metric\n+          query: 'FROM test3 | WHERE @timestamp == \"2021-04-28T19:51:04.467Z\" | KEEP agg_metric'\n \n   - length: {values: 1}\n   - length: {values.0: 1}\n@@ -597,18 +560,14 @@ to_string aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test4\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | EVAL agg = to_string(agg_metric)\n-            | KEEP agg\n+          query: 'FROM test4 | EVAL agg = to_string(agg_metric) | KEEP agg'\n \n   - length: {values: 1}\n   - length: {values.0: 1}\n@@ -624,17 +583,14 @@ from index pattern unsupported counter:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_partial_submetrics, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for partial submetrics in aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test*\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n+          query: 'FROM test*'\n \n   - match: {columns.0.name: \"@timestamp\"}\n   - match: {columns.0.type: \"date\"}\n@@ -719,7 +675,7 @@ to_aggregate_metric_double with multi_values:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_convert_to ]\n       reason: \"Support for to_aggregate_metric_double function\"\n \n   - do:\n@@ -769,19 +725,15 @@ avg of aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_avg, dense_vector_agg_metric_double_if_version]\n       reason: \"support avg aggregations with aggregate metric double\"\n \n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS avg = avg(agg_metric)\n-            | KEEP avg\n+          query: 'FROM test2 | STATS avg = avg(agg_metric) | KEEP avg'\n \n   - length: {values: 1}\n   - length: {values.0: 1}"
    },
    {
      "filename": "x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_unsupported_types.yml",
      "status": "modified",
      "additions": 20,
      "deletions": 13,
      "changes": 33,
      "patch": "@@ -145,8 +145,8 @@ unsupported:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [dense_vector_field_type_released, dense_vector_agg_metric_double_if_fns]\n-      reason: \"uses original_type\"\n+          capabilities: [dense_vector_agg_metric_double_if_version]\n+      reason: \"fetches dense_vector\"\n \n   - do:\n       allowed_warnings_regex:\n@@ -157,7 +157,8 @@ unsupported:\n           query: 'from test'\n \n   - match: { columns.0.name: aggregate_metric_double }\n-  - match: { columns.0.type: unsupported }\n+  - match: { columns.0.type: aggregate_metric_double }\n+  - is_false: columns.0.original_types\n   - match: { columns.1.name: binary }\n   - match: { columns.1.type: unsupported }\n   - match: { columns.1.original_types: [binary] }\n@@ -169,7 +170,7 @@ unsupported:\n   - match: { columns.4.name: date_range }\n   - match: { columns.4.type: unsupported }\n   - match: { columns.5.name: dense_vector }\n-  - match: { columns.5.type: unsupported }\n+  - match: { columns.5.type: dense_vector }\n   - match: { columns.6.name: double_range }\n   - match: { columns.6.type: unsupported }\n   - match: { columns.7.name: float_range }\n@@ -218,12 +219,14 @@ unsupported:\n   - match: { columns.28.type: integer }\n \n   - length: { values: 1 }\n-  - match: { values.0.0: null }\n+  - match: { values.0.0: '{\"min\":1.0,\"max\":3.0,\"sum\":10.1,\"value_count\":5}' }\n   - match: { values.0.1: null }\n   - match: { values.0.2: null }\n   - match: { values.0.3: \"2015-01-01T12:10:30.123456789Z\" }\n   - match: { values.0.4: null }\n-  - match: { values.0.5: null }\n+  - match: { values.0.5.0: 0.5 }\n+  - match: { values.0.5.1: 10.0 }\n+  - match: { values.0.5.2: 6.0 }\n   - match: { values.0.6: null }\n   - match: { values.0.7: null }\n   - match: { values.0.8: \"POINT (10.0 12.0)\" }\n@@ -255,7 +258,8 @@ unsupported:\n         body:\n           query: 'from test | limit 0'\n   - match: { columns.0.name: aggregate_metric_double }\n-  - match: { columns.0.type: unsupported }\n+  - match: { columns.0.type: aggregate_metric_double }\n+  - is_false: columns.0.original_types\n   - match: { columns.1.name: binary }\n   - match: { columns.1.type: unsupported }\n   - match: { columns.1.original_types: [binary] }\n@@ -267,7 +271,7 @@ unsupported:\n   - match: { columns.4.name: date_range }\n   - match: { columns.4.type: unsupported }\n   - match: { columns.5.name: dense_vector }\n-  - match: { columns.5.type: unsupported }\n+  - match: { columns.5.type: dense_vector }\n   - match: { columns.6.name: double_range }\n   - match: { columns.6.type: unsupported }\n   - match: { columns.7.name: float_range }\n@@ -338,7 +342,7 @@ unsupported with sort:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ dense_vector_field_type_released, dense_vector_agg_metric_double_if_fns ]\n+          capabilities: [ dense_vector_agg_metric_double_if_version ]\n       reason: \"support for sorting when dense_vector_field_type present\"\n \n   - do:\n@@ -350,7 +354,7 @@ unsupported with sort:\n           query: 'from test | sort some_doc.bar'\n \n   - match: { columns.0.name: aggregate_metric_double }\n-  - match: { columns.0.type: unsupported }\n+  - match: { columns.0.type: aggregate_metric_double }\n   - match: { columns.1.name: binary }\n   - match: { columns.1.type: unsupported }\n   - match: { columns.2.name: completion }\n@@ -360,7 +364,7 @@ unsupported with sort:\n   - match: { columns.4.name: date_range }\n   - match: { columns.4.type: unsupported }\n   - match: { columns.5.name: dense_vector }\n-  - match: { columns.5.type: unsupported }\n+  - match: { columns.5.type: dense_vector }\n   - match: { columns.6.name: double_range }\n   - match: { columns.6.type: unsupported }\n   - match: { columns.7.name: float_range }\n@@ -409,12 +413,14 @@ unsupported with sort:\n   - match: { columns.28.type: integer }\n \n   - length: { values: 1 }\n-  - match: { values.0.0: null }\n+  - match: { values.0.0: '{\"min\":1.0,\"max\":3.0,\"sum\":10.1,\"value_count\":5}' }\n   - match: { values.0.1: null }\n   - match: { values.0.2: null }\n   - match: { values.0.3: \"2015-01-01T12:10:30.123456789Z\" }\n   - match: { values.0.4: null }\n-  - match: { values.0.5: null }\n+  - match: { values.0.5.0: 0.5 }\n+  - match: { values.0.5.1: 10.0 }\n+  - match: { values.0.5.2: 6.0 }\n   - match: { values.0.6: null }\n   - match: { values.0.7: null }\n   - match: { values.0.8: \"POINT (10.0 12.0)\" }\n@@ -438,6 +444,7 @@ unsupported with sort:\n   - match: { values.0.26: xy }\n   - match: { values.0.27: \"foo bar\" }\n   - match: { values.0.28: 3 }\n+\n ---\n nested declared inline:\n   - do:"
    },
    {
      "filename": "x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/46_downsample.yml",
      "status": "modified",
      "additions": 37,
      "deletions": 65,
      "changes": 102,
      "patch": "@@ -83,7 +83,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0, dense_vector_agg_metric_double_if_fns]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       indices.downsample:\n@@ -98,12 +98,9 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-downsample\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx)\n-            | LIMIT 100\n+          query: \"FROM test-downsample |\n+          STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx)\n+          | LIMIT 100\"\n \n   - length: {values: 1}\n   - length: {values.0: 4}\n@@ -128,7 +125,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       indices.downsample:\n@@ -143,13 +140,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-downsample\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | WHERE @timestamp == \"2021-04-28T19:00:00.000Z\"\n-            | KEEP k8s.pod.network.rx\n-            | LIMIT 100\n+          query: \"FROM test-downsample | WHERE @timestamp == \\\"2021-04-28T19:00:00.000Z\\\" | KEEP k8s.pod.network.rx | LIMIT 100\"\n   - length: {values: 1}\n   - length: {values.0: 1}\n   - match: {columns.0.name: \"k8s.pod.network.rx\"}\n@@ -164,7 +155,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_convert_to, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for to_aggregate_metric_double function\"\n \n   - do:\n@@ -240,12 +231,11 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-*\n-            | WHERE k8s.pod.uid == \"947e4ced-1786-4e53-9e0c-5c447e959507\"\n-            | EVAL rx = to_aggregate_metric_double(k8s.pod.network.rx)\n-            | STATS max(rx), min(rx), sum(rx), count(rx)\n-            | LIMIT 100\n+          query: \"FROM test-* |\n+          WHERE k8s.pod.uid == \\\"947e4ced-1786-4e53-9e0c-5c447e959507\\\" |\n+          EVAL rx = to_aggregate_metric_double(k8s.pod.network.rx) |\n+          STATS max(rx), min(rx), sum(rx), count(rx) |\n+          LIMIT 100\"\n \n   - length: {values: 1}\n   - length: {values.0: 4}\n@@ -270,7 +260,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [metrics_capability, aggregate_metric_double_implicit_casting_in_aggs, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for casting aggregate metric double implicitly when present in aggregations\"\n \n   - do:\n@@ -346,12 +336,10 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-*\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | WHERE k8s.pod.uid == \"947e4ced-1786-4e53-9e0c-5c447e959507\"\n-            | STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx), avg(k8s.pod.network.rx)\n-            | LIMIT 100\n+          query: \"FROM test-* |\n+          WHERE k8s.pod.uid == \\\"947e4ced-1786-4e53-9e0c-5c447e959507\\\" |\n+          STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx), avg(k8s.pod.network.rx) |\n+          LIMIT 100\"\n \n   - length: {values: 1}\n   - length: {values.0: 5}\n@@ -374,7 +362,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: \"FROM test-* | STATS max = max(k8s.pod.network.rx)  | LIMIT 100\"\n+          query: \"TS test-* | STATS max = max(k8s.pod.network.rx)  | LIMIT 100\"\n   - length: {values: 1}\n   - length: {values.0: 1}\n   - match: {columns.0.name: \"max\"}\n@@ -389,7 +377,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [ts_command_v0, aggregate_metric_double_v0]\n+          capabilities: [metrics_command, aggregate_metric_double_implicit_casting_in_aggs, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for casting aggregate metric double implicitly when present in aggregations\"\n \n   - do:\n@@ -469,14 +457,12 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            TS test-*\n-            | STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n-                    count = sum(count_over_time(k8s.pod.network.rx)),\n-                    sum = sum(sum_over_time(k8s.pod.network.rx))\n-                 BY time_bucket = bucket(@timestamp, 1 hour)\n-            | SORT time_bucket\n-            | LIMIT 10\n+          query: \"TS test-* |\n+          STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n+                count = sum(count_over_time(k8s.pod.network.rx)),\n+                sum = sum(sum_over_time(k8s.pod.network.rx))\n+                BY time_bucket = bucket(@timestamp, 1 hour) |\n+          SORT time_bucket | LIMIT 10\"\n \n   - length: {values: 4}\n   - length: {values.0: 4}\n@@ -513,7 +499,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [ts_command_v0, aggregate_metric_double_v0]\n+          capabilities: [metrics_command, aggregate_metric_double_implicit_casting_in_aggs]\n       reason: \"Support for casting aggregate metric double implicitly when present in aggregations\"\n \n   - do:\n@@ -593,14 +579,13 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            TS test-*\n-            | STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n-                    count = sum(count_over_time(k8s.pod.network.rx)),\n-                    sum = sum(sum_over_time(k8s.pod.network.rx))\n-                 BY k8s.pod.name, time_bucket = bucket(@timestamp, 1 hour)\n-            | SORT time_bucket, k8s.pod.name\n-            |LIMIT 10\n+          query: \"TS test-* |\n+          STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n+                count = sum(count_over_time(k8s.pod.network.rx)),\n+                sum = sum(sum_over_time(k8s.pod.network.rx))\n+          BY k8s.pod.name, time_bucket = bucket(@timestamp, 1 hour) |\n+          SORT time_bucket, k8s.pod.name |\n+          LIMIT 10\"\n \n   - length: {values: 6}\n   - length: {values.0: 5}\n@@ -653,7 +638,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_sorting_fixed]\n       reason: \"Fix sorting for rows comprised of docs from multiple indices where agg metric is missing from some\"\n \n   - do:\n@@ -686,13 +671,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-*\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | SORT some_field, @timestamp, k8s.pod.uid\n-            | KEEP k8s.pod.network.rx, some_field, @timestamp\n-            | LIMIT 10\n+          query: \"FROM test-* | SORT some_field, @timestamp, k8s.pod.uid | KEEP k8s.pod.network.rx, some_field, @timestamp | LIMIT 10\"\n \n   - length: {values: 5}\n   - length: {values.0: 3}\n@@ -726,7 +705,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_mv_expand, dense_vector_agg_metric_double_if_version]\n       reason: \"Have MV_EXPAND not error out when applied to aggregate_metric_doubles (is a no-op)\"\n \n   - do:\n@@ -742,14 +721,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-downsample\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | MV_EXPAND k8s.pod.network.rx\n-            | SORT @timestamp, k8s.pod.uid\n-            | KEEP k8s.pod.network.rx, @timestamp\n-            | LIMIT 10\n+          query: \"FROM test-downsample | MV_EXPAND k8s.pod.network.rx | SORT @timestamp, k8s.pod.uid | KEEP k8s.pod.network.rx, @timestamp | LIMIT 10\"\n \n   - length: {values: 4}\n   - length: {values.0: 2}"
    }
  ],
  "diff": "diff --git a/docs/changelog/136327.yaml b/docs/changelog/136327.yaml\nnew file mode 100644\nindex 0000000000000..b665e7af01a3d\n--- /dev/null\n+++ b/docs/changelog/136327.yaml\n@@ -0,0 +1,5 @@\n+pr: 136327\n+summary: Enable new data types with created version\n+area: ES|QL\n+type: enhancement\n+issues: []\ndiff --git a/x-pack/plugin/build.gradle b/x-pack/plugin/build.gradle\nindex f3b6079fcb68e..90786f7c0c678 100644\n--- a/x-pack/plugin/build.gradle\n+++ b/x-pack/plugin/build.gradle\n@@ -143,19 +143,6 @@ tasks.named(\"yamlRestCompatTestTransform\").configure({ task ->\n   task.skipTest(\"ml/sparse_vector_search/Search on a sparse_vector field with dots in the field names\", \"Vectors are no longer returned by default\")\n   task.skipTest(\"ml/sparse_vector_search/Search on a nested sparse_vector field with dots in the field names and conflicting child fields\", \"Vectors are no longer returned by default\")\n   task.skipTest(\"esql/190_lookup_join/lookup-no-key-only-key\", \"Requires the fix\")\n-  task.skipTest(\"esql/40_tsdb/aggregate_metric_double unsortable\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/avg of aggregate_metric_double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/grouping stats on aggregate_metric_double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/render aggregate_metric_double when missing min and max\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/render aggregate_metric_double when missing value\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/sorting with aggregate_metric_double with partial submetrics\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/stats on aggregate_metric_double missing min and max\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/to_string aggregate_metric_double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/40_tsdb/stats on aggregate_metric_double with partial submetrics\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/MV_EXPAND on non-MV aggregate metric double\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/Query stats on downsampled index\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/Render stats from downsampled index\", \"Extra function required to enable the field type\")\n-  task.skipTest(\"esql/46_downsample/Sort from multiple indices one with aggregate metric double\", \"Extra function required to enable the field type\")\n })\n \n tasks.named('yamlRestCompatTest').configure {\ndiff --git a/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/DataType.java b/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/DataType.java\nindex 756c06b72f9d8..b05d6784c072f 100644\n--- a/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/DataType.java\n+++ b/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/DataType.java\n@@ -7,6 +7,7 @@\n package org.elasticsearch.xpack.esql.core.type;\n \n import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.Build;\n import org.elasticsearch.TransportVersion;\n import org.elasticsearch.common.io.stream.StreamInput;\n import org.elasticsearch.common.io.stream.StreamOutput;\n@@ -748,7 +749,7 @@ public DataType counter() {\n \n     @Override\n     public void writeTo(StreamOutput out) throws IOException {\n-        if (supportedVersion.supportedOn(out.getTransportVersion()) == false) {\n+        if (supportedVersion.supportedOn(out.getTransportVersion(), Build.current().isSnapshot()) == false) {\n             /*\n              * TODO when we implement version aware planning flip this to an IllegalStateException\n              * so we throw a 500 error. It'll be our bug then. Right now it's a sign that the user\ndiff --git a/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/SupportedVersion.java b/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/SupportedVersion.java\nindex 6aefd1908d5b2..8881592660861 100644\n--- a/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/SupportedVersion.java\n+++ b/x-pack/plugin/esql-core/src/main/java/org/elasticsearch/xpack/esql/core/type/SupportedVersion.java\n@@ -11,15 +11,15 @@\n import org.elasticsearch.TransportVersion;\n \n public interface SupportedVersion {\n-    boolean supportedOn(TransportVersion version);\n+    boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot);\n \n     default boolean supportedLocally() {\n-        return supportedOn(TransportVersion.current());\n+        return supportedOn(TransportVersion.current(), Build.current().isSnapshot());\n     }\n \n     SupportedVersion SUPPORTED_ON_ALL_NODES = new SupportedVersion() {\n         @Override\n-        public boolean supportedOn(TransportVersion version) {\n+        public boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot) {\n             return true;\n         }\n \n@@ -56,8 +56,8 @@ public String toString() {\n     // Check usage of this constant to be sure.\n     SupportedVersion UNDER_CONSTRUCTION = new SupportedVersion() {\n         @Override\n-        public boolean supportedOn(TransportVersion version) {\n-            return Build.current().isSnapshot();\n+        public boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot) {\n+            return currentBuildIsSnapshot;\n         }\n \n         @Override\n@@ -76,8 +76,8 @@ public String toString() {\n     static SupportedVersion supportedSince(TransportVersion supportedVersion) {\n         return new SupportedVersion() {\n             @Override\n-            public boolean supportedOn(TransportVersion version) {\n-                return version.supports(supportedVersion) || Build.current().isSnapshot();\n+            public boolean supportedOn(TransportVersion version, boolean currentBuildIsSnapshot) {\n+                return version.supports(supportedVersion) || currentBuildIsSnapshot;\n             }\n \n             @Override\ndiff --git a/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java b/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java\nindex 1e77a62711990..0617f03402730 100644\n--- a/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java\n+++ b/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java\n@@ -32,6 +32,7 @@\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Comparator;\n import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n@@ -43,6 +44,7 @@\n import static org.elasticsearch.test.ListMatcher.matchesList;\n import static org.elasticsearch.test.MapMatcher.assertMap;\n import static org.elasticsearch.test.MapMatcher.matchesMap;\n+import static org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse.RESOLVE_FIELDS_RESPONSE_CREATED_TV;\n import static org.hamcrest.Matchers.any;\n import static org.hamcrest.Matchers.anyOf;\n import static org.hamcrest.Matchers.containsString;\n@@ -76,11 +78,6 @@ public class AllSupportedFieldsTestCase extends ESRestTestCase {\n \n     @ParametersFactory(argumentFormatting = \"pref=%s mode=%s\")\n     public static List<Object[]> args() {\n-        if (Build.current().isSnapshot()) {\n-            // We only test behavior in release builds. Snapshot builds will have data types enabled that are still under construction.\n-            return List.of();\n-        }\n-\n         List<Object[]> args = new ArrayList<>();\n         for (MappedFieldType.FieldExtractPreference extractPreference : Arrays.asList(\n             null,\n@@ -102,7 +99,7 @@ protected AllSupportedFieldsTestCase(MappedFieldType.FieldExtractPreference extr\n         this.indexMode = indexMode;\n     }\n \n-    protected record NodeInfo(String cluster, String id, TransportVersion version, Set<String> roles) {}\n+    protected record NodeInfo(String cluster, String id, boolean snapshot, TransportVersion version, Set<String> roles) {}\n \n     private static Map<String, NodeInfo> nodeToInfo;\n \n@@ -126,6 +123,19 @@ protected boolean fetchDenseVectorAggMetricDoubleIfFns() throws IOException {\n         return clusterHasCapability(\"GET\", \"/_query\", List.of(), List.of(\"DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_FNS\")).orElse(false);\n     }\n \n+    private static Boolean denseVectorAggMetricDoubleIfVersion;\n+\n+    private boolean denseVectorAggMetricDoubleIfVersion() throws IOException {\n+        if (denseVectorAggMetricDoubleIfVersion == null) {\n+            denseVectorAggMetricDoubleIfVersion = fetchDenseVectorAggMetricDoubleIfVersion();\n+        }\n+        return denseVectorAggMetricDoubleIfVersion;\n+    }\n+\n+    protected boolean fetchDenseVectorAggMetricDoubleIfVersion() throws IOException {\n+        return clusterHasCapability(\"GET\", \"/_query\", List.of(), List.of(\"DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_VERSION\")).orElse(false);\n+    }\n+\n     private static Boolean supportsNodeAssignment;\n \n     protected boolean supportsNodeAssignment() throws IOException {\n@@ -153,11 +163,21 @@ protected static Map<String, NodeInfo> fetchNodeToInfo(RestClient client, String\n             String id = (String) n.getKey();\n             Map<?, ?> nodeInfo = (Map<?, ?>) n.getValue();\n             String nodeName = (String) extractValue(nodeInfo, \"name\");\n+\n+            /*\n+             * Figuring out is a node is a snapshot is kind of tricky. The main version\n+             * doesn't include -SNAPSHOT. But ${VERSION}-SNAPSHOT is in the node info\n+             * *somewhere*. So we do this silly toString here.\n+             */\n+            String version = (String) extractValue(nodeInfo, \"version\");\n+            boolean snapshot = nodeInfo.toString().contains(version + \"-SNAPSHOT\");\n+\n             TransportVersion transportVersion = TransportVersion.fromId((Integer) extractValue(nodeInfo, \"transport_version\"));\n             List<?> roles = (List<?>) nodeInfo.get(\"roles\");\n+\n             nodeToInfo.put(\n                 nodeName,\n-                new NodeInfo(cluster, id, transportVersion, roles.stream().map(Object::toString).collect(Collectors.toSet()))\n+                new NodeInfo(cluster, id, snapshot, transportVersion, roles.stream().map(Object::toString).collect(Collectors.toSet()))\n             );\n         }\n \n@@ -175,6 +195,22 @@ public void createIndices() throws IOException {\n         }\n     }\n \n+    /**\n+     * Make sure the test doesn't run on snapshot builds. Release builds only.\n+     * <p>\n+     *     {@link Build#isSnapshot()} checks if the version under test is a snapshot.\n+     *     But! This run test runs against many versions and if *any* are snapshots\n+     *     then this will fail. So we check the versions of each node in the cluster too.\n+     * </p>\n+     */\n+    @Before\n+    public void skipSnapshots() throws IOException {\n+        assumeFalse(\"Only supported on production builds\", Build.current().isSnapshot());\n+        for (NodeInfo n : allNodeToInfo().values()) {\n+            assumeFalse(\"Only supported on production builds\", n.snapshot());\n+        }\n+    }\n+\n     // TODO: Also add a test for _tsid once we can determine the minimum transport version of all nodes.\n     public final void testFetchAll() throws IOException {\n         Map<String, Object> response = esql(\"\"\"\n@@ -212,7 +248,7 @@ public final void testFetchAll() throws IOException {\n                 if (supportedInIndex(type) == false) {\n                     continue;\n                 }\n-                expectedValues = expectedValues.entry(fieldName(type), expectedValue(type));\n+                expectedValues = expectedValues.entry(fieldName(type), expectedValue(type, nodeInfo));\n             }\n             expectedValues = expectedValues.entry(\"_id\", any(String.class))\n                 .entry(\"_ignored\", nullValue())\n@@ -227,15 +263,23 @@ public final void testFetchAll() throws IOException {\n         profileLogger.clearProfile();\n     }\n \n-    // Tests a workaround and will become obsolete once we can determine the actual minimum transport version of all nodes.\n+    /**\n+     * Tests fetching {@code dense_vector} if possible. Uses the {@code dense_vector_agg_metric_double_if_fns}\n+     * work around if required.\n+     */\n     public final void testFetchDenseVector() throws IOException {\n         Map<String, Object> response;\n         try {\n-            response = esql(\"\"\"\n-                | EVAL k = v_l2_norm(f_dense_vector, [1])  // workaround to enable fetching dense_vector\n+            String request = \"\"\"\n                 | KEEP _index, f_dense_vector\n                 | LIMIT 1000\n-                \"\"\");\n+                \"\"\";\n+            if (denseVectorAggMetricDoubleIfVersion() == false) {\n+                request = \"\"\"\n+                    | EVAL k = v_l2_norm(f_dense_vector, [1])  // workaround to enable fetching dense_vector\n+                    \"\"\" + request;\n+            }\n+            response = esql(request);\n             if ((Boolean) response.get(\"is_partial\")) {\n                 Map<?, ?> clusters = (Map<?, ?>) response.get(\"_clusters\");\n                 Map<?, ?> details = (Map<?, ?>) clusters.get(\"details\");\n@@ -410,7 +454,7 @@ private void createAllTypesDoc(RestClient client, String indexName) throws IOExc\n     }\n \n     // This will become dependent on the minimum transport version of all nodes once we can determine that.\n-    private Matcher<?> expectedValue(DataType type) {\n+    private Matcher<?> expectedValue(DataType type, NodeInfo nodeInfo) throws IOException {\n         return switch (type) {\n             case BOOLEAN -> equalTo(true);\n             case COUNTER_LONG, LONG, COUNTER_INTEGER, INTEGER, UNSIGNED_LONG, SHORT, BYTE -> equalTo(1);\n@@ -429,14 +473,24 @@ private Matcher<?> expectedValue(DataType type) {\n             case GEO_SHAPE -> equalTo(\"POINT (-71.34 41.12)\");\n             case NULL -> nullValue();\n             case AGGREGATE_METRIC_DOUBLE -> {\n-                // Currently, we cannot tell if all nodes support it or not so we treat it as unsupported.\n-                // TODO: Fix this once we know the node versions.\n-                yield nullValue();\n+                /*\n+                 * We need both AGGREGATE_METRIC_DOUBLE_CREATED and RESOLVE_FIELDS_RESPONSE_CREATED_TV\n+                 * but RESOLVE_FIELDS_RESPONSE_CREATED_TV came last so it's enough to check just it.\n+                 */\n+                if (minVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) == false) {\n+                    yield nullValue();\n+                }\n+                yield equalTo(\"{\\\"min\\\":-302.5,\\\"max\\\":702.3,\\\"sum\\\":200.0,\\\"value_count\\\":25}\");\n             }\n             case DENSE_VECTOR -> {\n-                // Currently, we cannot tell if all nodes support it or not so we treat it as unsupported.\n-                // TODO: Fix this once we know the node versions.\n-                yield nullValue();\n+                /*\n+                 * We need both DENSE_VECTOR_CREATED and RESOLVE_FIELDS_RESPONSE_CREATED_TV\n+                 * but RESOLVE_FIELDS_RESPONSE_CREATED_TV came last so it's enough to check just it.\n+                 */\n+                if (minVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) == false) {\n+                    yield nullValue();\n+                }\n+                yield equalTo(List.of(0.5, 10.0, 5.9999995));\n             }\n             default -> throw new AssertionError(\"unsupported field type [\" + type + \"]\");\n         };\n@@ -507,7 +561,7 @@ private Map<String, Object> nameToValue(List<String> names, List<?> values) {\n     }\n \n     // This will become dependent on the minimum transport version of all nodes once we can determine that.\n-    private Matcher<String> expectedType(DataType type) {\n+    private Matcher<String> expectedType(DataType type) throws IOException {\n         return switch (type) {\n             case COUNTER_DOUBLE, COUNTER_LONG, COUNTER_INTEGER -> {\n                 if (indexMode == IndexMode.TIME_SERIES) {\n@@ -518,10 +572,16 @@ private Matcher<String> expectedType(DataType type) {\n             case BYTE, SHORT -> equalTo(\"integer\");\n             case HALF_FLOAT, SCALED_FLOAT, FLOAT -> equalTo(\"double\");\n             case NULL -> equalTo(\"keyword\");\n-            // Currently unsupported without TS command or KNN function\n-            case AGGREGATE_METRIC_DOUBLE, DENSE_VECTOR ->\n-                // TODO: Fix this once we know the node versions.\n-                equalTo(\"unsupported\");\n+            case AGGREGATE_METRIC_DOUBLE, DENSE_VECTOR -> {\n+                /*\n+                 * We need both <type_name>_CREATED and RESOLVE_FIELDS_RESPONSE_CREATED_TV\n+                 * but RESOLVE_FIELDS_RESPONSE_CREATED_TV came last so it's enough to check just it.\n+                 */\n+                if (minVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) == false) {\n+                    yield equalTo(\"unsupported\");\n+                }\n+                yield equalTo(type.esType());\n+            }\n             default -> equalTo(type.esType());\n         };\n     }\n@@ -555,9 +615,13 @@ private Map<String, NodeInfo> expectedIndices() throws IOException {\n                     name = e.getValue().cluster + \":\" + name;\n                 }\n                 // We should only end up with one per cluster\n-                result.put(name, new NodeInfo(e.getValue().cluster, null, e.getValue().version(), null));\n+                result.put(name, new NodeInfo(e.getValue().cluster, null, e.getValue().snapshot(), e.getValue().version(), null));\n             }\n         }\n         return result;\n     }\n+\n+    protected TransportVersion minVersion() throws IOException {\n+        return allNodeToInfo().values().stream().map(NodeInfo::version).min(Comparator.naturalOrder()).get();\n+    }\n }\ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-bit.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-bit.csv-spec\nindex 613b70b550789..dba1c1fbaa1f6 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-bit.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-bit.csv-spec\n@@ -1,9 +1,8 @@\n retrieveBitVectorData\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL k = v_l2_norm(bit_vector, [1,2])  // workaround to enable fetching dense_vector\n | KEEP id, bit_vector\n | SORT id\n ;\n@@ -16,11 +15,11 @@ id:l | bit_vector:dense_vector\n ;\n \n denseBitVectorWithEval\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL v = bit_vector, k = v_l2_norm(bit_vector, [1,2])  // workaround to enable fetching dense_vector\n+| EVAL v = bit_vector\n | KEEP id, v\n | SORT id\n ;\n@@ -33,14 +32,13 @@ id:l | v:dense_vector\n ;\n \n denseBitVectorWithRenameAndDrop\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n | EVAL v = bit_vector\n-| EVAL k = v_l2_norm(bit_vector, [1,2])  // workaround to enable fetching dense_vector\n | RENAME v AS new_vector\n-| DROP float_vector, byte_vector, bit_vector, k\n+| DROP float_vector, byte_vector, bit_vector\n | SORT id\n ;\n \ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-byte.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-byte.csv-spec\nindex ac4959f58c5bc..aaa3215a88781 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-byte.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector-byte.csv-spec\n@@ -1,9 +1,8 @@\n retrieveByteVectorData\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL k = v_l2_norm(byte_vector, [1,2,3]) // workaround to enable fetching dense_vector\n | KEEP id, byte_vector\n | SORT id\n ;\n@@ -16,12 +15,11 @@ id:l | byte_vector:dense_vector\n ;\n \n denseByteVectorWithEval\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n | EVAL v = byte_vector\n-| EVAL k = v_l2_norm(byte_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | KEEP id, v\n | SORT id\n ;\n@@ -34,14 +32,13 @@ id:l | v:dense_vector\n ;\n \n denseByteVectorWithRenameAndDrop\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector \n | EVAL v = byte_vector\n-| EVAL k = v_l2_norm(byte_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | RENAME v AS new_vector \n-| DROP float_vector, byte_vector, bit_vector, k\n+| DROP float_vector, byte_vector, bit_vector\n | SORT id\n ;\n \ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector.csv-spec\nindex 6a96ed14a9ed8..33a957d8a56cc 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/dense_vector.csv-spec\n@@ -1,10 +1,9 @@\n retrieveDenseVectorData\n required_capability: dense_vector_field_type_released\n-required_capability: dense_vector_agg_metric_double_if_fns\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n-| EVAL k = v_l2_norm(float_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | KEEP id, float_vector\n | SORT id\n ;\n@@ -17,12 +16,11 @@ id:l | float_vector:dense_vector\n ;\n \n denseVectorWithEval\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector\n | EVAL v = float_vector\n-| EVAL k = v_l2_norm(float_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | KEEP id, v\n | SORT id\n ;\n@@ -35,14 +33,13 @@ id:l | v:dense_vector\n ;\n \n denseVectorWithRenameAndDrop\n-required_capability: dense_vector_field_type_released\n+required_capability: dense_vector_agg_metric_double_if_version\n required_capability: l2_norm_vector_similarity_function\n \n FROM dense_vector \n | EVAL v = float_vector \n-| EVAL k = v_l2_norm(float_vector, [1,2,3])  // workaround to enable fetching dense_vector\n | RENAME v AS new_vector \n-| DROP float_vector, byte_vector, bit_vector, k\n+| DROP float_vector, byte_vector, bit_vector\n | SORT id\n ;\n \ndiff --git a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/inlinestats.csv-spec b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/inlinestats.csv-spec\nindex 586a0d87cee71..d77ee9d6e6c59 100644\n--- a/x-pack/plugin/esql/qa/testFixtures/src/main/resources/inlinestats.csv-spec\n+++ b/x-pack/plugin/esql/qa/testFixtures/src/main/resources/inlinestats.csv-spec\n@@ -4125,8 +4125,8 @@ from employees\n inlineStatsOnAggregateMetricDouble\n required_capability: inline_stats\n required_capability: aggregate_metric_double_v0\n+required_capability: dense_vector_agg_metric_double_if_version\n FROM k8s-downsampled\n-| EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)     // Temporary workaround to enable aggregate_metric_double\n | INLINE STATS tx_max = MAX(network.eth0.tx) BY pod\n | SORT @timestamp, cluster, pod\n | KEEP @timestamp, cluster, pod, network.eth0.tx, tx_max\ndiff --git a/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/DenseVectorFieldTypeIT.java b/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/DenseVectorFieldTypeIT.java\nindex 93f75f8395ab3..d551ca1679725 100644\n--- a/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/DenseVectorFieldTypeIT.java\n+++ b/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/DenseVectorFieldTypeIT.java\n@@ -95,8 +95,6 @@ public void testRetrieveFieldType() {\n \n         var query = \"\"\"\n             FROM test\n-            | EVAL k = v_l2_norm(vector, [1])  // workaround to enable fetching dense_vector\n-            | DROP k\n             \"\"\";\n \n         try (var resp = run(query)) {\n@@ -111,7 +109,6 @@ public void testRetrieveTopNDenseVectorFieldData() {\n \n         var query = \"\"\"\n                 FROM test\n-                | EVAL k = v_l2_norm(vector, [1])  // workaround to enable fetching dense_vector\n                 | KEEP id, vector\n                 | SORT id ASC\n             \"\"\";\n@@ -141,7 +138,6 @@ public void testRetrieveDenseVectorFieldData() {\n \n         var query = \"\"\"\n             FROM test\n-            | EVAL k = v_l2_norm(vector, [1])  // workaround to enable fetching dense_vector\n             | KEEP id, vector\n             \"\"\";\n \ndiff --git a/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/plugin/KnnFunctionIT.java b/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/plugin/KnnFunctionIT.java\nindex f6635defd1857..af5fa7cb72db0 100644\n--- a/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/plugin/KnnFunctionIT.java\n+++ b/x-pack/plugin/esql/src/internalClusterTest/java/org/elasticsearch/xpack/esql/plugin/KnnFunctionIT.java\n@@ -219,12 +219,10 @@ public void testKnnWithLookupJoin() {\n         var error = expectThrows(VerificationException.class, () -> run(query));\n         assertThat(\n             error.getMessage(),\n-            // TODO revert this when we have proper versioned type resolutions\n-            // containsString(\n-            // \"line 3:13: [KNN] function cannot operate on [lookup_vector], supplied by an index [test_lookup] in non-STANDARD \"\n-            // + \"mode [lookup]\"\n-            // )\n-            containsString(\"line 3:13: Cannot use field [lookup_vector] with unsupported type [dense_vector]\")\n+            containsString(\n+                \"line 3:13: [KNN] function cannot operate on [lookup_vector], supplied by an index [test_lookup] in non-STANDARD \"\n+                    + \"mode [lookup]\"\n+            )\n         );\n     }\n \ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlCapabilities.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlCapabilities.java\nindex 1338d337b1eaa..8dfc5f6a1a5b6 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlCapabilities.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlCapabilities.java\n@@ -1479,6 +1479,8 @@ public enum Cap {\n \n         DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_FNS,\n \n+        DENSE_VECTOR_AGG_METRIC_DOUBLE_IF_VERSION,\n+\n         /**\n          * FUSE L2_NORM score normalization support\n          */\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java\nindex 365b2b976e2f1..45dcfda444354 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java\n@@ -17,7 +17,7 @@\n import java.io.IOException;\n \n public class EsqlResolveFieldsResponse extends ActionResponse {\n-    private static final TransportVersion RESOLVE_FIELDS_RESPONSE_CREATED_TV = TransportVersion.fromName(\n+    public static final TransportVersion RESOLVE_FIELDS_RESPONSE_CREATED_TV = TransportVersion.fromName(\n         \"esql_resolve_fields_response_created\"\n     );\n \ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/analysis/PreAnalyzer.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/analysis/PreAnalyzer.java\nindex baa9a9519b231..13419894ffc50 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/analysis/PreAnalyzer.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/analysis/PreAnalyzer.java\n@@ -29,8 +29,8 @@ public record PreAnalysis(\n         Map<IndexPattern, IndexMode> indexes,\n         List<Enrich> enriches,\n         List<IndexPattern> lookupIndices,\n-        boolean supportsAggregateMetricDouble,\n-        boolean supportsDenseVector\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported\n     ) {\n         public static final PreAnalysis EMPTY = new PreAnalysis(Map.of(), List.of(), List.of(), false, false);\n     }\n@@ -64,19 +64,22 @@ protected PreAnalysis doPreAnalyze(LogicalPlan plan) {\n         plan.forEachUp(Enrich.class, unresolvedEnriches::add);\n \n         /*\n-         * Enable aggregate_metric_double and dense_vector when we see certain function\n-         * or the TS command. This allows us to release these when not all nodes understand\n+         * Enable aggregate_metric_double and dense_vector when we see certain functions\n+         * or the TS command. This allowed us to release these when not all nodes understand\n          * these types. These functions are only supported on newer nodes, so we use them\n          * as a signal that the query is only for nodes that support these types.\n          *\n-         * This work around is temporary until we flow the minimum transport version\n-         * back through a cross cluster search field caps call.\n+         * This was a workaround that was required to enable these in 9.2.0. These days\n+         * we enable these field types if all nodes in all clusters support them. But this\n+         * work around persists to support force-enabling them on queries that might touch\n+         * nodes that don't have 9.2.1 or 9.3.0. If all nodes in the cluster have 9.2.1 or 9.3.0\n+         * this code doesn't do anything.\n          */\n-        Holder<Boolean> supportsAggregateMetricDouble = new Holder<>(false);\n-        Holder<Boolean> supportsDenseVector = new Holder<>(false);\n+        Holder<Boolean> useAggregateMetricDoubleWhenNotSupported = new Holder<>(false);\n+        Holder<Boolean> useDenseVectorWhenNotSupported = new Holder<>(false);\n         indexes.forEach((ip, mode) -> {\n             if (mode == IndexMode.TIME_SERIES) {\n-                supportsAggregateMetricDouble.set(true);\n+                useAggregateMetricDoubleWhenNotSupported.set(true);\n             }\n         });\n         plan.forEachDown(p -> p.forEachExpression(UnresolvedFunction.class, fn -> {\n@@ -88,16 +91,22 @@ protected PreAnalysis doPreAnalyze(LogicalPlan plan) {\n                 || fn.name().equalsIgnoreCase(\"v_l2_norm\")\n                 || fn.name().equalsIgnoreCase(\"v_dot_product\")\n                 || fn.name().equalsIgnoreCase(\"v_magnitude\")) {\n-                supportsDenseVector.set(true);\n+                useDenseVectorWhenNotSupported.set(true);\n             }\n             if (fn.name().equalsIgnoreCase(\"to_aggregate_metric_double\")) {\n-                supportsAggregateMetricDouble.set(true);\n+                useAggregateMetricDoubleWhenNotSupported.set(true);\n             }\n         }));\n \n         // mark plan as preAnalyzed (if it were marked, there would be no analysis)\n         plan.forEachUp(LogicalPlan::setPreAnalyzed);\n \n-        return new PreAnalysis(indexes, unresolvedEnriches, lookupIndices, supportsAggregateMetricDouble.get(), supportsDenseVector.get());\n+        return new PreAnalysis(\n+            indexes,\n+            unresolvedEnriches,\n+            lookupIndices,\n+            useAggregateMetricDoubleWhenNotSupported.get(),\n+            useDenseVectorWhenNotSupported.get()\n+        );\n     }\n }\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java\nindex ed88903dd5e0f..bb92a9de1d42f 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/EsqlSession.java\n@@ -856,8 +856,8 @@ private void preAnalyzeMainIndices(\n                     default -> requestFilter;\n                 },\n                 indexMode == IndexMode.TIME_SERIES,\n-                preAnalysis.supportsAggregateMetricDouble(),\n-                preAnalysis.supportsDenseVector(),\n+                preAnalysis.useAggregateMetricDoubleWhenNotSupported(),\n+                preAnalysis.useDenseVectorWhenNotSupported(),\n                 listener.delegateFailureAndWrap((l, indexResolution) -> {\n                     EsqlCCSUtils.updateExecutionInfoWithUnavailableClusters(executionInfo, indexResolution.inner().failures());\n                     l.onResponse(\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java\nindex 49d9476126d4d..c5d89b3cb98f8 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java\n@@ -6,6 +6,7 @@\n  */\n package org.elasticsearch.xpack.esql.session;\n \n+import org.elasticsearch.Build;\n import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n@@ -16,6 +17,7 @@\n import org.elasticsearch.client.internal.Client;\n import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.util.Maps;\n+import org.elasticsearch.core.Nullable;\n import org.elasticsearch.index.IndexMode;\n import org.elasticsearch.index.mapper.TimeSeriesParams;\n import org.elasticsearch.index.query.QueryBuilder;\n@@ -23,12 +25,14 @@\n import org.elasticsearch.logging.Logger;\n import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsAction;\n+import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse;\n import org.elasticsearch.xpack.esql.core.expression.MetadataAttribute;\n import org.elasticsearch.xpack.esql.core.type.DataType;\n import org.elasticsearch.xpack.esql.core.type.DateEsField;\n import org.elasticsearch.xpack.esql.core.type.EsField;\n import org.elasticsearch.xpack.esql.core.type.InvalidMappedField;\n import org.elasticsearch.xpack.esql.core.type.KeywordEsField;\n+import org.elasticsearch.xpack.esql.core.type.SupportedVersion;\n import org.elasticsearch.xpack.esql.core.type.TextEsField;\n import org.elasticsearch.xpack.esql.core.type.UnsupportedEsField;\n import org.elasticsearch.xpack.esql.index.EsIndex;\n@@ -45,9 +49,7 @@\n import java.util.TreeMap;\n import java.util.TreeSet;\n \n-import static org.elasticsearch.xpack.esql.core.type.DataType.AGGREGATE_METRIC_DOUBLE;\n import static org.elasticsearch.xpack.esql.core.type.DataType.DATETIME;\n-import static org.elasticsearch.xpack.esql.core.type.DataType.DENSE_VECTOR;\n import static org.elasticsearch.xpack.esql.core.type.DataType.KEYWORD;\n import static org.elasticsearch.xpack.esql.core.type.DataType.OBJECT;\n import static org.elasticsearch.xpack.esql.core.type.DataType.TEXT;\n@@ -89,8 +91,8 @@ public void resolveAsMergedMapping(\n         Set<String> fieldNames,\n         QueryBuilder requestFilter,\n         boolean includeAllDimensions,\n-        boolean supportsAggregateMetricDouble,\n-        boolean supportsDenseVector,\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported,\n         ActionListener<IndexResolution> listener\n     ) {\n         ActionListener<Versioned<IndexResolution>> ignoreVersion = listener.delegateFailureAndWrap(\n@@ -102,8 +104,8 @@ public void resolveAsMergedMapping(\n             fieldNames,\n             requestFilter,\n             includeAllDimensions,\n-            supportsAggregateMetricDouble,\n-            supportsDenseVector,\n+            useAggregateMetricDoubleWhenNotSupported,\n+            useDenseVectorWhenNotSupported,\n             ignoreVersion\n         );\n     }\n@@ -117,30 +119,81 @@ public void resolveAsMergedMappingAndRetrieveMinimumVersion(\n         Set<String> fieldNames,\n         QueryBuilder requestFilter,\n         boolean includeAllDimensions,\n-        boolean supportsAggregateMetricDouble,\n-        boolean supportsDenseVector,\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported,\n         ActionListener<Versioned<IndexResolution>> listener\n     ) {\n         client.execute(\n             EsqlResolveFieldsAction.TYPE,\n             createFieldCapsRequest(indexWildcard, fieldNames, requestFilter, includeAllDimensions),\n             listener.delegateFailureAndWrap((l, response) -> {\n-                TransportVersion minimumVersion = response.minTransportVersion();\n-\n-                LOGGER.debug(\"minimum transport version {}\", minimumVersion);\n-                l.onResponse(\n-                    new Versioned<>(\n-                        mergedMappings(indexWildcard, new FieldsInfo(response.caps(), supportsAggregateMetricDouble, supportsDenseVector)),\n-                        // The minimum transport version was added to the field caps response in 9.2.1; in clusters with older nodes,\n-                        // we don't have that information and need to assume the oldest supported version.\n-                        minimumVersion == null ? TransportVersion.minimumCompatible() : minimumVersion\n-                    )\n+                FieldsInfo info = new FieldsInfo(\n+                    response.caps(),\n+                    response.minTransportVersion(),\n+                    Build.current().isSnapshot(),\n+                    useAggregateMetricDoubleWhenNotSupported,\n+                    useDenseVectorWhenNotSupported\n                 );\n+                LOGGER.debug(\"minimum transport version {} {}\", response.minTransportVersion(), info.effectiveMinTransportVersion());\n+                l.onResponse(new Versioned<>(mergedMappings(indexWildcard, info), info.effectiveMinTransportVersion()));\n             })\n         );\n     }\n \n-    public record FieldsInfo(FieldCapabilitiesResponse caps, boolean supportAggregateMetricDouble, boolean supportDenseVector) {}\n+    /**\n+     * Information for resolving a field.\n+     * @param caps {@link FieldCapabilitiesResponse} from all indices involved in the query\n+     * @param minTransportVersion The minimum {@link TransportVersion} of any node that <strong>might</strong> receive the request.\n+     *                            More precisely, it's the minimum transport version of ALL nodes in ALL the clusters that the query\n+     *                            is targeting. It doesn't matter if the node is a data node or an ML node or a unicorn, it's transport\n+     *                            version counts. BUT if the query doesn't dispatch to that cluster AT ALL, we don't count the versions\n+     *                            of any nodes in that cluster.\n+     * @param currentBuildIsSnapshot is the current build a snapshot? Note: This is always {@code Build.current().isSnapshot()} in\n+     *                               production but tests need more control\n+     * @param useAggregateMetricDoubleWhenNotSupported does the query itself force us to use {@code aggregate_metric_double} fields\n+     *                                                 even if the remotes don't report that they support the type? This exists because\n+     *                                                 some remotes <strong>do</strong> support {@code aggregate_metric_double} without\n+     *                                                 reporting that they do. And, for a while, we used the query itself to opt into\n+     *                                                 reading these fields.\n+     * @param useDenseVectorWhenNotSupported does the query itself force us to use {@code dense_vector} fields even if the remotes don't\n+     *                                       report that they support the type? This exists because some remotes <strong>do</strong>\n+     *                                       support {@code dense_vector} without reporting that they do. And, for a while, we used the\n+     *                                       query itself to opt into reading these fields.\n+     */\n+    public record FieldsInfo(\n+        FieldCapabilitiesResponse caps,\n+        @Nullable TransportVersion minTransportVersion,\n+        boolean currentBuildIsSnapshot,\n+        boolean useAggregateMetricDoubleWhenNotSupported,\n+        boolean useDenseVectorWhenNotSupported\n+    ) {\n+        /**\n+         * The {@link #minTransportVersion}, but if any remote didn't tell us the version we assume\n+         * that it's very, very old. This effectively disables any fields that were created \"recently\".\n+         * Which is appropriate because those fields are not supported on *almost* all versions that\n+         * don't return the transport version in the response.\n+         * <p>\n+         *     \"Very, very old\" above means that there are versions of Elasticsearch that we're wire\n+         *     compatible that with that don't support sending the version back. That's anything\n+         *     from {@code 8.19.FIRST} to {@code 9.2.0}. \"Recently\" means any field types we\n+         *     added support for after the initial release of ESQL. These fields use\n+         *     {@link SupportedVersion#supportedOn} rather than {@link SupportedVersion#SUPPORTED_ON_ALL_NODES}.\n+         *     Except for DATE_NANOS. For DATE_NANOS we got lucky/made a mistake. It wasn't widely\n+         *     used before ESQL added support for it and we weren't careful about enabling it. So\n+         *     queries on mixed version clusters that touch DATE_NANOS will fail. All the types\n+         *     added after that, like DENSE_VECTOR, will gracefully disable themselves when talking\n+         *     to older nodes.\n+         * </p>\n+         * <p>\n+         *     Note: Once {@link EsqlResolveFieldsResponse}'s CREATED version is live everywhere\n+         *     we can remove this and make sure {@link #minTransportVersion} is non-null. That'll\n+         *     be 10.0-ish.\n+         * </p>\n+         */\n+        TransportVersion effectiveMinTransportVersion() {\n+            return minTransportVersion != null ? minTransportVersion : TransportVersion.minimumCompatible();\n+        }\n+    }\n \n     // public for testing only\n     public static IndexResolution mergedMappings(String indexPattern, FieldsInfo fieldsInfo) {\n@@ -275,11 +328,16 @@ private static EsField createField(\n         IndexFieldCapabilities first = fcs.get(0);\n         List<IndexFieldCapabilities> rest = fcs.subList(1, fcs.size());\n         DataType type = EsqlDataTypeRegistry.INSTANCE.fromEs(first.type(), first.metricType());\n-        type = switch (type) {\n-            case AGGREGATE_METRIC_DOUBLE -> fieldsInfo.supportAggregateMetricDouble ? AGGREGATE_METRIC_DOUBLE : UNSUPPORTED;\n-            case DENSE_VECTOR -> fieldsInfo.supportDenseVector ? DENSE_VECTOR : UNSUPPORTED;\n-            default -> type;\n-        };\n+        boolean typeSupported = type.supportedVersion()\n+            .supportedOn(fieldsInfo.effectiveMinTransportVersion(), fieldsInfo.currentBuildIsSnapshot)\n+            || switch (type) {\n+                case AGGREGATE_METRIC_DOUBLE -> fieldsInfo.useAggregateMetricDoubleWhenNotSupported;\n+                case DENSE_VECTOR -> fieldsInfo.useDenseVectorWhenNotSupported;\n+                default -> false;\n+            };\n+        if (false == typeSupported) {\n+            type = UNSUPPORTED;\n+        }\n         boolean aggregatable = first.isAggregatable();\n         EsField.TimeSeriesFieldType timeSeriesFieldType = EsField.TimeSeriesFieldType.fromIndexFieldCapabilities(first);\n         if (rest.isEmpty() == false) {\ndiff --git a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/analysis/AnalyzerTests.java b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/analysis/AnalyzerTests.java\nindex 46c8b73213abe..5a2be5973bc95 100644\n--- a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/analysis/AnalyzerTests.java\n+++ b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/analysis/AnalyzerTests.java\n@@ -8,6 +8,7 @@\n package org.elasticsearch.xpack.esql.analysis;\n \n import org.elasticsearch.Build;\n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n import org.elasticsearch.action.fieldcaps.IndexFieldCapabilities;\n@@ -3152,16 +3153,14 @@ public void testResolveInsist_multiIndexFieldPartiallyMappedWithSingleKeywordTyp\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"keyword\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n \n@@ -3179,16 +3178,14 @@ public void testResolveInsist_multiIndexFieldExistsWithSingleTypeButIsNotKeyword\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3207,7 +3204,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesNoKeyw\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n@@ -3215,9 +3212,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesNoKeyw\n                         fieldCapabilitiesIndexResponse(\"bazz\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3235,16 +3230,14 @@ public void testResolveInsist_multiIndexSameMapping_fieldIsMapped() {\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n                         fieldCapabilitiesIndexResponse(\"bar\", messageResponseMap(\"long\"))\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3260,7 +3253,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesWithKe\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n@@ -3269,9 +3262,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesWithKe\n                         fieldCapabilitiesIndexResponse(\"qux\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         var plan = analyze(\"FROM foo, bar | INSIST_üêî message\", analyzer(resolution, TEST_VERIFIER));\n@@ -3289,7 +3280,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesWithCa\n \n         IndexResolution resolution = IndexResolver.mergedMappings(\n             \"foo,bar\",\n-            new IndexResolver.FieldsInfo(\n+            fieldsInfoOnCurrentVersion(\n                 new FieldCapabilitiesResponse(\n                     List.of(\n                         fieldCapabilitiesIndexResponse(\"foo\", messageResponseMap(\"long\")),\n@@ -3297,9 +3288,7 @@ public void testResolveInsist_multiIndexFieldPartiallyExistsWithMultiTypesWithCa\n                         fieldCapabilitiesIndexResponse(\"bazz\", Map.of())\n                     ),\n                     List.of()\n-                ),\n-                true,\n-                true\n+                )\n             )\n         );\n         VerificationException e = expectThrows(\n@@ -3319,13 +3308,19 @@ public void testResolveDenseVector() {\n             List.of()\n         );\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, true, true));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, true, true)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(plan.output().getFirst().dataType(), equalTo(DENSE_VECTOR));\n         }\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, true, false));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, true, false)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(plan.output().getFirst().dataType(), equalTo(UNSUPPORTED));\n@@ -3343,7 +3338,10 @@ public void testResolveAggregateMetricDouble() {\n             List.of()\n         );\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, true, true));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, true, true)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(\n@@ -3352,7 +3350,10 @@ public void testResolveAggregateMetricDouble() {\n             );\n         }\n         {\n-            IndexResolution resolution = IndexResolver.mergedMappings(\"foo\", new IndexResolver.FieldsInfo(caps, false, true));\n+            IndexResolution resolution = IndexResolver.mergedMappings(\n+                \"foo\",\n+                new IndexResolver.FieldsInfo(caps, TransportVersion.minimumCompatible(), false, false, true)\n+            );\n             var plan = analyze(\"FROM foo\", analyzer(resolution, TEST_VERIFIER));\n             assertThat(plan.output(), hasSize(1));\n             assertThat(plan.output().getFirst().dataType(), equalTo(UNSUPPORTED));\n@@ -3802,7 +3803,7 @@ private static LogicalPlan analyzeWithEmptyFieldCapsResponse(String query) throw\n         List<FieldCapabilitiesIndexResponse> idxResponses = List.of(\n             new FieldCapabilitiesIndexResponse(\"idx\", \"idx\", Map.of(), true, IndexMode.STANDARD)\n         );\n-        IndexResolver.FieldsInfo caps = new IndexResolver.FieldsInfo(new FieldCapabilitiesResponse(idxResponses, List.of()), true, true);\n+        IndexResolver.FieldsInfo caps = fieldsInfoOnCurrentVersion(new FieldCapabilitiesResponse(idxResponses, List.of()));\n         IndexResolution resolution = IndexResolver.mergedMappings(\"test*\", caps);\n         var analyzer = analyzer(indexResolutions(resolution), TEST_VERIFIER, configuration(query));\n         return analyze(query, analyzer);\n@@ -4692,4 +4693,8 @@ static Literal string(String value) {\n     static Literal literal(int value) {\n         return new Literal(EMPTY, value, INTEGER);\n     }\n+\n+    static IndexResolver.FieldsInfo fieldsInfoOnCurrentVersion(FieldCapabilitiesResponse caps) {\n+        return new IndexResolver.FieldsInfo(caps, TransportVersion.current(), false, false, false);\n+    }\n }\ndiff --git a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/type/EsqlDataTypeRegistryTests.java b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/type/EsqlDataTypeRegistryTests.java\nindex fbb22c49af331..c201f544372db 100644\n--- a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/type/EsqlDataTypeRegistryTests.java\n+++ b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/type/EsqlDataTypeRegistryTests.java\n@@ -6,6 +6,7 @@\n  */\n package org.elasticsearch.xpack.esql.type;\n \n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n import org.elasticsearch.action.fieldcaps.IndexFieldCapabilitiesBuilder;\n@@ -28,7 +29,6 @@ public void testCounter() {\n         resolve(\"long\", TimeSeriesParams.MetricType.COUNTER, DataType.COUNTER_LONG);\n         resolve(\"integer\", TimeSeriesParams.MetricType.COUNTER, DataType.COUNTER_INTEGER);\n         resolve(\"double\", TimeSeriesParams.MetricType.COUNTER, DataType.COUNTER_DOUBLE);\n-\n     }\n \n     public void testGauge() {\n@@ -54,7 +54,10 @@ private void resolve(String esTypeName, TimeSeriesParams.MetricType metricType,\n \n         FieldCapabilitiesResponse caps = new FieldCapabilitiesResponse(idxResponses, List.of());\n         // IndexResolver uses EsqlDataTypeRegistry directly\n-        IndexResolution resolution = IndexResolver.mergedMappings(\"idx-*\", new IndexResolver.FieldsInfo(caps, true, true));\n+        IndexResolution resolution = IndexResolver.mergedMappings(\n+            \"idx-*\",\n+            new IndexResolver.FieldsInfo(caps, TransportVersion.current(), false, false, false)\n+        );\n         EsField f = resolution.get().mapping().get(field);\n         assertThat(f.getDataType(), equalTo(expected));\n     }\ndiff --git a/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_tsdb.yml b/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_tsdb.yml\nindex 30c7d73affe19..0478548f51da1 100644\n--- a/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_tsdb.yml\n+++ b/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_tsdb.yml\n@@ -231,10 +231,7 @@ filter on counter without cast:\n       catch: bad_request\n       esql.query:\n         body:\n-          query: |\n-            FROM test\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | WHERE k8s.pod.network.tx == 1434577921\n+          query: 'from test | where k8s.pod.network.tx == 1434577921'\n \n ---\n cast counter then filter:\n@@ -244,8 +241,9 @@ cast counter then filter:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n-      reason: \"Uses TO_AGGREGATE_METRIC_DOUBLE\"\n+          capabilities: [ dense_vector_agg_metric_double_if_version ]\n+      reason: \"uses aggregate_metric_double\"\n+\n   - do:\n       esql.query:\n         body:\n@@ -268,18 +266,13 @@ sort on counter without cast:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [sorting_on_source_and_counters_forbidden, aggregate_metric_double_v0]\n+          capabilities: [sorting_on_source_and_counters_forbidden, dense_vector_agg_metric_double_if_version]\n       reason: \"Sorting on counters shouldn't have been possible\"\n   - do:\n       catch: /cannot sort on counter_long/\n       esql.query:\n         body:\n-          query: |\n-            FROM test\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | KEEP k8s.pod.network.tx\n-            | SORT k8s.pod.network.tx\n-            | LIMIT 1\n+          query: 'from test |  KEEP k8s.pod.network.tx | sort k8s.pod.network.tx | limit 1'\n \n ---\n cast then sort on counter:\n@@ -298,17 +291,14 @@ from doc with aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n+          query: 'from test2'\n \n   - match: {columns.0.name: \"@timestamp\"}\n   - match: {columns.0.type: \"date\"}\n@@ -330,17 +320,14 @@ stats on aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)\n+          query: 'FROM test2 | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)'\n   - length: {values: 1}\n   - length: {values.0: 4}\n   - match: {columns.0.name: \"max(agg_metric)\"}\n@@ -364,18 +351,16 @@ grouping stats on aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY dim\n-            | SORT dim\n+          query: \"FROM test2\n+          | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY dim\n+          | SORT dim\"\n   - length: {values: 2}\n   - length: {values.0: 5}\n   - match: {columns.0.name: \"max(agg_metric)\"}\n@@ -407,18 +392,14 @@ sorting with aggregate_metric_double with partial submetrics:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_sorting, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for sorting when aggregate_metric_double present\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test3\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | SORT @timestamp\n-            | KEEP @timestamp, agg_metric\n+          query: 'FROM test3 | SORT @timestamp | KEEP @timestamp, agg_metric'\n \n   - length: {values: 4}\n   - length: {values.0: 2}\n@@ -443,17 +424,13 @@ aggregate_metric_double unsortable:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_sorting, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for sorting when aggregate_metric_double present\"\n   - do:\n       catch: /cannot sort on aggregate_metric_double/\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | SORT agg_metric\n+          query: 'FROM test2 | sort agg_metric'\n \n ---\n stats on aggregate_metric_double with partial submetrics:\n@@ -463,18 +440,14 @@ stats on aggregate_metric_double with partial submetrics:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_partial_submetrics, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for partial submetrics in aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test3\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY k8s.pod.uid\n-            | SORT k8s.pod.uid\n+          query: 'FROM test3 | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric) BY k8s.pod.uid | SORT k8s.pod.uid'\n \n   - length: {values: 2}\n   - length: {values.0: 5}\n@@ -507,17 +480,14 @@ stats on aggregate_metric_double missing min and max:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_partial_submetrics, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for partial submetrics in aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test4\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)\n+          query: 'FROM test4 | STATS max(agg_metric), min(agg_metric), sum(agg_metric), count(agg_metric)'\n \n   - length: {values: 1}\n   - length: {values.0: 4}\n@@ -542,17 +512,14 @@ render aggregate_metric_double when missing min and max:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test4\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | KEEP agg_metric\n+          query: 'FROM test4 | KEEP agg_metric'\n \n   - length: {values: 1}\n   - length: {values.0: 1}\n@@ -569,18 +536,14 @@ render aggregate_metric_double when missing value:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test3\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | WHERE @timestamp == \"2021-04-28T19:51:04.467Z\"\n-            | KEEP agg_metric\n+          query: 'FROM test3 | WHERE @timestamp == \"2021-04-28T19:51:04.467Z\" | KEEP agg_metric'\n \n   - length: {values: 1}\n   - length: {values.0: 1}\n@@ -597,18 +560,14 @@ to_string aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version ]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test4\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | EVAL agg = to_string(agg_metric)\n-            | KEEP agg\n+          query: 'FROM test4 | EVAL agg = to_string(agg_metric) | KEEP agg'\n \n   - length: {values: 1}\n   - length: {values.0: 1}\n@@ -624,17 +583,14 @@ from index pattern unsupported counter:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_partial_submetrics, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for partial submetrics in aggregate_metric_double\"\n   - do:\n       allowed_warnings_regex:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test*\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n+          query: 'FROM test*'\n \n   - match: {columns.0.name: \"@timestamp\"}\n   - match: {columns.0.type: \"date\"}\n@@ -719,7 +675,7 @@ to_aggregate_metric_double with multi_values:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ aggregate_metric_double_v0 ]\n+          capabilities: [ aggregate_metric_double_convert_to ]\n       reason: \"Support for to_aggregate_metric_double function\"\n \n   - do:\n@@ -769,7 +725,7 @@ avg of aggregate_metric_double:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_avg, dense_vector_agg_metric_double_if_version]\n       reason: \"support avg aggregations with aggregate metric double\"\n \n   - do:\n@@ -777,11 +733,7 @@ avg of aggregate_metric_double:\n         - \"No limit defined, adding default limit of \\\\[.*\\\\]\"\n       esql.query:\n         body:\n-          query: |\n-            FROM test2\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | STATS avg = avg(agg_metric)\n-            | KEEP avg\n+          query: 'FROM test2 | STATS avg = avg(agg_metric) | KEEP avg'\n \n   - length: {values: 1}\n   - length: {values.0: 1}\ndiff --git a/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_unsupported_types.yml b/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_unsupported_types.yml\nindex 453d86a1bdf35..e015e73f503b2 100644\n--- a/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_unsupported_types.yml\n+++ b/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/40_unsupported_types.yml\n@@ -145,8 +145,8 @@ unsupported:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [dense_vector_field_type_released, dense_vector_agg_metric_double_if_fns]\n-      reason: \"uses original_type\"\n+          capabilities: [dense_vector_agg_metric_double_if_version]\n+      reason: \"fetches dense_vector\"\n \n   - do:\n       allowed_warnings_regex:\n@@ -157,7 +157,8 @@ unsupported:\n           query: 'from test'\n \n   - match: { columns.0.name: aggregate_metric_double }\n-  - match: { columns.0.type: unsupported }\n+  - match: { columns.0.type: aggregate_metric_double }\n+  - is_false: columns.0.original_types\n   - match: { columns.1.name: binary }\n   - match: { columns.1.type: unsupported }\n   - match: { columns.1.original_types: [binary] }\n@@ -169,7 +170,7 @@ unsupported:\n   - match: { columns.4.name: date_range }\n   - match: { columns.4.type: unsupported }\n   - match: { columns.5.name: dense_vector }\n-  - match: { columns.5.type: unsupported }\n+  - match: { columns.5.type: dense_vector }\n   - match: { columns.6.name: double_range }\n   - match: { columns.6.type: unsupported }\n   - match: { columns.7.name: float_range }\n@@ -218,12 +219,14 @@ unsupported:\n   - match: { columns.28.type: integer }\n \n   - length: { values: 1 }\n-  - match: { values.0.0: null }\n+  - match: { values.0.0: '{\"min\":1.0,\"max\":3.0,\"sum\":10.1,\"value_count\":5}' }\n   - match: { values.0.1: null }\n   - match: { values.0.2: null }\n   - match: { values.0.3: \"2015-01-01T12:10:30.123456789Z\" }\n   - match: { values.0.4: null }\n-  - match: { values.0.5: null }\n+  - match: { values.0.5.0: 0.5 }\n+  - match: { values.0.5.1: 10.0 }\n+  - match: { values.0.5.2: 6.0 }\n   - match: { values.0.6: null }\n   - match: { values.0.7: null }\n   - match: { values.0.8: \"POINT (10.0 12.0)\" }\n@@ -255,7 +258,8 @@ unsupported:\n         body:\n           query: 'from test | limit 0'\n   - match: { columns.0.name: aggregate_metric_double }\n-  - match: { columns.0.type: unsupported }\n+  - match: { columns.0.type: aggregate_metric_double }\n+  - is_false: columns.0.original_types\n   - match: { columns.1.name: binary }\n   - match: { columns.1.type: unsupported }\n   - match: { columns.1.original_types: [binary] }\n@@ -267,7 +271,7 @@ unsupported:\n   - match: { columns.4.name: date_range }\n   - match: { columns.4.type: unsupported }\n   - match: { columns.5.name: dense_vector }\n-  - match: { columns.5.type: unsupported }\n+  - match: { columns.5.type: dense_vector }\n   - match: { columns.6.name: double_range }\n   - match: { columns.6.type: unsupported }\n   - match: { columns.7.name: float_range }\n@@ -338,7 +342,7 @@ unsupported with sort:\n         - method: POST\n           path: /_query\n           parameters: [ ]\n-          capabilities: [ dense_vector_field_type_released, dense_vector_agg_metric_double_if_fns ]\n+          capabilities: [ dense_vector_agg_metric_double_if_version ]\n       reason: \"support for sorting when dense_vector_field_type present\"\n \n   - do:\n@@ -350,7 +354,7 @@ unsupported with sort:\n           query: 'from test | sort some_doc.bar'\n \n   - match: { columns.0.name: aggregate_metric_double }\n-  - match: { columns.0.type: unsupported }\n+  - match: { columns.0.type: aggregate_metric_double }\n   - match: { columns.1.name: binary }\n   - match: { columns.1.type: unsupported }\n   - match: { columns.2.name: completion }\n@@ -360,7 +364,7 @@ unsupported with sort:\n   - match: { columns.4.name: date_range }\n   - match: { columns.4.type: unsupported }\n   - match: { columns.5.name: dense_vector }\n-  - match: { columns.5.type: unsupported }\n+  - match: { columns.5.type: dense_vector }\n   - match: { columns.6.name: double_range }\n   - match: { columns.6.type: unsupported }\n   - match: { columns.7.name: float_range }\n@@ -409,12 +413,14 @@ unsupported with sort:\n   - match: { columns.28.type: integer }\n \n   - length: { values: 1 }\n-  - match: { values.0.0: null }\n+  - match: { values.0.0: '{\"min\":1.0,\"max\":3.0,\"sum\":10.1,\"value_count\":5}' }\n   - match: { values.0.1: null }\n   - match: { values.0.2: null }\n   - match: { values.0.3: \"2015-01-01T12:10:30.123456789Z\" }\n   - match: { values.0.4: null }\n-  - match: { values.0.5: null }\n+  - match: { values.0.5.0: 0.5 }\n+  - match: { values.0.5.1: 10.0 }\n+  - match: { values.0.5.2: 6.0 }\n   - match: { values.0.6: null }\n   - match: { values.0.7: null }\n   - match: { values.0.8: \"POINT (10.0 12.0)\" }\n@@ -438,6 +444,7 @@ unsupported with sort:\n   - match: { values.0.26: xy }\n   - match: { values.0.27: \"foo bar\" }\n   - match: { values.0.28: 3 }\n+\n ---\n nested declared inline:\n   - do:\ndiff --git a/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/46_downsample.yml b/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/46_downsample.yml\nindex ac219b4071319..6ae4ce894d997 100644\n--- a/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/46_downsample.yml\n+++ b/x-pack/plugin/src/yamlRestTest/resources/rest-api-spec/test/esql/46_downsample.yml\n@@ -83,7 +83,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0, dense_vector_agg_metric_double_if_fns]\n+          capabilities: [aggregate_metric_double, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for aggregate_metric_double\"\n   - do:\n       indices.downsample:\n@@ -98,12 +98,9 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-downsample\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx)\n-            | LIMIT 100\n+          query: \"FROM test-downsample |\n+          STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx)\n+          | LIMIT 100\"\n \n   - length: {values: 1}\n   - length: {values.0: 4}\n@@ -128,7 +125,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_rendering, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for rendering aggregate_metric_doubles\"\n   - do:\n       indices.downsample:\n@@ -143,13 +140,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-downsample\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | WHERE @timestamp == \"2021-04-28T19:00:00.000Z\"\n-            | KEEP k8s.pod.network.rx\n-            | LIMIT 100\n+          query: \"FROM test-downsample | WHERE @timestamp == \\\"2021-04-28T19:00:00.000Z\\\" | KEEP k8s.pod.network.rx | LIMIT 100\"\n   - length: {values: 1}\n   - length: {values.0: 1}\n   - match: {columns.0.name: \"k8s.pod.network.rx\"}\n@@ -164,7 +155,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_convert_to, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for to_aggregate_metric_double function\"\n \n   - do:\n@@ -240,12 +231,11 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-*\n-            | WHERE k8s.pod.uid == \"947e4ced-1786-4e53-9e0c-5c447e959507\"\n-            | EVAL rx = to_aggregate_metric_double(k8s.pod.network.rx)\n-            | STATS max(rx), min(rx), sum(rx), count(rx)\n-            | LIMIT 100\n+          query: \"FROM test-* |\n+          WHERE k8s.pod.uid == \\\"947e4ced-1786-4e53-9e0c-5c447e959507\\\" |\n+          EVAL rx = to_aggregate_metric_double(k8s.pod.network.rx) |\n+          STATS max(rx), min(rx), sum(rx), count(rx) |\n+          LIMIT 100\"\n \n   - length: {values: 1}\n   - length: {values.0: 4}\n@@ -270,7 +260,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [metrics_capability, aggregate_metric_double_implicit_casting_in_aggs, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for casting aggregate metric double implicitly when present in aggregations\"\n \n   - do:\n@@ -346,12 +336,10 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-*\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | WHERE k8s.pod.uid == \"947e4ced-1786-4e53-9e0c-5c447e959507\"\n-            | STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx), avg(k8s.pod.network.rx)\n-            | LIMIT 100\n+          query: \"FROM test-* |\n+          WHERE k8s.pod.uid == \\\"947e4ced-1786-4e53-9e0c-5c447e959507\\\" |\n+          STATS max(k8s.pod.network.rx), min(k8s.pod.network.rx), sum(k8s.pod.network.rx), count(k8s.pod.network.rx), avg(k8s.pod.network.rx) |\n+          LIMIT 100\"\n \n   - length: {values: 1}\n   - length: {values.0: 5}\n@@ -374,7 +362,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: \"FROM test-* | STATS max = max(k8s.pod.network.rx)  | LIMIT 100\"\n+          query: \"TS test-* | STATS max = max(k8s.pod.network.rx)  | LIMIT 100\"\n   - length: {values: 1}\n   - length: {values.0: 1}\n   - match: {columns.0.name: \"max\"}\n@@ -389,7 +377,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [ts_command_v0, aggregate_metric_double_v0]\n+          capabilities: [metrics_command, aggregate_metric_double_implicit_casting_in_aggs, dense_vector_agg_metric_double_if_version]\n       reason: \"Support for casting aggregate metric double implicitly when present in aggregations\"\n \n   - do:\n@@ -469,14 +457,12 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            TS test-*\n-            | STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n-                    count = sum(count_over_time(k8s.pod.network.rx)),\n-                    sum = sum(sum_over_time(k8s.pod.network.rx))\n-                 BY time_bucket = bucket(@timestamp, 1 hour)\n-            | SORT time_bucket\n-            | LIMIT 10\n+          query: \"TS test-* |\n+          STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n+                count = sum(count_over_time(k8s.pod.network.rx)),\n+                sum = sum(sum_over_time(k8s.pod.network.rx))\n+                BY time_bucket = bucket(@timestamp, 1 hour) |\n+          SORT time_bucket | LIMIT 10\"\n \n   - length: {values: 4}\n   - length: {values.0: 4}\n@@ -513,7 +499,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [ts_command_v0, aggregate_metric_double_v0]\n+          capabilities: [metrics_command, aggregate_metric_double_implicit_casting_in_aggs]\n       reason: \"Support for casting aggregate metric double implicitly when present in aggregations\"\n \n   - do:\n@@ -593,14 +579,13 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            TS test-*\n-            | STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n-                    count = sum(count_over_time(k8s.pod.network.rx)),\n-                    sum = sum(sum_over_time(k8s.pod.network.rx))\n-                 BY k8s.pod.name, time_bucket = bucket(@timestamp, 1 hour)\n-            | SORT time_bucket, k8s.pod.name\n-            |LIMIT 10\n+          query: \"TS test-* |\n+          STATS avg = sum(avg_over_time(k8s.pod.network.rx)),\n+                count = sum(count_over_time(k8s.pod.network.rx)),\n+                sum = sum(sum_over_time(k8s.pod.network.rx))\n+          BY k8s.pod.name, time_bucket = bucket(@timestamp, 1 hour) |\n+          SORT time_bucket, k8s.pod.name |\n+          LIMIT 10\"\n \n   - length: {values: 6}\n   - length: {values.0: 5}\n@@ -653,7 +638,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_sorting_fixed]\n       reason: \"Fix sorting for rows comprised of docs from multiple indices where agg metric is missing from some\"\n \n   - do:\n@@ -686,13 +671,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-*\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | SORT some_field, @timestamp, k8s.pod.uid\n-            | KEEP k8s.pod.network.rx, some_field, @timestamp\n-            | LIMIT 10\n+          query: \"FROM test-* | SORT some_field, @timestamp, k8s.pod.uid | KEEP k8s.pod.network.rx, some_field, @timestamp | LIMIT 10\"\n \n   - length: {values: 5}\n   - length: {values.0: 3}\n@@ -726,7 +705,7 @@ setup:\n         - method: POST\n           path: /_query\n           parameters: []\n-          capabilities: [aggregate_metric_double_v0]\n+          capabilities: [aggregate_metric_double_mv_expand, dense_vector_agg_metric_double_if_version]\n       reason: \"Have MV_EXPAND not error out when applied to aggregate_metric_doubles (is a no-op)\"\n \n   - do:\n@@ -742,14 +721,7 @@ setup:\n   - do:\n       esql.query:\n         body:\n-          query: |\n-            FROM test-downsample\n-            | EVAL a = TO_AGGREGATE_METRIC_DOUBLE(1)  // Temporary workaround to enable aggregate_metric_double\n-            | DROP a\n-            | MV_EXPAND k8s.pod.network.rx\n-            | SORT @timestamp, k8s.pod.uid\n-            | KEEP k8s.pod.network.rx, @timestamp\n-            | LIMIT 10\n+          query: \"FROM test-downsample | MV_EXPAND k8s.pod.network.rx | SORT @timestamp, k8s.pod.uid | KEEP k8s.pod.network.rx, @timestamp | LIMIT 10\"\n \n   - length: {values: 4}\n   - length: {values.0: 2}\n",
  "additions": 358,
  "deletions": 307,
  "changed_files": 21,
  "url": "https://github.com/elastic/elasticsearch/pull/136327",
  "mined_at": "2025-10-25T13:36:36.117296"
}