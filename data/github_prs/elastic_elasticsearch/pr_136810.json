{
  "id": 136810,
  "repository": "elastic/elasticsearch",
  "title": "Introduce basic support for synthetic ids",
  "body": "This change introduces basic support for the synthetic id feature, in which document identifiers (`_id`) are not indexed but instead computed at runtime from other document fields (`_tsid` and `@timestamp`).\r\n\r\nThis pull request adds a new `tsdb_synthetic_id` flag along with a new `index.mapping.use_synthetic_id` index setting that can only be applied to time series indices. When both are enabled, the index uses a specialized Lucene codec that ensures the `_id` field is never indexed, and also overrides the field informations when a segment is opened to expose the `_id` field as if it has postings. This way it is still possible to soft-update Lucene documents using a `term` value while not paying the price of indexing such a field.  Note that the synthetic `_id` field is still declared as stored since we plan to use the stored value in the future in a bloom filter. \r\n\r\nThis change also introduces a new posting format to support synthetic _id. This implementation is not important for now: it only exists to demonstrate that doc values updates can work with a synthetic id posting format. The implementation uses the current _id format of TSDB indices which is hashed from a routing hash value, the tsid and the timestamp. This format cannot be unhashed to extract the real _tsid value and is also not naturally sorted, something that Lucene expects when applying doc values updates. To work around this the integration test only updates the first documents of the segments. In a follow up we'd like to change the _id format for TSDS to be [_tsid, @timestamp] and maybe also include the routing hash?.\r\n\r\nI'm opening this change for review in order to:\r\n- have approval that synthetic _id only targets time-series datastream (TSDS) for now\r\n- have feedback on the TSDB codec, field infos change and other flag/setting naming\r\n- have feedback on why updates are blocked (but not deletes?)\r\n\r\nRelates #136304",
  "state": "closed",
  "merged": true,
  "merged_at": "2025-10-23T14:57:17+00:00",
  "created_at": "2025-10-20T12:18:48+00:00",
  "updated_at": "2025-10-24T07:07:55+00:00",
  "author": "tlrx",
  "reviewers": [
    "martijnvg",
    "tlrx",
    "kkrik-es",
    "fcofdez"
  ],
  "base_sha": "3b1bdc92ac1885ae8e2cdb0906e7db94123063cc",
  "head_sha": "e262e5901a0856f0e92892b25bf04b405d3162f2",
  "review_comments": [
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T10:42:26+00:00"
    },
    {
      "user": "fcofdez",
      "state": "COMMENTED",
      "body": "Looks good, I left a few minor comments and questions üëç ",
      "submitted_at": "2025-10-21T11:59:17+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:13:13+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:16:07+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:20:22+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:20:30+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:20:59+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:22:10+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:30:25+00:00"
    },
    {
      "user": "fcofdez",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-21T14:42:47+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-22T08:13:27+00:00"
    },
    {
      "user": "fcofdez",
      "state": "COMMENTED",
      "body": "This looks great, I think that we can mark this ready for review",
      "submitted_at": "2025-10-22T08:20:21+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-22T08:29:21+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T06:10:34+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T06:13:26+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T06:20:08+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T06:24:22+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T06:24:57+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T06:32:59+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "APPROVED",
      "body": "Well done! Please wait for Martijn to take a look as well.",
      "submitted_at": "2025-10-23T06:43:01+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T06:48:37+00:00"
    },
    {
      "user": "martijnvg",
      "state": "APPROVED",
      "body": "Thanks @tlrx! This looks good to me and first good step on avoid storing _id for tsdb.",
      "submitted_at": "2025-10-23T08:25:21+00:00"
    },
    {
      "user": "fcofdez",
      "state": "APPROVED",
      "body": "LGTM",
      "submitted_at": "2025-10-23T10:29:24+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T11:49:36+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T11:51:29+00:00"
    },
    {
      "user": "kkrik-es",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T11:53:40+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T12:09:07+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T12:10:37+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T12:10:47+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T12:10:51+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T12:10:56+00:00"
    },
    {
      "user": "tlrx",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-23T12:11:15+00:00"
    },
    {
      "user": "martijnvg",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-24T07:07:55+00:00"
    }
  ],
  "pr_comments": [
    {
      "user": "tlrx",
      "body": "@martijnvg @kkrik-es I added both of you as reviewers, I'm leaving it to you to decide who can help reviewing this :)\r\n\r\nThanks!",
      "created_at": "2025-10-22T08:23:44+00:00"
    },
    {
      "user": "tlrx",
      "body": "Thanks all!",
      "created_at": "2025-10-23T14:57:31+00:00"
    }
  ],
  "files_changed": [
    {
      "filename": "modules/data-streams/src/internalClusterTest/java/org/elasticsearch/datastreams/TSDBSyntheticIdsIT.java",
      "status": "added",
      "additions": 279,
      "deletions": 0,
      "changes": 279,
      "patch": "@@ -0,0 +1,279 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.datastreams;\n+\n+import org.apache.lucene.tests.util.LuceneTestCase;\n+import org.elasticsearch.action.DocWriteRequest;\n+import org.elasticsearch.action.DocWriteResponse;\n+import org.elasticsearch.action.admin.indices.diskusage.AnalyzeIndexDiskUsageRequest;\n+import org.elasticsearch.action.admin.indices.diskusage.AnalyzeIndexDiskUsageTestUtils;\n+import org.elasticsearch.action.admin.indices.diskusage.IndexDiskUsageStats;\n+import org.elasticsearch.action.admin.indices.diskusage.TransportAnalyzeIndexDiskUsageAction;\n+import org.elasticsearch.action.admin.indices.template.put.TransportPutComposableIndexTemplateAction;\n+import org.elasticsearch.action.bulk.BulkItemResponse;\n+import org.elasticsearch.cluster.metadata.ComposableIndexTemplate;\n+import org.elasticsearch.cluster.metadata.Template;\n+import org.elasticsearch.common.compress.CompressedXContent;\n+import org.elasticsearch.common.time.DateFormatter;\n+import org.elasticsearch.index.IndexMode;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.mapper.IdFieldMapper;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.elasticsearch.xcontent.XContentBuilder;\n+import org.elasticsearch.xcontent.XContentFactory;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import static org.elasticsearch.common.time.FormatNames.STRICT_DATE_OPTIONAL_TIME;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.notNullValue;\n+\n+/**\n+ * Test suite for time series indices that use synthetic ids for documents.\n+ * <p>\n+ * Synthetic _id fields are not indexed in Lucene, instead they are generated on demand by concatenating the values of two other fields of\n+ * the document (typically the {@code @timestamp} and {@code _tsid} fields).\n+ * </p>\n+ */\n+@LuceneTestCase.SuppressCodecs(\"*\") // requires codecs used in production only\n+public class TSDBSyntheticIdsIT extends ESIntegTestCase {\n+\n+    private static final DateFormatter DATE_FORMATTER = DateFormatter.forPattern(STRICT_DATE_OPTIONAL_TIME.getName());\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        var plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(InternalSettingsPlugin.class);\n+        plugins.add(DataStreamsPlugin.class);\n+        return plugins;\n+    }\n+\n+    public void testInvalidIndexMode() {\n+        assumeTrue(\"Test should only run with feature flag\", IndexSettings.TSDB_SYNTHETIC_ID_FEATURE_FLAG);\n+        final var indexName = randomIdentifier();\n+        var randomNonTsdbIndexMode = randomValueOtherThan(IndexMode.TIME_SERIES, () -> randomFrom(IndexMode.values()));\n+\n+        var exception = expectThrows(\n+            IllegalArgumentException.class,\n+            () -> createIndex(\n+                indexName,\n+                indexSettings(1, 0).put(IndexSettings.MODE.getKey(), randomNonTsdbIndexMode)\n+                    .put(IndexSettings.USE_SYNTHETIC_ID.getKey(), true)\n+                    .build()\n+            )\n+        );\n+        assertThat(\n+            exception.getMessage(),\n+            containsString(\n+                \"The setting [\"\n+                    + IndexSettings.USE_SYNTHETIC_ID.getKey()\n+                    + \"] is only permitted when [index.mode] is set to [TIME_SERIES]. Current mode: [\"\n+                    + randomNonTsdbIndexMode.getName().toUpperCase(Locale.ROOT)\n+                    + \"].\"\n+            )\n+        );\n+    }\n+\n+    @TestLogging(reason = \"debug\", value = \"org.elasticsearch.index.engine.Engine:TRACE\")\n+    public void testSyntheticId() throws Exception {\n+        assumeTrue(\"Test should only run with feature flag\", IndexSettings.TSDB_SYNTHETIC_ID_FEATURE_FLAG);\n+        final var indexName = randomIdentifier();\n+        putDataStreamTemplate(random(), indexName);\n+\n+        final var timestamp = Instant.now();\n+\n+        // Index 5 docs in datastream\n+        var results = createDocuments(\n+            indexName,\n+            document(timestamp, \"vm-dev01\", \"cpu-load\", 0),                                // will be updated\n+            document(timestamp.plusSeconds(2), \"vm-dev01\", \"cpu-load\", 1),    // will be deleted\n+            document(timestamp, \"vm-dev02\", \"cpu-load\", 2),\n+            document(timestamp.plusSeconds(2), \"vm-dev03\", \"cpu-load\", 3),\n+            document(timestamp.plusSeconds(3), \"vm-dev03\", \"cpu-load\", 4)\n+        );\n+\n+        // Verify documents\n+        assertThat(results[0].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[0].getVersion(), equalTo(1L));\n+\n+        assertThat(results[1].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[1].getVersion(), equalTo(1L));\n+\n+        assertThat(results[2].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[2].getVersion(), equalTo(1L));\n+\n+        assertThat(results[3].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[3].getVersion(), equalTo(1L));\n+\n+        assertThat(results[4].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[4].getVersion(), equalTo(1L));\n+\n+        final var docIndex = results[1].getIndex();\n+        final var docId = results[1].getId();\n+\n+        enum Operation {\n+            FLUSH,\n+            REFRESH,\n+            NONE\n+        }\n+        switch (randomFrom(Operation.values())) {\n+            case FLUSH:\n+                flush(indexName);\n+                break;\n+            case REFRESH:\n+                refresh(indexName);\n+                break;\n+            case NONE:\n+            default:\n+                break;\n+        }\n+\n+        // Get by synthetic _id\n+        // Note: before synthetic _id this would have required postings on disks\n+        var getResponse = client().prepareGet(docIndex, docId).setFetchSource(true).execute().actionGet();\n+        assertThat(getResponse.isExists(), equalTo(true));\n+        assertThat(getResponse.getVersion(), equalTo(1L));\n+        var source = asInstanceOf(Map.class, getResponse.getSourceAsMap().get(\"metric\"));\n+        assertThat(asInstanceOf(Integer.class, source.get(\"value\")), equalTo(1));\n+\n+        // Update by synthetic _id\n+        // Note: it doesn't work, is that expected? Is is blocked by IndexRouting.ExtractFromSource.updateShard\n+        var exception = expectThrows(IllegalArgumentException.class, () -> {\n+            var doc = document(timestamp, \"vm-dev01\", \"cpu-load\", 10); // update\n+            client().prepareUpdate(docIndex, docId).setDoc(doc).get();\n+        });\n+        assertThat(\n+            exception.getMessage(),\n+            containsString(\"update is not supported because the destination index [\" + docIndex + \"] is in time_series mode\")\n+        );\n+\n+        // Delete by synthetic _id\n+        var deleteResponse = client().prepareDelete(docIndex, docId).get();\n+        assertThat(deleteResponse.getId(), equalTo(docId));\n+        assertThat(deleteResponse.getResult(), equalTo(DocWriteResponse.Result.DELETED));\n+        assertThat(deleteResponse.getVersion(), equalTo(2L));\n+\n+        // Index more docs\n+        // TODO Randomize this to have segments only composed of deleted docs\n+        createDocuments(\n+            indexName,\n+            document(timestamp.plusSeconds(4), \"vm-dev03\", \"cpu-load\", 5),\n+            document(timestamp.plusSeconds(5), \"vm-dev03\", \"cpu-load\", 6)\n+        );\n+\n+        flushAndRefresh(indexName);\n+\n+        // Check that synthetic _id field has no postings on disk\n+        var diskUsage = diskUsage(docIndex);\n+        var diskUsageIdField = AnalyzeIndexDiskUsageTestUtils.getPerFieldDiskUsage(diskUsage, IdFieldMapper.NAME);\n+        assertThat(\"_id field should not have postings on disk\", diskUsageIdField.getInvertedIndexBytes(), equalTo(0L));\n+\n+        // TODO Search datastream and count hits\n+    }\n+\n+    private static XContentBuilder document(Instant timestamp, String hostName, String metricField, Integer metricValue)\n+        throws IOException {\n+        var source = XContentFactory.jsonBuilder();\n+        source.startObject();\n+        {\n+            source.field(\"@timestamp\", DATE_FORMATTER.format(timestamp));\n+            source.field(\"hostname\", hostName);\n+            source.startObject(\"metric\");\n+            {\n+                source.field(\"field\", metricField);\n+                source.field(\"value\", metricValue);\n+\n+            }\n+            source.endObject();\n+        }\n+        source.endObject();\n+        return source;\n+    }\n+\n+    private static BulkItemResponse[] createDocuments(String indexName, XContentBuilder... docs) throws IOException {\n+        assertThat(docs, notNullValue());\n+        final var client = client();\n+        var bulkRequest = client.prepareBulk();\n+        for (var doc : docs) {\n+            bulkRequest.add(client.prepareIndex(indexName).setOpType(DocWriteRequest.OpType.CREATE).setSource(doc));\n+        }\n+        var bulkResponse = bulkRequest.get();\n+        assertNoFailures(bulkResponse);\n+        return bulkResponse.getItems();\n+    }\n+\n+    private static void putDataStreamTemplate(Random random, String indexPattern) throws IOException {\n+        final var settings = indexSettings(1, 0).put(IndexSettings.MODE.getKey(), IndexMode.TIME_SERIES.getName())\n+            .put(IndexSettings.BLOOM_FILTER_ID_FIELD_ENABLED_SETTING.getKey(), false)\n+            .put(IndexSettings.INDEX_REFRESH_INTERVAL_SETTING.getKey(), -1)\n+            .put(IndexSettings.USE_SYNTHETIC_ID.getKey(), true);\n+\n+        final var mappings = \"\"\"\n+            {\n+                \"_doc\": {\n+                    \"properties\": {\n+                        \"@timestamp\": {\n+                            \"type\": \"date\"\n+                        },\n+                        \"hostname\": {\n+                            \"type\": \"keyword\",\n+                            \"time_series_dimension\": true\n+                        },\n+                        \"metric\": {\n+                            \"properties\": {\n+                                \"field\": {\n+                                    \"type\": \"keyword\",\n+                                    \"time_series_dimension\": true\n+                                },\n+                                \"value\": {\n+                                    \"type\": \"integer\",\n+                                    \"time_series_metric\": \"counter\"\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\"\"\";\n+\n+        var putTemplateRequest = new TransportPutComposableIndexTemplateAction.Request(getTestClass().getName().toLowerCase(Locale.ROOT))\n+            .indexTemplate(\n+                ComposableIndexTemplate.builder()\n+                    .indexPatterns(List.of(indexPattern))\n+                    .template(new Template(settings.build(), new CompressedXContent(mappings), null))\n+                    .dataStreamTemplate(new ComposableIndexTemplate.DataStreamTemplate(false, false))\n+                    .build()\n+            );\n+        assertAcked(client().execute(TransportPutComposableIndexTemplateAction.TYPE, putTemplateRequest).actionGet());\n+    }\n+\n+    private static IndexDiskUsageStats diskUsage(String indexName) {\n+        var diskUsageResponse = client().execute(\n+            TransportAnalyzeIndexDiskUsageAction.TYPE,\n+            new AnalyzeIndexDiskUsageRequest(new String[] { indexName }, AnalyzeIndexDiskUsageRequest.DEFAULT_INDICES_OPTIONS, false)\n+        ).actionGet();\n+\n+        var indexDiskUsageStats = AnalyzeIndexDiskUsageTestUtils.getIndexStats(diskUsageResponse, indexName);\n+        assertNotNull(indexDiskUsageStats);\n+        return indexDiskUsageStats;\n+    }\n+}"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java",
      "status": "modified",
      "additions": 3,
      "deletions": 0,
      "changes": 3,
      "patch": "@@ -246,6 +246,9 @@ public final class IndexScopedSettings extends AbstractScopedSettings {\n         if (IndexSettings.DOC_VALUES_SKIPPER) {\n             settings.add(IndexSettings.USE_DOC_VALUES_SKIPPER);\n         }\n+        if (IndexSettings.TSDB_SYNTHETIC_ID_FEATURE_FLAG) {\n+            settings.add(IndexSettings.USE_SYNTHETIC_ID);\n+        }\n         settings.add(IndexSettings.INDEX_MAPPING_EXCLUDE_SOURCE_VECTORS_SETTING);\n         BUILT_IN_INDEX_SETTINGS = Collections.unmodifiableSet(settings);\n     };"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/IndexSettings.java",
      "status": "modified",
      "additions": 48,
      "deletions": 0,
      "changes": 48,
      "patch": "@@ -676,6 +676,44 @@ public boolean isES87TSDBCodecEnabled() {\n         Property.Final\n     );\n \n+    public static final boolean TSDB_SYNTHETIC_ID_FEATURE_FLAG = new FeatureFlag(\"tsdb_synthetic_id\").isEnabled();\n+    public static final Setting<Boolean> USE_SYNTHETIC_ID = Setting.boolSetting(\n+        \"index.mapping.use_synthetic_id\",\n+        false,\n+        new Setting.Validator<>() {\n+            @Override\n+            public void validate(Boolean value) {}\n+\n+            @Override\n+            public void validate(Boolean enabled, Map<Setting<?>, Object> settings) {\n+                if (enabled) {\n+                    // Verify if index mode is TIME_SERIES\n+                    var indexMode = (IndexMode) settings.get(MODE);\n+                    if (indexMode != IndexMode.TIME_SERIES) {\n+                        throw new IllegalArgumentException(\n+                            String.format(\n+                                Locale.ROOT,\n+                                \"The setting [%s] is only permitted when [%s] is set to [%s]. Current mode: [%s].\",\n+                                USE_SYNTHETIC_ID.getKey(),\n+                                MODE.getKey(),\n+                                IndexMode.TIME_SERIES.name(),\n+                                indexMode.name()\n+                            )\n+                        );\n+                    }\n+                }\n+            }\n+\n+            @Override\n+            public Iterator<Setting<?>> settings() {\n+                List<Setting<?>> list = List.of(MODE);\n+                return list.iterator();\n+            }\n+        },\n+        Property.IndexScope,\n+        Property.Final\n+    );\n+\n     /**\n      * The {@link IndexMode \"mode\"} of the index.\n      */\n@@ -937,6 +975,7 @@ private void setRetentionLeaseMillis(final TimeValue retentionLease) {\n     private final boolean recoverySourceEnabled;\n     private final boolean recoverySourceSyntheticEnabled;\n     private final boolean useDocValuesSkipper;\n+    private final boolean tsdbSyntheticId;\n \n     /**\n      * The maximum number of refresh listeners allows on this shard.\n@@ -1123,6 +1162,8 @@ public IndexSettings(final IndexMetadata indexMetadata, final Settings nodeSetti\n             && scopedSettings.get(RECOVERY_USE_SYNTHETIC_SOURCE_SETTING);\n         useDocValuesSkipper = DOC_VALUES_SKIPPER && scopedSettings.get(USE_DOC_VALUES_SKIPPER);\n         seqNoIndexOptions = scopedSettings.get(SEQ_NO_INDEX_OPTIONS_SETTING);\n+        tsdbSyntheticId = TSDB_SYNTHETIC_ID_FEATURE_FLAG && scopedSettings.get(USE_SYNTHETIC_ID);\n+        assert tsdbSyntheticId == false || mode == IndexMode.TIME_SERIES : mode;\n         if (recoverySourceSyntheticEnabled) {\n             if (DiscoveryNode.isStateless(settings)) {\n                 throw new IllegalArgumentException(\"synthetic recovery source is only allowed in stateful\");\n@@ -1855,6 +1896,13 @@ public boolean useDocValuesSkipper() {\n         return useDocValuesSkipper;\n     }\n \n+    /**\n+     * @return Whether the index is a time-series index that use synthetic ids.\n+     */\n+    public boolean useTsdbSyntheticId() {\n+        return tsdbSyntheticId;\n+    }\n+\n     /**\n      * The bounds for {@code @timestamp} on this index or\n      * {@code null} if there are no bounds."
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/codec/CodecService.java",
      "status": "modified",
      "additions": 14,
      "deletions": 4,
      "changes": 18,
      "patch": "@@ -16,6 +16,8 @@\n import org.elasticsearch.common.util.BigArrays;\n import org.elasticsearch.common.util.FeatureFlag;\n import org.elasticsearch.core.Nullable;\n+import org.elasticsearch.index.IndexMode;\n+import org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdCodec;\n import org.elasticsearch.index.codec.zstd.Zstd814StoredFieldsFormat;\n import org.elasticsearch.index.mapper.MapperService;\n \n@@ -65,12 +67,20 @@ public CodecService(@Nullable MapperService mapperService, BigArrays bigArrays)\n         for (String codec : Codec.availableCodecs()) {\n             codecs.put(codec, Codec.forName(codec));\n         }\n+        final boolean useTsdbSyntheticId = mapperService != null && mapperService.getIndexSettings().useTsdbSyntheticId();\n+        assert useTsdbSyntheticId == false || mapperService.getIndexSettings().getMode() == IndexMode.TIME_SERIES;\n+\n         this.codecs = codecs.entrySet().stream().collect(Collectors.toUnmodifiableMap(Map.Entry::getKey, e -> {\n-            var codec = e.getValue();\n-            if (codec instanceof DeduplicateFieldInfosCodec) {\n-                return codec;\n+            Codec codec;\n+            if (e.getValue() instanceof DeduplicateFieldInfosCodec dedupCodec) {\n+                codec = dedupCodec;\n+            } else {\n+                codec = new DeduplicateFieldInfosCodec(e.getValue().getName(), e.getValue());\n+            }\n+            if (useTsdbSyntheticId && codec instanceof TSDBSyntheticIdCodec == false) {\n+                codec = new TSDBSyntheticIdCodec(codec.getName(), codec);\n             }\n-            return new DeduplicateFieldInfosCodec(codec.getName(), codec);\n+            return codec;\n         }));\n     }\n "
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdCodec.java",
      "status": "added",
      "additions": 158,
      "deletions": 0,
      "changes": 158,
      "patch": "@@ -0,0 +1,158 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.codec.tsdb;\n+\n+import org.apache.lucene.codecs.Codec;\n+import org.apache.lucene.codecs.FieldInfosFormat;\n+import org.apache.lucene.codecs.FilterCodec;\n+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;\n+import org.apache.lucene.index.FieldInfo;\n+import org.apache.lucene.index.FieldInfos;\n+import org.apache.lucene.index.IndexOptions;\n+import org.apache.lucene.index.SegmentInfo;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.IOContext;\n+import org.elasticsearch.index.mapper.SyntheticIdField;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.SYNTHETIC_ID;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TIMESTAMP;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TS_ID;\n+\n+/**\n+ * Special codec for time-series datastreams that use synthetic ids.\n+ * <p>\n+ *     The role of this codec is to ensure that no inverted index is created when indexing a document id in Lucene, while allowing the usage\n+ *     of terms and postings on the field (now called a \"synthetic _id\" field) as if it was backed by an in inverted index.\n+ * </p>\n+ * <p>\n+ *     In order to do this, it enforces synthetic _id fields to be indexed with the {@link IndexOptions#NONE} option, hence preventing the\n+ *     building of a term dictionary with postings lists. The codec also changes this {@link IndexOptions#NONE} option back to\n+ *     {@link IndexOptions#DOCS} when reading the {@link FieldInfos} during the opening of a new segment core reader. This allows to use a\n+ *     Lucene term dictionary on top of a synthetic _id field that does not have corresponding postings files on disk. Finally, the codec\n+ *     injects additional {@link FieldInfos} attributes so that Lucene's {@link PerFieldPostingsFormat} correctly instantiates a\n+ *     {@link TSDBSyntheticIdPostingsFormat} to access the term and postings of the synthetic _id field.\n+ * </p>\n+ */\n+public class TSDBSyntheticIdCodec extends FilterCodec {\n+\n+    private final TSDBSyntheticIdFieldInfosFormat fieldInfosFormat;\n+\n+    public TSDBSyntheticIdCodec(String name, Codec delegate) {\n+        super(name, delegate);\n+        this.fieldInfosFormat = new TSDBSyntheticIdFieldInfosFormat(delegate.fieldInfosFormat());\n+    }\n+\n+    @Override\n+    public final FieldInfosFormat fieldInfosFormat() {\n+        return fieldInfosFormat;\n+    }\n+\n+    /**\n+     * {@link FieldInfosFormat} that ensures the _id field is synthetic\n+     */\n+    private static class TSDBSyntheticIdFieldInfosFormat extends FieldInfosFormat {\n+\n+        private final FieldInfosFormat delegate;\n+\n+        private TSDBSyntheticIdFieldInfosFormat(FieldInfosFormat delegate) {\n+            this.delegate = delegate;\n+        }\n+\n+        private void ensureSyntheticIdFields(FieldInfos fieldInfos) {\n+            // Ensure _tsid exists\n+            var fi = fieldInfos.fieldInfo(TS_ID);\n+            if (fi == null) {\n+                var message = \"Field [\" + TS_ID + \"] does not exist\";\n+                assert false : message;\n+                throw new IllegalArgumentException(message);\n+            }\n+            // Ensure @timestamp exists\n+            fi = fieldInfos.fieldInfo(TIMESTAMP);\n+            if (fi == null) {\n+                var message = \"Field [\" + TIMESTAMP + \"] does not exist\";\n+                assert false : message;\n+                throw new IllegalArgumentException(message);\n+            }\n+            // Ensure _id exists and not indexed\n+            fi = fieldInfos.fieldInfo(SYNTHETIC_ID);\n+            if (fi == null) {\n+                var message = \"Field [\" + SYNTHETIC_ID + \"] does not exist\";\n+                assert false : message;\n+                throw new IllegalArgumentException(message);\n+            }\n+            if (fi.getIndexOptions() != IndexOptions.NONE) {\n+                assert false;\n+                throw new IllegalArgumentException(\"Field [\" + SYNTHETIC_ID + \"] has incorrect index options\");\n+            }\n+            if (SyntheticIdField.hasSyntheticIdAttributes(fi.attributes()) == false) {\n+                throw new IllegalArgumentException(\"Field [\" + SYNTHETIC_ID + \"] is not synthetic\");\n+            }\n+        }\n+\n+        @Override\n+        public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos fieldInfos, IOContext context)\n+            throws IOException {\n+            ensureSyntheticIdFields(fieldInfos);\n+            delegate.write(directory, segmentInfo, segmentSuffix, fieldInfos, context);\n+        }\n+\n+        @Override\n+        public FieldInfos read(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, IOContext iocontext) throws IOException {\n+            final var fieldInfos = delegate.read(directory, segmentInfo, segmentSuffix, iocontext);\n+            ensureSyntheticIdFields(fieldInfos);\n+\n+            // Change the _id field index options from IndexOptions.NONE to IndexOptions.DOCS, so that terms and postings work when\n+            // applying doc values updates in Lucene.\n+            final var infos = new FieldInfo[fieldInfos.size()];\n+            int i = 0;\n+            for (FieldInfo fi : fieldInfos) {\n+                if (SYNTHETIC_ID.equals(fi.getName())) {\n+                    final var attributes = new HashMap<>(fi.attributes());\n+\n+                    // Assert that PerFieldPostingsFormat are not written to field infos on disk\n+                    assert attributes.containsKey(PerFieldPostingsFormat.PER_FIELD_FORMAT_KEY) == false;\n+                    assert attributes.containsKey(PerFieldPostingsFormat.PER_FIELD_SUFFIX_KEY) == false;\n+\n+                    // Inject attributes so that PerFieldPostingsFormat maps the synthetic _id field to the TSDBSyntheticIdPostingsFormat\n+                    // This would normally be handled transparently by PerFieldPostingsFormat, but such attributes are only added if terms\n+                    // are produced during indexing, which is not the case for the synthetic _id field.\n+                    attributes.put(PerFieldPostingsFormat.PER_FIELD_FORMAT_KEY, TSDBSyntheticIdPostingsFormat.FORMAT_NAME);\n+                    attributes.put(PerFieldPostingsFormat.PER_FIELD_SUFFIX_KEY, TSDBSyntheticIdPostingsFormat.SUFFIX);\n+\n+                    fi = new FieldInfo(\n+                        fi.getName(),\n+                        fi.getFieldNumber(),\n+                        fi.hasTermVectors(),\n+                        true,\n+                        fi.hasPayloads(),\n+                        IndexOptions.DOCS,\n+                        fi.getDocValuesType(),\n+                        fi.docValuesSkipIndexType(),\n+                        fi.getDocValuesGen(),\n+                        attributes,\n+                        fi.getPointDimensionCount(),\n+                        fi.getPointIndexDimensionCount(),\n+                        fi.getPointNumBytes(),\n+                        fi.getVectorDimension(),\n+                        fi.getVectorEncoding(),\n+                        fi.getVectorSimilarityFunction(),\n+                        fi.isSoftDeletesField(),\n+                        fi.isParentField()\n+                    );\n+                }\n+                infos[i++] = fi;\n+            }\n+            return new FieldInfos(infos);\n+        }\n+    }\n+}"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdFieldsProducer.java",
      "status": "added",
      "additions": 390,
      "deletions": 0,
      "changes": 390,
      "patch": "@@ -0,0 +1,390 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.codec.tsdb;\n+\n+import org.apache.lucene.codecs.DocValuesProducer;\n+import org.apache.lucene.codecs.FieldsProducer;\n+import org.apache.lucene.index.BaseTermsEnum;\n+import org.apache.lucene.index.FieldInfos;\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.SegmentReadState;\n+import org.apache.lucene.index.SortedDocValues;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.core.IOUtils;\n+import org.elasticsearch.index.mapper.TimeSeriesRoutingHashFieldMapper;\n+import org.elasticsearch.index.mapper.TsidExtractingIdFieldMapper;\n+import org.elasticsearch.index.mapper.Uid;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Iterator;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.SYNTHETIC_ID;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TIMESTAMP;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TS_ID;\n+\n+public class TSDBSyntheticIdFieldsProducer extends FieldsProducer {\n+\n+    private static final Set<String> FIELDS_NAMES = Set.of(SYNTHETIC_ID);\n+\n+    private final DocValuesProducer docValuesProducer;\n+    private final FieldInfos fieldInfos;\n+    private final int maxDocs;\n+\n+    public TSDBSyntheticIdFieldsProducer(SegmentReadState state, DocValuesProducer docValuesProducer) {\n+        this(state.fieldInfos, docValuesProducer, state.segmentInfo.maxDoc());\n+    }\n+\n+    private TSDBSyntheticIdFieldsProducer(FieldInfos fieldInfos, DocValuesProducer docValuesProducer, int maxDocs) {\n+        assert assertFieldInfosExist(fieldInfos, SYNTHETIC_ID, TIMESTAMP, TS_ID);\n+        this.docValuesProducer = Objects.requireNonNull(docValuesProducer);\n+        this.fieldInfos = fieldInfos;\n+        this.maxDocs = maxDocs;\n+    }\n+\n+    @Override\n+    public int size() {\n+        return FIELDS_NAMES.size();\n+    }\n+\n+    @Override\n+    public Iterator<String> iterator() {\n+        return FIELDS_NAMES.iterator();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        IOUtils.close(docValuesProducer);\n+    }\n+\n+    @Override\n+    public void checkIntegrity() throws IOException {}\n+\n+    @Override\n+    public FieldsProducer getMergeInstance() {\n+        return new TSDBSyntheticIdFieldsProducer(fieldInfos, docValuesProducer, maxDocs);\n+    }\n+\n+    @Override\n+    public Terms terms(String field) throws IOException {\n+        assert FIELDS_NAMES.contains(field) : field;\n+        return new Terms() {\n+            @Override\n+            public TermsEnum iterator() {\n+                return new FakeTermsEnum();\n+            }\n+\n+            @Override\n+            public int getDocCount() {\n+                return maxDocs - 1; // All docs have a synthetic id\n+            }\n+\n+            @Override\n+            public long size() {\n+                return -1; // Number of terms is unknown\n+            }\n+\n+            @Override\n+            public long getSumTotalTermFreq() {\n+                return 0;\n+            }\n+\n+            @Override\n+            public long getSumDocFreq() {\n+                return 0;\n+            }\n+\n+            @Override\n+            public boolean hasFreqs() {\n+                return false;\n+            }\n+\n+            @Override\n+            public boolean hasOffsets() {\n+                return false;\n+            }\n+\n+            @Override\n+            public boolean hasPositions() {\n+                return false;\n+            }\n+\n+            @Override\n+            public boolean hasPayloads() {\n+                return false;\n+            }\n+        };\n+    }\n+\n+    /**\n+     * This is a fake TermsEnum that scans all documents for find docs matching a specific _id. This implementation is only here to show\n+     * that the synthetic _id terms is used when applying doc values updates during soft-updates. It is buggy and should not be used besides\n+     * some carefully crafted integration tests, because it relies on the current _id format for TSDB indices that has limitations:\n+     * - it is composed of a routing hash, a @timestamp and a tsid that cannot be un-hashed so all docs must be scanned to find matchings\n+     * - it is not sorted on _id in the Lucene segments so doc values updates stop too early when applying DV updates\n+     *\n+     * This fake terms enumeration will be changed to support a different _id format in a short future.\n+     */\n+    private class FakeTermsEnum extends BaseTermsEnum {\n+\n+        private BytesRef term = null;\n+        private int docID = -1;\n+\n+        private BytesRef latestTsId = null;\n+        private long latestTimestamp = -1L;\n+\n+        private FakeTermsEnum() {}\n+\n+        @Override\n+        public BytesRef next() throws IOException {\n+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n+                assert term == null;\n+                return null;\n+            }\n+            docID += 1;\n+            if (maxDocs <= docID) {\n+                docID = DocIdSetIterator.NO_MORE_DOCS;\n+                latestTimestamp = -1L;\n+                latestTsId = null;\n+                term = null;\n+                return null;\n+            }\n+\n+            // Retrieve _tsid\n+            SortedDocValues tsIdDocValues = docValuesProducer.getSorted(fieldInfos.fieldInfo(TS_ID));\n+            boolean found = tsIdDocValues.advanceExact(docID);\n+            assert found;\n+            int tsIdOrd = tsIdDocValues.ordValue();\n+            BytesRef tsId = tsIdDocValues.lookupOrd(tsIdOrd);\n+            assert tsId != null;\n+\n+            // Retrieve timestamp\n+            SortedNumericDocValues timestampDocValues = docValuesProducer.getSortedNumeric(fieldInfos.fieldInfo(TIMESTAMP));\n+            found = timestampDocValues.advanceExact(docID);\n+            assert found;\n+            assert timestampDocValues.docValueCount() == 1;\n+            long timestamp = timestampDocValues.nextValue();\n+\n+            // Retrieve routing hash\n+            var tsRoutingHash = fieldInfos.fieldInfo(TimeSeriesRoutingHashFieldMapper.NAME);\n+            assert tsRoutingHash != null;\n+            SortedDocValues routingHashDocValues = docValuesProducer.getSorted(tsRoutingHash);\n+            found = routingHashDocValues.advanceExact(docID);\n+            assert found;\n+            BytesRef routingHashBytes = routingHashDocValues.lookupOrd(routingHashDocValues.ordValue());\n+\n+            int routingHash = TimeSeriesRoutingHashFieldMapper.decode(\n+                Uid.decodeId(routingHashBytes.bytes, routingHashBytes.offset, routingHashBytes.length)\n+            );\n+            term = Uid.encodeId(TsidExtractingIdFieldMapper.createId(routingHash, tsId, timestamp));\n+            latestTimestamp = timestamp;\n+            latestTsId = tsId;\n+            return term;\n+        }\n+\n+        @Override\n+        public SeekStatus seekCeil(BytesRef id) {\n+            assert id != null;\n+            if (term != null && term.equals(id)) {\n+                return SeekStatus.FOUND;\n+            }\n+            try {\n+                while (next() != null) {\n+                    if (term.equals(id)) {\n+                        return SeekStatus.FOUND;\n+                    }\n+                }\n+            } catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+            return SeekStatus.END;\n+        }\n+\n+        @Override\n+        public BytesRef term() {\n+            return term;\n+        }\n+\n+        @Override\n+        public PostingsEnum postings(PostingsEnum reuse, int flags) {\n+            return new FakePostingsEnum(docID, latestTsId, latestTimestamp, maxDocs);\n+        }\n+\n+        /**\n+         * This is an optional method as per the {@link TermsEnum#ord()} documentation that is not supported by the current implementation.\n+         * This method always throws an {@link UnsupportedOperationException}.\n+         */\n+        @Override\n+        public long ord() {\n+            throw unsupportedException();\n+        }\n+\n+        /**\n+         * This is an optional method as per the {@link TermsEnum#ord()} documentation that is not supported by the current implementation.\n+         * This method always throws an {@link UnsupportedOperationException}.\n+         */\n+        @Override\n+        public void seekExact(long ord) {\n+            throw unsupportedException();\n+        }\n+\n+        @Override\n+        public int docFreq() throws IOException {\n+            return 0;\n+        }\n+\n+        @Override\n+        public long totalTermFreq() throws IOException {\n+            return 0;\n+        }\n+\n+        @Override\n+        public ImpactsEnum impacts(int flags) throws IOException {\n+            return null;\n+        }\n+    }\n+\n+    /**\n+     * Do not use in production. See {@link FakeTermsEnum}.\n+     */\n+    private class FakePostingsEnum extends PostingsEnum {\n+\n+        private final int startDocID;\n+        private final BytesRef latestTsId;\n+        private final long latestTimestamp;\n+        private final int maxDocs;\n+        private int docID;\n+\n+        private FakePostingsEnum(int docID, BytesRef latestTsId, long latestTimestamp, int maxDocs) {\n+            this.startDocID = docID;\n+            this.latestTsId = latestTsId;\n+            this.latestTimestamp = latestTimestamp;\n+            this.maxDocs = maxDocs;\n+            this.docID = -1;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return docID;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n+                return docID;\n+            } else if (docID == -1) {\n+                docID = startDocID;\n+            } else {\n+                docID = docID + 1;\n+                if (maxDocs <= docID) {\n+                    docID = DocIdSetIterator.NO_MORE_DOCS;\n+                    return docID;\n+                }\n+            }\n+\n+            // Retrieve _tsid\n+            SortedDocValues tsIdDocValues = docValuesProducer.getSorted(fieldInfos.fieldInfo(TS_ID));\n+            boolean found = tsIdDocValues.advanceExact(docID);\n+            assert found;\n+            int tsIdOrd = tsIdDocValues.ordValue();\n+            BytesRef tsId = tsIdDocValues.lookupOrd(tsIdOrd);\n+            assert tsId != null;\n+\n+            if (latestTsId != null && latestTsId.equals(tsId) == false) {\n+                // Different _tsid, stop here\n+                docID = DocIdSetIterator.NO_MORE_DOCS;\n+                return docID;\n+            }\n+\n+            // Retrieve timestamp\n+            SortedNumericDocValues timestampDocValues = docValuesProducer.getSortedNumeric(fieldInfos.fieldInfo(TIMESTAMP));\n+            found = timestampDocValues.advanceExact(docID);\n+            assert found;\n+            assert timestampDocValues.docValueCount() == 1;\n+            long timestamp = timestampDocValues.nextValue();\n+\n+            if (latestTimestamp != -1L && latestTimestamp != timestamp) {\n+                // Different @timestamp, stop here\n+                docID = DocIdSetIterator.NO_MORE_DOCS;\n+                return docID;\n+            }\n+\n+            // Retrieve routing hash\n+            var tsRoutingHash = fieldInfos.fieldInfo(TimeSeriesRoutingHashFieldMapper.NAME);\n+            assert tsRoutingHash != null;\n+            SortedDocValues routingHashDocValues = docValuesProducer.getSorted(tsRoutingHash);\n+            found = routingHashDocValues.advanceExact(docID);\n+            assert found;\n+            BytesRef routingHashBytes = routingHashDocValues.lookupOrd(routingHashDocValues.ordValue());\n+            assert routingHashBytes != null;\n+            return docID;\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            int doc;\n+            while ((doc = nextDoc()) < target) {\n+                // Continue\n+            }\n+            return doc;\n+        }\n+\n+        @Override\n+        public long cost() {\n+            return 0L;\n+        }\n+\n+        @Override\n+        public int freq() throws IOException {\n+            return 0; // not supported\n+        }\n+\n+        @Override\n+        public int nextPosition() throws IOException {\n+            return -1; // not supported\n+        }\n+\n+        @Override\n+        public int startOffset() throws IOException {\n+            return -1; // not supported\n+        }\n+\n+        @Override\n+        public int endOffset() throws IOException {\n+            return -1; // not supported\n+        }\n+\n+        @Override\n+        public BytesRef getPayload() throws IOException {\n+            return null; // not supported\n+        }\n+    }\n+\n+    private static boolean assertFieldInfosExist(FieldInfos fieldInfos, String... fieldNames) {\n+        assert fieldNames != null && fieldNames.length > 0 : \"fieldNames should be > 0\";\n+        for (var fieldName : fieldNames) {\n+            assert fieldInfos.fieldInfo(fieldName) != null : \"field [\" + fieldName + \"] not found\";\n+        }\n+        return true;\n+    }\n+\n+    private static UnsupportedOperationException unsupportedException() {\n+        var error = \"This method should not be called on this enum\";\n+        assert false : error;\n+        return new UnsupportedOperationException(error);\n+    }\n+}"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdPostingsFormat.java",
      "status": "added",
      "additions": 61,
      "deletions": 0,
      "changes": 61,
      "patch": "@@ -0,0 +1,61 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.codec.tsdb;\n+\n+import org.apache.lucene.codecs.DocValuesProducer;\n+import org.apache.lucene.codecs.FieldsConsumer;\n+import org.apache.lucene.codecs.FieldsProducer;\n+import org.apache.lucene.codecs.PostingsFormat;\n+import org.apache.lucene.index.SegmentReadState;\n+import org.apache.lucene.index.SegmentWriteState;\n+import org.elasticsearch.core.IOUtils;\n+import org.elasticsearch.index.mapper.DataStreamTimestampFieldMapper;\n+import org.elasticsearch.index.mapper.SyntheticIdField;\n+import org.elasticsearch.index.mapper.TimeSeriesIdFieldMapper;\n+\n+import java.io.IOException;\n+\n+public class TSDBSyntheticIdPostingsFormat extends PostingsFormat {\n+\n+    public static final String SYNTHETIC_ID = SyntheticIdField.NAME;\n+    public static final String TIMESTAMP = DataStreamTimestampFieldMapper.DEFAULT_PATH;\n+    public static final String TS_ID = TimeSeriesIdFieldMapper.NAME;\n+\n+    static final String FORMAT_NAME = \"TSDBSyntheticId\";\n+    static final String SUFFIX = \"0\";\n+\n+    public TSDBSyntheticIdPostingsFormat() {\n+        super(FORMAT_NAME);\n+    }\n+\n+    @Override\n+    public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n+        DocValuesProducer docValuesProducer = null;\n+        boolean success = false;\n+        try {\n+            var codec = state.segmentInfo.getCodec();\n+            // Erase the segment suffix (used only for reading postings)\n+            docValuesProducer = codec.docValuesFormat().fieldsProducer(new SegmentReadState(state, \"\"));\n+            var fieldsProducer = new TSDBSyntheticIdFieldsProducer(state, docValuesProducer);\n+            success = true;\n+            return fieldsProducer;\n+        } finally {\n+            if (success == false) {\n+                IOUtils.close(docValuesProducer);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {\n+        assert false : \"this should never be called\";\n+        throw new UnsupportedOperationException();\n+    }\n+}"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java",
      "status": "modified",
      "additions": 18,
      "deletions": 0,
      "changes": 18,
      "patch": "@@ -231,6 +231,8 @@ public class InternalEngine extends Engine {\n \n     private final ByteSizeValue totalDiskSpace;\n \n+    private final boolean useTsdbSyntheticId;\n+\n     protected static final String REAL_TIME_GET_REFRESH_SOURCE = \"realtime_get\";\n     protected static final String UNSAFE_VERSION_MAP_REFRESH_SOURCE = \"unsafe_version_map\";\n \n@@ -243,6 +245,12 @@ public InternalEngine(EngineConfig engineConfig) {\n     InternalEngine(EngineConfig engineConfig, int maxDocs, BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier) {\n         super(engineConfig);\n         this.maxDocs = maxDocs;\n+        if (engineConfig.getIndexSettings().useTsdbSyntheticId()) {\n+            logger.info(\"using TSDB with synthetic id\");\n+            useTsdbSyntheticId = true;\n+        } else {\n+            useTsdbSyntheticId = false;\n+        }\n         this.relativeTimeInNanosSupplier = config().getRelativeTimeInNanosSupplier();\n         this.lastFlushTimestamp = relativeTimeInNanosSupplier.getAsLong(); // default to creation timestamp\n         this.liveVersionMapArchive = createLiveVersionMapArchive();\n@@ -1434,6 +1442,7 @@ private IndexResult indexIntoLucene(Index index, IndexingStrategy plan) throws I\n         index.parsedDoc().updateSeqID(index.seqNo(), index.primaryTerm());\n         index.parsedDoc().version().setLongValue(plan.versionForIndexing);\n         try {\n+            logDocumentsDetails(index.docs());\n             if (plan.addStaleOpToLucene) {\n                 addStaleDocs(index.docs(), indexWriter);\n             } else if (plan.useLuceneUpdateDocument) {\n@@ -1469,6 +1478,14 @@ && treatDocumentFailureAsTragicError(index) == false) {\n         }\n     }\n \n+    private void logDocumentsDetails(List<LuceneDocument> docs) {\n+        if (useTsdbSyntheticId && logger.isTraceEnabled()) {\n+            for (var doc : docs) {\n+                logger.trace(\"indexing document fields [{}]\", doc.getFields());\n+            }\n+        }\n+    }\n+\n     /**\n      * Whether we should treat any document failure as tragic error.\n      * If we hit any failure while processing an indexing on a replica, we should treat that error as tragic and fail the engine.\n@@ -1842,6 +1859,7 @@ private DeleteResult deleteInLucene(Delete delete, DeletionStrategy plan) throws\n         try {\n             final ParsedDocument tombstone = ParsedDocument.deleteTombstone(\n                 engineConfig.getIndexSettings().seqNoIndexOptions(),\n+                useTsdbSyntheticId,\n                 delete.id()\n             );\n             assert tombstone.docs().size() == 1 : \"Tombstone doc should have single doc [\" + tombstone + \"]\";"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/mapper/IdFieldMapper.java",
      "status": "modified",
      "additions": 7,
      "deletions": 0,
      "changes": 7,
      "patch": "@@ -78,6 +78,13 @@ public static Field standardIdField(String id) {\n         return new StringField(NAME, Uid.encodeId(id), Field.Store.YES);\n     }\n \n+    /**\n+     * Create a {@link Field} corresponding to a synthetic {@code _id} field, which is not indexed but instead resolved at runtime.\n+     */\n+    public static Field syntheticIdField(String id) {\n+        return new SyntheticIdField(Uid.encodeId(id));\n+    }\n+\n     protected abstract static class AbstractIdFieldType extends TermBasedFieldType {\n \n         public AbstractIdFieldType() {"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java",
      "status": "modified",
      "additions": 19,
      "deletions": 2,
      "changes": 21,
      "patch": "@@ -70,15 +70,32 @@ public static ParsedDocument noopTombstone(SeqNoFieldMapper.SeqNoIndexOptions se\n     /**\n      * Create a delete tombstone document, which will be used in soft-update methods.\n      * The returned document consists only _uid, _seqno, _term and _version fields; other metadata fields are excluded.\n-     * @param id    the id of the deleted document\n+     * @param id                the id of the deleted document\n      */\n     public static ParsedDocument deleteTombstone(SeqNoFieldMapper.SeqNoIndexOptions seqNoIndexOptions, String id) {\n+        return deleteTombstone(seqNoIndexOptions, false, id);\n+    }\n+\n+    /**\n+     * Create a delete tombstone document, which will be used in soft-update methods.\n+     * The returned document consists only _uid, _seqno, _term and _version fields; other metadata fields are excluded.\n+     * @param useSyntheticId    whether the id is synthetic or not\n+     * @param id                the id of the deleted document\n+     */\n+    public static ParsedDocument deleteTombstone(SeqNoFieldMapper.SeqNoIndexOptions seqNoIndexOptions, boolean useSyntheticId, String id) {\n         LuceneDocument document = new LuceneDocument();\n         SeqNoFieldMapper.SequenceIDFields seqIdFields = SeqNoFieldMapper.SequenceIDFields.tombstone(seqNoIndexOptions);\n         seqIdFields.addFields(document);\n         Field versionField = VersionFieldMapper.versionField();\n         document.add(versionField);\n-        document.add(IdFieldMapper.standardIdField(id));\n+        if (useSyntheticId) {\n+            // Use a synthetic _id field which is not indexed nor stored\n+            document.add(IdFieldMapper.syntheticIdField(id));\n+            // TODO I think we also need to add the fields that compose the synthetic _id.\n+        } else {\n+            // Use standard _id field (indexed and stored, some indices also trim the stored field at some point)\n+            document.add(IdFieldMapper.standardIdField(id));\n+        }\n         return new ParsedDocument(\n             versionField,\n             seqIdFields,"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/mapper/SyntheticIdField.java",
      "status": "added",
      "additions": 67,
      "deletions": 0,
      "changes": 67,
      "patch": "@@ -0,0 +1,67 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.mapper;\n+\n+import org.apache.lucene.analysis.Analyzer;\n+import org.apache.lucene.analysis.TokenStream;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.FieldType;\n+import org.apache.lucene.index.IndexOptions;\n+import org.apache.lucene.util.BytesRef;\n+\n+import java.util.Map;\n+\n+public final class SyntheticIdField extends Field {\n+\n+    public static final String NAME = IdFieldMapper.NAME;\n+\n+    private static final String ENABLED_ATTRIBUTE_KEY = SyntheticIdField.class.getSimpleName() + \".enabled\";\n+    private static final String ENABLED_ATTRIBUTE_VALUE = Boolean.TRUE.toString();\n+\n+    private static final FieldType TYPE;\n+    static {\n+        TYPE = new FieldType();\n+        TYPE.putAttribute(ENABLED_ATTRIBUTE_KEY, ENABLED_ATTRIBUTE_VALUE);\n+        // Make sure the field is not indexed, but this option is changed at runtime\n+        // in FieldInfos so that the field can use terms and postings.\n+        TYPE.setIndexOptions(IndexOptions.NONE);\n+        TYPE.setTokenized(false);\n+        TYPE.setOmitNorms(true);\n+        // The field is marked as stored, but storage on disk might be skipped\n+        TYPE.setStored(true);\n+        TYPE.freeze();\n+    }\n+\n+    public SyntheticIdField(BytesRef bytes) {\n+        super(NAME, bytes, TYPE);\n+    }\n+\n+    @Override\n+    public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n+        assert false : \"this should never be called\";\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void setTokenStream(TokenStream tokenStream) {\n+        assert false : \"this should never be called\";\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static boolean hasSyntheticIdAttributes(Map<String, String> attributes) {\n+        if (attributes != null) {\n+            var attributeValue = attributes.get(SyntheticIdField.ENABLED_ATTRIBUTE_KEY);\n+            if (attributeValue != null) {\n+                return SyntheticIdField.ENABLED_ATTRIBUTE_VALUE.equals(attributeValue);\n+            }\n+        }\n+        return false;\n+    }\n+}"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/index/mapper/TsidExtractingIdFieldMapper.java",
      "status": "modified",
      "additions": 12,
      "deletions": 4,
      "changes": 16,
      "patch": "@@ -10,7 +10,6 @@\n package org.elasticsearch.index.mapper;\n \n import org.apache.lucene.document.Field;\n-import org.apache.lucene.document.StringField;\n import org.apache.lucene.index.IndexableField;\n import org.apache.lucene.util.BytesRef;\n import org.elasticsearch.cluster.routing.IndexRouting;\n@@ -91,11 +90,20 @@ public static BytesRef createField(DocumentParserContext context, RoutingHashBui\n                 )\n             );\n         }\n+        assert id != null;\n         context.id(id);\n \n-        BytesRef uidEncoded = Uid.encodeId(context.id());\n-        context.doc().add(new StringField(NAME, uidEncoded, Field.Store.YES));\n-        return uidEncoded;\n+        final Field idField;\n+        if (context.indexSettings().useTsdbSyntheticId()) {\n+            idField = syntheticIdField(context.id());\n+        } else {\n+            idField = standardIdField(context.id());\n+        }\n+        assert NAME.equals(idField.name()) : idField.name();\n+        assert idField.binaryValue() != null;\n+\n+        context.doc().add(idField);\n+        return idField.binaryValue();\n     }\n \n     public static String createId(int routingHash, BytesRef tsid, long timestamp) {"
    },
    {
      "filename": "server/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat",
      "status": "modified",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "patch": "@@ -1,3 +1,4 @@\n org.elasticsearch.index.codec.bloomfilter.ES85BloomFilterPostingsFormat\n org.elasticsearch.index.codec.bloomfilter.ES87BloomFilterPostingsFormat\n org.elasticsearch.index.codec.postings.ES812PostingsFormat\n+org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat"
    },
    {
      "filename": "server/src/test/java/org/elasticsearch/common/lucene/uid/VersionLookupTests.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "patch": "@@ -158,7 +158,8 @@ public void testLoadTimestampRange() throws Exception {\n     public void testLoadTimestampRangeWithDeleteTombstone() throws Exception {\n         Directory dir = newDirectory();\n         IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Lucene.STANDARD_ANALYZER).setMergePolicy(NoMergePolicy.INSTANCE));\n-        writer.addDocument(ParsedDocument.deleteTombstone(randomFrom(SeqNoFieldMapper.SeqNoIndexOptions.values()), \"_id\").docs().get(0));\n+        var randomSeqNoIndexOptions = randomFrom(SeqNoFieldMapper.SeqNoIndexOptions.values());\n+        writer.addDocument(ParsedDocument.deleteTombstone(randomSeqNoIndexOptions, false, \"_id\").docs().get(0));\n         DirectoryReader reader = DirectoryReader.open(writer);\n         LeafReaderContext segment = reader.leaves().get(0);\n         PerThreadIDVersionAndSeqNoLookup lookup = new PerThreadIDVersionAndSeqNoLookup(segment.reader(), true);"
    },
    {
      "filename": "server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "patch": "@@ -4558,7 +4558,7 @@ public void testOnCloseStats() throws IOException {\n     public void testSupplyTombstoneDoc() throws Exception {\n         IndexShard shard = newStartedShard();\n         String id = randomRealisticUnicodeOfLengthBetween(1, 10);\n-        ParsedDocument deleteTombstone = ParsedDocument.deleteTombstone(shard.indexSettings.seqNoIndexOptions(), id);\n+        ParsedDocument deleteTombstone = ParsedDocument.deleteTombstone(shard.indexSettings.seqNoIndexOptions(), randomBoolean(), id);\n         assertThat(deleteTombstone.docs(), hasSize(1));\n         LuceneDocument deleteDoc = deleteTombstone.docs().get(0);\n         assertThat("
    },
    {
      "filename": "test/framework/src/main/java/org/elasticsearch/action/admin/indices/diskusage/AnalyzeIndexDiskUsageTestUtils.java",
      "status": "added",
      "additions": 40,
      "deletions": 0,
      "changes": 40,
      "patch": "@@ -0,0 +1,40 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.action.admin.indices.diskusage;\n+\n+import org.elasticsearch.core.Nullable;\n+\n+public class AnalyzeIndexDiskUsageTestUtils {\n+\n+    private AnalyzeIndexDiskUsageTestUtils() {}\n+\n+    @Nullable\n+    public static IndexDiskUsageStats getIndexStats(final AnalyzeIndexDiskUsageResponse diskUsageResponse, final String indexName) {\n+        var stats = diskUsageResponse.getStats();\n+        if (stats != null) {\n+            return stats.get(indexName);\n+        }\n+        return null;\n+    }\n+\n+    @Nullable\n+    public static IndexDiskUsageStats.PerFieldDiskUsage getPerFieldDiskUsage(\n+        final IndexDiskUsageStats indexDiskUsageStats,\n+        final String fieldName\n+    ) {\n+        if (indexDiskUsageStats != null) {\n+            var fields = indexDiskUsageStats.getFields();\n+            if (fields != null) {\n+                return fields.get(fieldName);\n+            }\n+        }\n+        return null;\n+    }\n+}"
    }
  ],
  "diff": "diff --git a/modules/data-streams/src/internalClusterTest/java/org/elasticsearch/datastreams/TSDBSyntheticIdsIT.java b/modules/data-streams/src/internalClusterTest/java/org/elasticsearch/datastreams/TSDBSyntheticIdsIT.java\nnew file mode 100644\nindex 0000000000000..b0d14d0d80221\n--- /dev/null\n+++ b/modules/data-streams/src/internalClusterTest/java/org/elasticsearch/datastreams/TSDBSyntheticIdsIT.java\n@@ -0,0 +1,279 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.datastreams;\n+\n+import org.apache.lucene.tests.util.LuceneTestCase;\n+import org.elasticsearch.action.DocWriteRequest;\n+import org.elasticsearch.action.DocWriteResponse;\n+import org.elasticsearch.action.admin.indices.diskusage.AnalyzeIndexDiskUsageRequest;\n+import org.elasticsearch.action.admin.indices.diskusage.AnalyzeIndexDiskUsageTestUtils;\n+import org.elasticsearch.action.admin.indices.diskusage.IndexDiskUsageStats;\n+import org.elasticsearch.action.admin.indices.diskusage.TransportAnalyzeIndexDiskUsageAction;\n+import org.elasticsearch.action.admin.indices.template.put.TransportPutComposableIndexTemplateAction;\n+import org.elasticsearch.action.bulk.BulkItemResponse;\n+import org.elasticsearch.cluster.metadata.ComposableIndexTemplate;\n+import org.elasticsearch.cluster.metadata.Template;\n+import org.elasticsearch.common.compress.CompressedXContent;\n+import org.elasticsearch.common.time.DateFormatter;\n+import org.elasticsearch.index.IndexMode;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.mapper.IdFieldMapper;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.elasticsearch.xcontent.XContentBuilder;\n+import org.elasticsearch.xcontent.XContentFactory;\n+\n+import java.io.IOException;\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Random;\n+\n+import static org.elasticsearch.common.time.FormatNames.STRICT_DATE_OPTIONAL_TIME;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.notNullValue;\n+\n+/**\n+ * Test suite for time series indices that use synthetic ids for documents.\n+ * <p>\n+ * Synthetic _id fields are not indexed in Lucene, instead they are generated on demand by concatenating the values of two other fields of\n+ * the document (typically the {@code @timestamp} and {@code _tsid} fields).\n+ * </p>\n+ */\n+@LuceneTestCase.SuppressCodecs(\"*\") // requires codecs used in production only\n+public class TSDBSyntheticIdsIT extends ESIntegTestCase {\n+\n+    private static final DateFormatter DATE_FORMATTER = DateFormatter.forPattern(STRICT_DATE_OPTIONAL_TIME.getName());\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        var plugins = new ArrayList<>(super.nodePlugins());\n+        plugins.add(InternalSettingsPlugin.class);\n+        plugins.add(DataStreamsPlugin.class);\n+        return plugins;\n+    }\n+\n+    public void testInvalidIndexMode() {\n+        assumeTrue(\"Test should only run with feature flag\", IndexSettings.TSDB_SYNTHETIC_ID_FEATURE_FLAG);\n+        final var indexName = randomIdentifier();\n+        var randomNonTsdbIndexMode = randomValueOtherThan(IndexMode.TIME_SERIES, () -> randomFrom(IndexMode.values()));\n+\n+        var exception = expectThrows(\n+            IllegalArgumentException.class,\n+            () -> createIndex(\n+                indexName,\n+                indexSettings(1, 0).put(IndexSettings.MODE.getKey(), randomNonTsdbIndexMode)\n+                    .put(IndexSettings.USE_SYNTHETIC_ID.getKey(), true)\n+                    .build()\n+            )\n+        );\n+        assertThat(\n+            exception.getMessage(),\n+            containsString(\n+                \"The setting [\"\n+                    + IndexSettings.USE_SYNTHETIC_ID.getKey()\n+                    + \"] is only permitted when [index.mode] is set to [TIME_SERIES]. Current mode: [\"\n+                    + randomNonTsdbIndexMode.getName().toUpperCase(Locale.ROOT)\n+                    + \"].\"\n+            )\n+        );\n+    }\n+\n+    @TestLogging(reason = \"debug\", value = \"org.elasticsearch.index.engine.Engine:TRACE\")\n+    public void testSyntheticId() throws Exception {\n+        assumeTrue(\"Test should only run with feature flag\", IndexSettings.TSDB_SYNTHETIC_ID_FEATURE_FLAG);\n+        final var indexName = randomIdentifier();\n+        putDataStreamTemplate(random(), indexName);\n+\n+        final var timestamp = Instant.now();\n+\n+        // Index 5 docs in datastream\n+        var results = createDocuments(\n+            indexName,\n+            document(timestamp, \"vm-dev01\", \"cpu-load\", 0),                                // will be updated\n+            document(timestamp.plusSeconds(2), \"vm-dev01\", \"cpu-load\", 1),    // will be deleted\n+            document(timestamp, \"vm-dev02\", \"cpu-load\", 2),\n+            document(timestamp.plusSeconds(2), \"vm-dev03\", \"cpu-load\", 3),\n+            document(timestamp.plusSeconds(3), \"vm-dev03\", \"cpu-load\", 4)\n+        );\n+\n+        // Verify documents\n+        assertThat(results[0].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[0].getVersion(), equalTo(1L));\n+\n+        assertThat(results[1].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[1].getVersion(), equalTo(1L));\n+\n+        assertThat(results[2].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[2].getVersion(), equalTo(1L));\n+\n+        assertThat(results[3].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[3].getVersion(), equalTo(1L));\n+\n+        assertThat(results[4].getResponse().getResult(), equalTo(DocWriteResponse.Result.CREATED));\n+        assertThat(results[4].getVersion(), equalTo(1L));\n+\n+        final var docIndex = results[1].getIndex();\n+        final var docId = results[1].getId();\n+\n+        enum Operation {\n+            FLUSH,\n+            REFRESH,\n+            NONE\n+        }\n+        switch (randomFrom(Operation.values())) {\n+            case FLUSH:\n+                flush(indexName);\n+                break;\n+            case REFRESH:\n+                refresh(indexName);\n+                break;\n+            case NONE:\n+            default:\n+                break;\n+        }\n+\n+        // Get by synthetic _id\n+        // Note: before synthetic _id this would have required postings on disks\n+        var getResponse = client().prepareGet(docIndex, docId).setFetchSource(true).execute().actionGet();\n+        assertThat(getResponse.isExists(), equalTo(true));\n+        assertThat(getResponse.getVersion(), equalTo(1L));\n+        var source = asInstanceOf(Map.class, getResponse.getSourceAsMap().get(\"metric\"));\n+        assertThat(asInstanceOf(Integer.class, source.get(\"value\")), equalTo(1));\n+\n+        // Update by synthetic _id\n+        // Note: it doesn't work, is that expected? Is is blocked by IndexRouting.ExtractFromSource.updateShard\n+        var exception = expectThrows(IllegalArgumentException.class, () -> {\n+            var doc = document(timestamp, \"vm-dev01\", \"cpu-load\", 10); // update\n+            client().prepareUpdate(docIndex, docId).setDoc(doc).get();\n+        });\n+        assertThat(\n+            exception.getMessage(),\n+            containsString(\"update is not supported because the destination index [\" + docIndex + \"] is in time_series mode\")\n+        );\n+\n+        // Delete by synthetic _id\n+        var deleteResponse = client().prepareDelete(docIndex, docId).get();\n+        assertThat(deleteResponse.getId(), equalTo(docId));\n+        assertThat(deleteResponse.getResult(), equalTo(DocWriteResponse.Result.DELETED));\n+        assertThat(deleteResponse.getVersion(), equalTo(2L));\n+\n+        // Index more docs\n+        // TODO Randomize this to have segments only composed of deleted docs\n+        createDocuments(\n+            indexName,\n+            document(timestamp.plusSeconds(4), \"vm-dev03\", \"cpu-load\", 5),\n+            document(timestamp.plusSeconds(5), \"vm-dev03\", \"cpu-load\", 6)\n+        );\n+\n+        flushAndRefresh(indexName);\n+\n+        // Check that synthetic _id field has no postings on disk\n+        var diskUsage = diskUsage(docIndex);\n+        var diskUsageIdField = AnalyzeIndexDiskUsageTestUtils.getPerFieldDiskUsage(diskUsage, IdFieldMapper.NAME);\n+        assertThat(\"_id field should not have postings on disk\", diskUsageIdField.getInvertedIndexBytes(), equalTo(0L));\n+\n+        // TODO Search datastream and count hits\n+    }\n+\n+    private static XContentBuilder document(Instant timestamp, String hostName, String metricField, Integer metricValue)\n+        throws IOException {\n+        var source = XContentFactory.jsonBuilder();\n+        source.startObject();\n+        {\n+            source.field(\"@timestamp\", DATE_FORMATTER.format(timestamp));\n+            source.field(\"hostname\", hostName);\n+            source.startObject(\"metric\");\n+            {\n+                source.field(\"field\", metricField);\n+                source.field(\"value\", metricValue);\n+\n+            }\n+            source.endObject();\n+        }\n+        source.endObject();\n+        return source;\n+    }\n+\n+    private static BulkItemResponse[] createDocuments(String indexName, XContentBuilder... docs) throws IOException {\n+        assertThat(docs, notNullValue());\n+        final var client = client();\n+        var bulkRequest = client.prepareBulk();\n+        for (var doc : docs) {\n+            bulkRequest.add(client.prepareIndex(indexName).setOpType(DocWriteRequest.OpType.CREATE).setSource(doc));\n+        }\n+        var bulkResponse = bulkRequest.get();\n+        assertNoFailures(bulkResponse);\n+        return bulkResponse.getItems();\n+    }\n+\n+    private static void putDataStreamTemplate(Random random, String indexPattern) throws IOException {\n+        final var settings = indexSettings(1, 0).put(IndexSettings.MODE.getKey(), IndexMode.TIME_SERIES.getName())\n+            .put(IndexSettings.BLOOM_FILTER_ID_FIELD_ENABLED_SETTING.getKey(), false)\n+            .put(IndexSettings.INDEX_REFRESH_INTERVAL_SETTING.getKey(), -1)\n+            .put(IndexSettings.USE_SYNTHETIC_ID.getKey(), true);\n+\n+        final var mappings = \"\"\"\n+            {\n+                \"_doc\": {\n+                    \"properties\": {\n+                        \"@timestamp\": {\n+                            \"type\": \"date\"\n+                        },\n+                        \"hostname\": {\n+                            \"type\": \"keyword\",\n+                            \"time_series_dimension\": true\n+                        },\n+                        \"metric\": {\n+                            \"properties\": {\n+                                \"field\": {\n+                                    \"type\": \"keyword\",\n+                                    \"time_series_dimension\": true\n+                                },\n+                                \"value\": {\n+                                    \"type\": \"integer\",\n+                                    \"time_series_metric\": \"counter\"\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\"\"\";\n+\n+        var putTemplateRequest = new TransportPutComposableIndexTemplateAction.Request(getTestClass().getName().toLowerCase(Locale.ROOT))\n+            .indexTemplate(\n+                ComposableIndexTemplate.builder()\n+                    .indexPatterns(List.of(indexPattern))\n+                    .template(new Template(settings.build(), new CompressedXContent(mappings), null))\n+                    .dataStreamTemplate(new ComposableIndexTemplate.DataStreamTemplate(false, false))\n+                    .build()\n+            );\n+        assertAcked(client().execute(TransportPutComposableIndexTemplateAction.TYPE, putTemplateRequest).actionGet());\n+    }\n+\n+    private static IndexDiskUsageStats diskUsage(String indexName) {\n+        var diskUsageResponse = client().execute(\n+            TransportAnalyzeIndexDiskUsageAction.TYPE,\n+            new AnalyzeIndexDiskUsageRequest(new String[] { indexName }, AnalyzeIndexDiskUsageRequest.DEFAULT_INDICES_OPTIONS, false)\n+        ).actionGet();\n+\n+        var indexDiskUsageStats = AnalyzeIndexDiskUsageTestUtils.getIndexStats(diskUsageResponse, indexName);\n+        assertNotNull(indexDiskUsageStats);\n+        return indexDiskUsageStats;\n+    }\n+}\ndiff --git a/server/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java b/server/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java\nindex 56357357af13a..d18fdc2702d56 100644\n--- a/server/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java\n+++ b/server/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java\n@@ -246,6 +246,9 @@ public final class IndexScopedSettings extends AbstractScopedSettings {\n         if (IndexSettings.DOC_VALUES_SKIPPER) {\n             settings.add(IndexSettings.USE_DOC_VALUES_SKIPPER);\n         }\n+        if (IndexSettings.TSDB_SYNTHETIC_ID_FEATURE_FLAG) {\n+            settings.add(IndexSettings.USE_SYNTHETIC_ID);\n+        }\n         settings.add(IndexSettings.INDEX_MAPPING_EXCLUDE_SOURCE_VECTORS_SETTING);\n         BUILT_IN_INDEX_SETTINGS = Collections.unmodifiableSet(settings);\n     };\ndiff --git a/server/src/main/java/org/elasticsearch/index/IndexSettings.java b/server/src/main/java/org/elasticsearch/index/IndexSettings.java\nindex 2abfc8348b9ce..a5fd8144c5492 100644\n--- a/server/src/main/java/org/elasticsearch/index/IndexSettings.java\n+++ b/server/src/main/java/org/elasticsearch/index/IndexSettings.java\n@@ -676,6 +676,44 @@ public boolean isES87TSDBCodecEnabled() {\n         Property.Final\n     );\n \n+    public static final boolean TSDB_SYNTHETIC_ID_FEATURE_FLAG = new FeatureFlag(\"tsdb_synthetic_id\").isEnabled();\n+    public static final Setting<Boolean> USE_SYNTHETIC_ID = Setting.boolSetting(\n+        \"index.mapping.use_synthetic_id\",\n+        false,\n+        new Setting.Validator<>() {\n+            @Override\n+            public void validate(Boolean value) {}\n+\n+            @Override\n+            public void validate(Boolean enabled, Map<Setting<?>, Object> settings) {\n+                if (enabled) {\n+                    // Verify if index mode is TIME_SERIES\n+                    var indexMode = (IndexMode) settings.get(MODE);\n+                    if (indexMode != IndexMode.TIME_SERIES) {\n+                        throw new IllegalArgumentException(\n+                            String.format(\n+                                Locale.ROOT,\n+                                \"The setting [%s] is only permitted when [%s] is set to [%s]. Current mode: [%s].\",\n+                                USE_SYNTHETIC_ID.getKey(),\n+                                MODE.getKey(),\n+                                IndexMode.TIME_SERIES.name(),\n+                                indexMode.name()\n+                            )\n+                        );\n+                    }\n+                }\n+            }\n+\n+            @Override\n+            public Iterator<Setting<?>> settings() {\n+                List<Setting<?>> list = List.of(MODE);\n+                return list.iterator();\n+            }\n+        },\n+        Property.IndexScope,\n+        Property.Final\n+    );\n+\n     /**\n      * The {@link IndexMode \"mode\"} of the index.\n      */\n@@ -937,6 +975,7 @@ private void setRetentionLeaseMillis(final TimeValue retentionLease) {\n     private final boolean recoverySourceEnabled;\n     private final boolean recoverySourceSyntheticEnabled;\n     private final boolean useDocValuesSkipper;\n+    private final boolean tsdbSyntheticId;\n \n     /**\n      * The maximum number of refresh listeners allows on this shard.\n@@ -1123,6 +1162,8 @@ public IndexSettings(final IndexMetadata indexMetadata, final Settings nodeSetti\n             && scopedSettings.get(RECOVERY_USE_SYNTHETIC_SOURCE_SETTING);\n         useDocValuesSkipper = DOC_VALUES_SKIPPER && scopedSettings.get(USE_DOC_VALUES_SKIPPER);\n         seqNoIndexOptions = scopedSettings.get(SEQ_NO_INDEX_OPTIONS_SETTING);\n+        tsdbSyntheticId = TSDB_SYNTHETIC_ID_FEATURE_FLAG && scopedSettings.get(USE_SYNTHETIC_ID);\n+        assert tsdbSyntheticId == false || mode == IndexMode.TIME_SERIES : mode;\n         if (recoverySourceSyntheticEnabled) {\n             if (DiscoveryNode.isStateless(settings)) {\n                 throw new IllegalArgumentException(\"synthetic recovery source is only allowed in stateful\");\n@@ -1855,6 +1896,13 @@ public boolean useDocValuesSkipper() {\n         return useDocValuesSkipper;\n     }\n \n+    /**\n+     * @return Whether the index is a time-series index that use synthetic ids.\n+     */\n+    public boolean useTsdbSyntheticId() {\n+        return tsdbSyntheticId;\n+    }\n+\n     /**\n      * The bounds for {@code @timestamp} on this index or\n      * {@code null} if there are no bounds.\ndiff --git a/server/src/main/java/org/elasticsearch/index/codec/CodecService.java b/server/src/main/java/org/elasticsearch/index/codec/CodecService.java\nindex 17028137b78d8..1e2fed61578a5 100644\n--- a/server/src/main/java/org/elasticsearch/index/codec/CodecService.java\n+++ b/server/src/main/java/org/elasticsearch/index/codec/CodecService.java\n@@ -16,6 +16,8 @@\n import org.elasticsearch.common.util.BigArrays;\n import org.elasticsearch.common.util.FeatureFlag;\n import org.elasticsearch.core.Nullable;\n+import org.elasticsearch.index.IndexMode;\n+import org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdCodec;\n import org.elasticsearch.index.codec.zstd.Zstd814StoredFieldsFormat;\n import org.elasticsearch.index.mapper.MapperService;\n \n@@ -65,12 +67,20 @@ public CodecService(@Nullable MapperService mapperService, BigArrays bigArrays)\n         for (String codec : Codec.availableCodecs()) {\n             codecs.put(codec, Codec.forName(codec));\n         }\n+        final boolean useTsdbSyntheticId = mapperService != null && mapperService.getIndexSettings().useTsdbSyntheticId();\n+        assert useTsdbSyntheticId == false || mapperService.getIndexSettings().getMode() == IndexMode.TIME_SERIES;\n+\n         this.codecs = codecs.entrySet().stream().collect(Collectors.toUnmodifiableMap(Map.Entry::getKey, e -> {\n-            var codec = e.getValue();\n-            if (codec instanceof DeduplicateFieldInfosCodec) {\n-                return codec;\n+            Codec codec;\n+            if (e.getValue() instanceof DeduplicateFieldInfosCodec dedupCodec) {\n+                codec = dedupCodec;\n+            } else {\n+                codec = new DeduplicateFieldInfosCodec(e.getValue().getName(), e.getValue());\n+            }\n+            if (useTsdbSyntheticId && codec instanceof TSDBSyntheticIdCodec == false) {\n+                codec = new TSDBSyntheticIdCodec(codec.getName(), codec);\n             }\n-            return new DeduplicateFieldInfosCodec(codec.getName(), codec);\n+            return codec;\n         }));\n     }\n \ndiff --git a/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdCodec.java b/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdCodec.java\nnew file mode 100644\nindex 0000000000000..970664844631a\n--- /dev/null\n+++ b/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdCodec.java\n@@ -0,0 +1,158 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.codec.tsdb;\n+\n+import org.apache.lucene.codecs.Codec;\n+import org.apache.lucene.codecs.FieldInfosFormat;\n+import org.apache.lucene.codecs.FilterCodec;\n+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;\n+import org.apache.lucene.index.FieldInfo;\n+import org.apache.lucene.index.FieldInfos;\n+import org.apache.lucene.index.IndexOptions;\n+import org.apache.lucene.index.SegmentInfo;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.store.IOContext;\n+import org.elasticsearch.index.mapper.SyntheticIdField;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.SYNTHETIC_ID;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TIMESTAMP;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TS_ID;\n+\n+/**\n+ * Special codec for time-series datastreams that use synthetic ids.\n+ * <p>\n+ *     The role of this codec is to ensure that no inverted index is created when indexing a document id in Lucene, while allowing the usage\n+ *     of terms and postings on the field (now called a \"synthetic _id\" field) as if it was backed by an in inverted index.\n+ * </p>\n+ * <p>\n+ *     In order to do this, it enforces synthetic _id fields to be indexed with the {@link IndexOptions#NONE} option, hence preventing the\n+ *     building of a term dictionary with postings lists. The codec also changes this {@link IndexOptions#NONE} option back to\n+ *     {@link IndexOptions#DOCS} when reading the {@link FieldInfos} during the opening of a new segment core reader. This allows to use a\n+ *     Lucene term dictionary on top of a synthetic _id field that does not have corresponding postings files on disk. Finally, the codec\n+ *     injects additional {@link FieldInfos} attributes so that Lucene's {@link PerFieldPostingsFormat} correctly instantiates a\n+ *     {@link TSDBSyntheticIdPostingsFormat} to access the term and postings of the synthetic _id field.\n+ * </p>\n+ */\n+public class TSDBSyntheticIdCodec extends FilterCodec {\n+\n+    private final TSDBSyntheticIdFieldInfosFormat fieldInfosFormat;\n+\n+    public TSDBSyntheticIdCodec(String name, Codec delegate) {\n+        super(name, delegate);\n+        this.fieldInfosFormat = new TSDBSyntheticIdFieldInfosFormat(delegate.fieldInfosFormat());\n+    }\n+\n+    @Override\n+    public final FieldInfosFormat fieldInfosFormat() {\n+        return fieldInfosFormat;\n+    }\n+\n+    /**\n+     * {@link FieldInfosFormat} that ensures the _id field is synthetic\n+     */\n+    private static class TSDBSyntheticIdFieldInfosFormat extends FieldInfosFormat {\n+\n+        private final FieldInfosFormat delegate;\n+\n+        private TSDBSyntheticIdFieldInfosFormat(FieldInfosFormat delegate) {\n+            this.delegate = delegate;\n+        }\n+\n+        private void ensureSyntheticIdFields(FieldInfos fieldInfos) {\n+            // Ensure _tsid exists\n+            var fi = fieldInfos.fieldInfo(TS_ID);\n+            if (fi == null) {\n+                var message = \"Field [\" + TS_ID + \"] does not exist\";\n+                assert false : message;\n+                throw new IllegalArgumentException(message);\n+            }\n+            // Ensure @timestamp exists\n+            fi = fieldInfos.fieldInfo(TIMESTAMP);\n+            if (fi == null) {\n+                var message = \"Field [\" + TIMESTAMP + \"] does not exist\";\n+                assert false : message;\n+                throw new IllegalArgumentException(message);\n+            }\n+            // Ensure _id exists and not indexed\n+            fi = fieldInfos.fieldInfo(SYNTHETIC_ID);\n+            if (fi == null) {\n+                var message = \"Field [\" + SYNTHETIC_ID + \"] does not exist\";\n+                assert false : message;\n+                throw new IllegalArgumentException(message);\n+            }\n+            if (fi.getIndexOptions() != IndexOptions.NONE) {\n+                assert false;\n+                throw new IllegalArgumentException(\"Field [\" + SYNTHETIC_ID + \"] has incorrect index options\");\n+            }\n+            if (SyntheticIdField.hasSyntheticIdAttributes(fi.attributes()) == false) {\n+                throw new IllegalArgumentException(\"Field [\" + SYNTHETIC_ID + \"] is not synthetic\");\n+            }\n+        }\n+\n+        @Override\n+        public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos fieldInfos, IOContext context)\n+            throws IOException {\n+            ensureSyntheticIdFields(fieldInfos);\n+            delegate.write(directory, segmentInfo, segmentSuffix, fieldInfos, context);\n+        }\n+\n+        @Override\n+        public FieldInfos read(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, IOContext iocontext) throws IOException {\n+            final var fieldInfos = delegate.read(directory, segmentInfo, segmentSuffix, iocontext);\n+            ensureSyntheticIdFields(fieldInfos);\n+\n+            // Change the _id field index options from IndexOptions.NONE to IndexOptions.DOCS, so that terms and postings work when\n+            // applying doc values updates in Lucene.\n+            final var infos = new FieldInfo[fieldInfos.size()];\n+            int i = 0;\n+            for (FieldInfo fi : fieldInfos) {\n+                if (SYNTHETIC_ID.equals(fi.getName())) {\n+                    final var attributes = new HashMap<>(fi.attributes());\n+\n+                    // Assert that PerFieldPostingsFormat are not written to field infos on disk\n+                    assert attributes.containsKey(PerFieldPostingsFormat.PER_FIELD_FORMAT_KEY) == false;\n+                    assert attributes.containsKey(PerFieldPostingsFormat.PER_FIELD_SUFFIX_KEY) == false;\n+\n+                    // Inject attributes so that PerFieldPostingsFormat maps the synthetic _id field to the TSDBSyntheticIdPostingsFormat\n+                    // This would normally be handled transparently by PerFieldPostingsFormat, but such attributes are only added if terms\n+                    // are produced during indexing, which is not the case for the synthetic _id field.\n+                    attributes.put(PerFieldPostingsFormat.PER_FIELD_FORMAT_KEY, TSDBSyntheticIdPostingsFormat.FORMAT_NAME);\n+                    attributes.put(PerFieldPostingsFormat.PER_FIELD_SUFFIX_KEY, TSDBSyntheticIdPostingsFormat.SUFFIX);\n+\n+                    fi = new FieldInfo(\n+                        fi.getName(),\n+                        fi.getFieldNumber(),\n+                        fi.hasTermVectors(),\n+                        true,\n+                        fi.hasPayloads(),\n+                        IndexOptions.DOCS,\n+                        fi.getDocValuesType(),\n+                        fi.docValuesSkipIndexType(),\n+                        fi.getDocValuesGen(),\n+                        attributes,\n+                        fi.getPointDimensionCount(),\n+                        fi.getPointIndexDimensionCount(),\n+                        fi.getPointNumBytes(),\n+                        fi.getVectorDimension(),\n+                        fi.getVectorEncoding(),\n+                        fi.getVectorSimilarityFunction(),\n+                        fi.isSoftDeletesField(),\n+                        fi.isParentField()\n+                    );\n+                }\n+                infos[i++] = fi;\n+            }\n+            return new FieldInfos(infos);\n+        }\n+    }\n+}\ndiff --git a/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdFieldsProducer.java b/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdFieldsProducer.java\nnew file mode 100644\nindex 0000000000000..2f624fd2d9cd0\n--- /dev/null\n+++ b/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdFieldsProducer.java\n@@ -0,0 +1,390 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.codec.tsdb;\n+\n+import org.apache.lucene.codecs.DocValuesProducer;\n+import org.apache.lucene.codecs.FieldsProducer;\n+import org.apache.lucene.index.BaseTermsEnum;\n+import org.apache.lucene.index.FieldInfos;\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.SegmentReadState;\n+import org.apache.lucene.index.SortedDocValues;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.search.DocIdSetIterator;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.core.IOUtils;\n+import org.elasticsearch.index.mapper.TimeSeriesRoutingHashFieldMapper;\n+import org.elasticsearch.index.mapper.TsidExtractingIdFieldMapper;\n+import org.elasticsearch.index.mapper.Uid;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Iterator;\n+import java.util.Objects;\n+import java.util.Set;\n+\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.SYNTHETIC_ID;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TIMESTAMP;\n+import static org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat.TS_ID;\n+\n+public class TSDBSyntheticIdFieldsProducer extends FieldsProducer {\n+\n+    private static final Set<String> FIELDS_NAMES = Set.of(SYNTHETIC_ID);\n+\n+    private final DocValuesProducer docValuesProducer;\n+    private final FieldInfos fieldInfos;\n+    private final int maxDocs;\n+\n+    public TSDBSyntheticIdFieldsProducer(SegmentReadState state, DocValuesProducer docValuesProducer) {\n+        this(state.fieldInfos, docValuesProducer, state.segmentInfo.maxDoc());\n+    }\n+\n+    private TSDBSyntheticIdFieldsProducer(FieldInfos fieldInfos, DocValuesProducer docValuesProducer, int maxDocs) {\n+        assert assertFieldInfosExist(fieldInfos, SYNTHETIC_ID, TIMESTAMP, TS_ID);\n+        this.docValuesProducer = Objects.requireNonNull(docValuesProducer);\n+        this.fieldInfos = fieldInfos;\n+        this.maxDocs = maxDocs;\n+    }\n+\n+    @Override\n+    public int size() {\n+        return FIELDS_NAMES.size();\n+    }\n+\n+    @Override\n+    public Iterator<String> iterator() {\n+        return FIELDS_NAMES.iterator();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        IOUtils.close(docValuesProducer);\n+    }\n+\n+    @Override\n+    public void checkIntegrity() throws IOException {}\n+\n+    @Override\n+    public FieldsProducer getMergeInstance() {\n+        return new TSDBSyntheticIdFieldsProducer(fieldInfos, docValuesProducer, maxDocs);\n+    }\n+\n+    @Override\n+    public Terms terms(String field) throws IOException {\n+        assert FIELDS_NAMES.contains(field) : field;\n+        return new Terms() {\n+            @Override\n+            public TermsEnum iterator() {\n+                return new FakeTermsEnum();\n+            }\n+\n+            @Override\n+            public int getDocCount() {\n+                return maxDocs - 1; // All docs have a synthetic id\n+            }\n+\n+            @Override\n+            public long size() {\n+                return -1; // Number of terms is unknown\n+            }\n+\n+            @Override\n+            public long getSumTotalTermFreq() {\n+                return 0;\n+            }\n+\n+            @Override\n+            public long getSumDocFreq() {\n+                return 0;\n+            }\n+\n+            @Override\n+            public boolean hasFreqs() {\n+                return false;\n+            }\n+\n+            @Override\n+            public boolean hasOffsets() {\n+                return false;\n+            }\n+\n+            @Override\n+            public boolean hasPositions() {\n+                return false;\n+            }\n+\n+            @Override\n+            public boolean hasPayloads() {\n+                return false;\n+            }\n+        };\n+    }\n+\n+    /**\n+     * This is a fake TermsEnum that scans all documents for find docs matching a specific _id. This implementation is only here to show\n+     * that the synthetic _id terms is used when applying doc values updates during soft-updates. It is buggy and should not be used besides\n+     * some carefully crafted integration tests, because it relies on the current _id format for TSDB indices that has limitations:\n+     * - it is composed of a routing hash, a @timestamp and a tsid that cannot be un-hashed so all docs must be scanned to find matchings\n+     * - it is not sorted on _id in the Lucene segments so doc values updates stop too early when applying DV updates\n+     *\n+     * This fake terms enumeration will be changed to support a different _id format in a short future.\n+     */\n+    private class FakeTermsEnum extends BaseTermsEnum {\n+\n+        private BytesRef term = null;\n+        private int docID = -1;\n+\n+        private BytesRef latestTsId = null;\n+        private long latestTimestamp = -1L;\n+\n+        private FakeTermsEnum() {}\n+\n+        @Override\n+        public BytesRef next() throws IOException {\n+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n+                assert term == null;\n+                return null;\n+            }\n+            docID += 1;\n+            if (maxDocs <= docID) {\n+                docID = DocIdSetIterator.NO_MORE_DOCS;\n+                latestTimestamp = -1L;\n+                latestTsId = null;\n+                term = null;\n+                return null;\n+            }\n+\n+            // Retrieve _tsid\n+            SortedDocValues tsIdDocValues = docValuesProducer.getSorted(fieldInfos.fieldInfo(TS_ID));\n+            boolean found = tsIdDocValues.advanceExact(docID);\n+            assert found;\n+            int tsIdOrd = tsIdDocValues.ordValue();\n+            BytesRef tsId = tsIdDocValues.lookupOrd(tsIdOrd);\n+            assert tsId != null;\n+\n+            // Retrieve timestamp\n+            SortedNumericDocValues timestampDocValues = docValuesProducer.getSortedNumeric(fieldInfos.fieldInfo(TIMESTAMP));\n+            found = timestampDocValues.advanceExact(docID);\n+            assert found;\n+            assert timestampDocValues.docValueCount() == 1;\n+            long timestamp = timestampDocValues.nextValue();\n+\n+            // Retrieve routing hash\n+            var tsRoutingHash = fieldInfos.fieldInfo(TimeSeriesRoutingHashFieldMapper.NAME);\n+            assert tsRoutingHash != null;\n+            SortedDocValues routingHashDocValues = docValuesProducer.getSorted(tsRoutingHash);\n+            found = routingHashDocValues.advanceExact(docID);\n+            assert found;\n+            BytesRef routingHashBytes = routingHashDocValues.lookupOrd(routingHashDocValues.ordValue());\n+\n+            int routingHash = TimeSeriesRoutingHashFieldMapper.decode(\n+                Uid.decodeId(routingHashBytes.bytes, routingHashBytes.offset, routingHashBytes.length)\n+            );\n+            term = Uid.encodeId(TsidExtractingIdFieldMapper.createId(routingHash, tsId, timestamp));\n+            latestTimestamp = timestamp;\n+            latestTsId = tsId;\n+            return term;\n+        }\n+\n+        @Override\n+        public SeekStatus seekCeil(BytesRef id) {\n+            assert id != null;\n+            if (term != null && term.equals(id)) {\n+                return SeekStatus.FOUND;\n+            }\n+            try {\n+                while (next() != null) {\n+                    if (term.equals(id)) {\n+                        return SeekStatus.FOUND;\n+                    }\n+                }\n+            } catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+            return SeekStatus.END;\n+        }\n+\n+        @Override\n+        public BytesRef term() {\n+            return term;\n+        }\n+\n+        @Override\n+        public PostingsEnum postings(PostingsEnum reuse, int flags) {\n+            return new FakePostingsEnum(docID, latestTsId, latestTimestamp, maxDocs);\n+        }\n+\n+        /**\n+         * This is an optional method as per the {@link TermsEnum#ord()} documentation that is not supported by the current implementation.\n+         * This method always throws an {@link UnsupportedOperationException}.\n+         */\n+        @Override\n+        public long ord() {\n+            throw unsupportedException();\n+        }\n+\n+        /**\n+         * This is an optional method as per the {@link TermsEnum#ord()} documentation that is not supported by the current implementation.\n+         * This method always throws an {@link UnsupportedOperationException}.\n+         */\n+        @Override\n+        public void seekExact(long ord) {\n+            throw unsupportedException();\n+        }\n+\n+        @Override\n+        public int docFreq() throws IOException {\n+            return 0;\n+        }\n+\n+        @Override\n+        public long totalTermFreq() throws IOException {\n+            return 0;\n+        }\n+\n+        @Override\n+        public ImpactsEnum impacts(int flags) throws IOException {\n+            return null;\n+        }\n+    }\n+\n+    /**\n+     * Do not use in production. See {@link FakeTermsEnum}.\n+     */\n+    private class FakePostingsEnum extends PostingsEnum {\n+\n+        private final int startDocID;\n+        private final BytesRef latestTsId;\n+        private final long latestTimestamp;\n+        private final int maxDocs;\n+        private int docID;\n+\n+        private FakePostingsEnum(int docID, BytesRef latestTsId, long latestTimestamp, int maxDocs) {\n+            this.startDocID = docID;\n+            this.latestTsId = latestTsId;\n+            this.latestTimestamp = latestTimestamp;\n+            this.maxDocs = maxDocs;\n+            this.docID = -1;\n+        }\n+\n+        @Override\n+        public int docID() {\n+            return docID;\n+        }\n+\n+        @Override\n+        public int nextDoc() throws IOException {\n+            if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n+                return docID;\n+            } else if (docID == -1) {\n+                docID = startDocID;\n+            } else {\n+                docID = docID + 1;\n+                if (maxDocs <= docID) {\n+                    docID = DocIdSetIterator.NO_MORE_DOCS;\n+                    return docID;\n+                }\n+            }\n+\n+            // Retrieve _tsid\n+            SortedDocValues tsIdDocValues = docValuesProducer.getSorted(fieldInfos.fieldInfo(TS_ID));\n+            boolean found = tsIdDocValues.advanceExact(docID);\n+            assert found;\n+            int tsIdOrd = tsIdDocValues.ordValue();\n+            BytesRef tsId = tsIdDocValues.lookupOrd(tsIdOrd);\n+            assert tsId != null;\n+\n+            if (latestTsId != null && latestTsId.equals(tsId) == false) {\n+                // Different _tsid, stop here\n+                docID = DocIdSetIterator.NO_MORE_DOCS;\n+                return docID;\n+            }\n+\n+            // Retrieve timestamp\n+            SortedNumericDocValues timestampDocValues = docValuesProducer.getSortedNumeric(fieldInfos.fieldInfo(TIMESTAMP));\n+            found = timestampDocValues.advanceExact(docID);\n+            assert found;\n+            assert timestampDocValues.docValueCount() == 1;\n+            long timestamp = timestampDocValues.nextValue();\n+\n+            if (latestTimestamp != -1L && latestTimestamp != timestamp) {\n+                // Different @timestamp, stop here\n+                docID = DocIdSetIterator.NO_MORE_DOCS;\n+                return docID;\n+            }\n+\n+            // Retrieve routing hash\n+            var tsRoutingHash = fieldInfos.fieldInfo(TimeSeriesRoutingHashFieldMapper.NAME);\n+            assert tsRoutingHash != null;\n+            SortedDocValues routingHashDocValues = docValuesProducer.getSorted(tsRoutingHash);\n+            found = routingHashDocValues.advanceExact(docID);\n+            assert found;\n+            BytesRef routingHashBytes = routingHashDocValues.lookupOrd(routingHashDocValues.ordValue());\n+            assert routingHashBytes != null;\n+            return docID;\n+        }\n+\n+        @Override\n+        public int advance(int target) throws IOException {\n+            int doc;\n+            while ((doc = nextDoc()) < target) {\n+                // Continue\n+            }\n+            return doc;\n+        }\n+\n+        @Override\n+        public long cost() {\n+            return 0L;\n+        }\n+\n+        @Override\n+        public int freq() throws IOException {\n+            return 0; // not supported\n+        }\n+\n+        @Override\n+        public int nextPosition() throws IOException {\n+            return -1; // not supported\n+        }\n+\n+        @Override\n+        public int startOffset() throws IOException {\n+            return -1; // not supported\n+        }\n+\n+        @Override\n+        public int endOffset() throws IOException {\n+            return -1; // not supported\n+        }\n+\n+        @Override\n+        public BytesRef getPayload() throws IOException {\n+            return null; // not supported\n+        }\n+    }\n+\n+    private static boolean assertFieldInfosExist(FieldInfos fieldInfos, String... fieldNames) {\n+        assert fieldNames != null && fieldNames.length > 0 : \"fieldNames should be > 0\";\n+        for (var fieldName : fieldNames) {\n+            assert fieldInfos.fieldInfo(fieldName) != null : \"field [\" + fieldName + \"] not found\";\n+        }\n+        return true;\n+    }\n+\n+    private static UnsupportedOperationException unsupportedException() {\n+        var error = \"This method should not be called on this enum\";\n+        assert false : error;\n+        return new UnsupportedOperationException(error);\n+    }\n+}\ndiff --git a/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdPostingsFormat.java b/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdPostingsFormat.java\nnew file mode 100644\nindex 0000000000000..cfe9975f33a1b\n--- /dev/null\n+++ b/server/src/main/java/org/elasticsearch/index/codec/tsdb/TSDBSyntheticIdPostingsFormat.java\n@@ -0,0 +1,61 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.codec.tsdb;\n+\n+import org.apache.lucene.codecs.DocValuesProducer;\n+import org.apache.lucene.codecs.FieldsConsumer;\n+import org.apache.lucene.codecs.FieldsProducer;\n+import org.apache.lucene.codecs.PostingsFormat;\n+import org.apache.lucene.index.SegmentReadState;\n+import org.apache.lucene.index.SegmentWriteState;\n+import org.elasticsearch.core.IOUtils;\n+import org.elasticsearch.index.mapper.DataStreamTimestampFieldMapper;\n+import org.elasticsearch.index.mapper.SyntheticIdField;\n+import org.elasticsearch.index.mapper.TimeSeriesIdFieldMapper;\n+\n+import java.io.IOException;\n+\n+public class TSDBSyntheticIdPostingsFormat extends PostingsFormat {\n+\n+    public static final String SYNTHETIC_ID = SyntheticIdField.NAME;\n+    public static final String TIMESTAMP = DataStreamTimestampFieldMapper.DEFAULT_PATH;\n+    public static final String TS_ID = TimeSeriesIdFieldMapper.NAME;\n+\n+    static final String FORMAT_NAME = \"TSDBSyntheticId\";\n+    static final String SUFFIX = \"0\";\n+\n+    public TSDBSyntheticIdPostingsFormat() {\n+        super(FORMAT_NAME);\n+    }\n+\n+    @Override\n+    public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {\n+        DocValuesProducer docValuesProducer = null;\n+        boolean success = false;\n+        try {\n+            var codec = state.segmentInfo.getCodec();\n+            // Erase the segment suffix (used only for reading postings)\n+            docValuesProducer = codec.docValuesFormat().fieldsProducer(new SegmentReadState(state, \"\"));\n+            var fieldsProducer = new TSDBSyntheticIdFieldsProducer(state, docValuesProducer);\n+            success = true;\n+            return fieldsProducer;\n+        } finally {\n+            if (success == false) {\n+                IOUtils.close(docValuesProducer);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {\n+        assert false : \"this should never be called\";\n+        throw new UnsupportedOperationException();\n+    }\n+}\ndiff --git a/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java\nindex 415670d811b82..47e9ab7803a84 100644\n--- a/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java\n+++ b/server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java\n@@ -231,6 +231,8 @@ public class InternalEngine extends Engine {\n \n     private final ByteSizeValue totalDiskSpace;\n \n+    private final boolean useTsdbSyntheticId;\n+\n     protected static final String REAL_TIME_GET_REFRESH_SOURCE = \"realtime_get\";\n     protected static final String UNSAFE_VERSION_MAP_REFRESH_SOURCE = \"unsafe_version_map\";\n \n@@ -243,6 +245,12 @@ public InternalEngine(EngineConfig engineConfig) {\n     InternalEngine(EngineConfig engineConfig, int maxDocs, BiFunction<Long, Long, LocalCheckpointTracker> localCheckpointTrackerSupplier) {\n         super(engineConfig);\n         this.maxDocs = maxDocs;\n+        if (engineConfig.getIndexSettings().useTsdbSyntheticId()) {\n+            logger.info(\"using TSDB with synthetic id\");\n+            useTsdbSyntheticId = true;\n+        } else {\n+            useTsdbSyntheticId = false;\n+        }\n         this.relativeTimeInNanosSupplier = config().getRelativeTimeInNanosSupplier();\n         this.lastFlushTimestamp = relativeTimeInNanosSupplier.getAsLong(); // default to creation timestamp\n         this.liveVersionMapArchive = createLiveVersionMapArchive();\n@@ -1434,6 +1442,7 @@ private IndexResult indexIntoLucene(Index index, IndexingStrategy plan) throws I\n         index.parsedDoc().updateSeqID(index.seqNo(), index.primaryTerm());\n         index.parsedDoc().version().setLongValue(plan.versionForIndexing);\n         try {\n+            logDocumentsDetails(index.docs());\n             if (plan.addStaleOpToLucene) {\n                 addStaleDocs(index.docs(), indexWriter);\n             } else if (plan.useLuceneUpdateDocument) {\n@@ -1469,6 +1478,14 @@ && treatDocumentFailureAsTragicError(index) == false) {\n         }\n     }\n \n+    private void logDocumentsDetails(List<LuceneDocument> docs) {\n+        if (useTsdbSyntheticId && logger.isTraceEnabled()) {\n+            for (var doc : docs) {\n+                logger.trace(\"indexing document fields [{}]\", doc.getFields());\n+            }\n+        }\n+    }\n+\n     /**\n      * Whether we should treat any document failure as tragic error.\n      * If we hit any failure while processing an indexing on a replica, we should treat that error as tragic and fail the engine.\n@@ -1842,6 +1859,7 @@ private DeleteResult deleteInLucene(Delete delete, DeletionStrategy plan) throws\n         try {\n             final ParsedDocument tombstone = ParsedDocument.deleteTombstone(\n                 engineConfig.getIndexSettings().seqNoIndexOptions(),\n+                useTsdbSyntheticId,\n                 delete.id()\n             );\n             assert tombstone.docs().size() == 1 : \"Tombstone doc should have single doc [\" + tombstone + \"]\";\ndiff --git a/server/src/main/java/org/elasticsearch/index/mapper/IdFieldMapper.java b/server/src/main/java/org/elasticsearch/index/mapper/IdFieldMapper.java\nindex 8e418f45ddb3a..61f5fa9abcf0a 100644\n--- a/server/src/main/java/org/elasticsearch/index/mapper/IdFieldMapper.java\n+++ b/server/src/main/java/org/elasticsearch/index/mapper/IdFieldMapper.java\n@@ -78,6 +78,13 @@ public static Field standardIdField(String id) {\n         return new StringField(NAME, Uid.encodeId(id), Field.Store.YES);\n     }\n \n+    /**\n+     * Create a {@link Field} corresponding to a synthetic {@code _id} field, which is not indexed but instead resolved at runtime.\n+     */\n+    public static Field syntheticIdField(String id) {\n+        return new SyntheticIdField(Uid.encodeId(id));\n+    }\n+\n     protected abstract static class AbstractIdFieldType extends TermBasedFieldType {\n \n         public AbstractIdFieldType() {\ndiff --git a/server/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java b/server/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java\nindex 72fd812d982d8..61b26ca33b1ef 100644\n--- a/server/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java\n+++ b/server/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java\n@@ -70,15 +70,32 @@ public static ParsedDocument noopTombstone(SeqNoFieldMapper.SeqNoIndexOptions se\n     /**\n      * Create a delete tombstone document, which will be used in soft-update methods.\n      * The returned document consists only _uid, _seqno, _term and _version fields; other metadata fields are excluded.\n-     * @param id    the id of the deleted document\n+     * @param id                the id of the deleted document\n      */\n     public static ParsedDocument deleteTombstone(SeqNoFieldMapper.SeqNoIndexOptions seqNoIndexOptions, String id) {\n+        return deleteTombstone(seqNoIndexOptions, false, id);\n+    }\n+\n+    /**\n+     * Create a delete tombstone document, which will be used in soft-update methods.\n+     * The returned document consists only _uid, _seqno, _term and _version fields; other metadata fields are excluded.\n+     * @param useSyntheticId    whether the id is synthetic or not\n+     * @param id                the id of the deleted document\n+     */\n+    public static ParsedDocument deleteTombstone(SeqNoFieldMapper.SeqNoIndexOptions seqNoIndexOptions, boolean useSyntheticId, String id) {\n         LuceneDocument document = new LuceneDocument();\n         SeqNoFieldMapper.SequenceIDFields seqIdFields = SeqNoFieldMapper.SequenceIDFields.tombstone(seqNoIndexOptions);\n         seqIdFields.addFields(document);\n         Field versionField = VersionFieldMapper.versionField();\n         document.add(versionField);\n-        document.add(IdFieldMapper.standardIdField(id));\n+        if (useSyntheticId) {\n+            // Use a synthetic _id field which is not indexed nor stored\n+            document.add(IdFieldMapper.syntheticIdField(id));\n+            // TODO I think we also need to add the fields that compose the synthetic _id.\n+        } else {\n+            // Use standard _id field (indexed and stored, some indices also trim the stored field at some point)\n+            document.add(IdFieldMapper.standardIdField(id));\n+        }\n         return new ParsedDocument(\n             versionField,\n             seqIdFields,\ndiff --git a/server/src/main/java/org/elasticsearch/index/mapper/SyntheticIdField.java b/server/src/main/java/org/elasticsearch/index/mapper/SyntheticIdField.java\nnew file mode 100644\nindex 0000000000000..0a37fc564a0a4\n--- /dev/null\n+++ b/server/src/main/java/org/elasticsearch/index/mapper/SyntheticIdField.java\n@@ -0,0 +1,67 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.index.mapper;\n+\n+import org.apache.lucene.analysis.Analyzer;\n+import org.apache.lucene.analysis.TokenStream;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.FieldType;\n+import org.apache.lucene.index.IndexOptions;\n+import org.apache.lucene.util.BytesRef;\n+\n+import java.util.Map;\n+\n+public final class SyntheticIdField extends Field {\n+\n+    public static final String NAME = IdFieldMapper.NAME;\n+\n+    private static final String ENABLED_ATTRIBUTE_KEY = SyntheticIdField.class.getSimpleName() + \".enabled\";\n+    private static final String ENABLED_ATTRIBUTE_VALUE = Boolean.TRUE.toString();\n+\n+    private static final FieldType TYPE;\n+    static {\n+        TYPE = new FieldType();\n+        TYPE.putAttribute(ENABLED_ATTRIBUTE_KEY, ENABLED_ATTRIBUTE_VALUE);\n+        // Make sure the field is not indexed, but this option is changed at runtime\n+        // in FieldInfos so that the field can use terms and postings.\n+        TYPE.setIndexOptions(IndexOptions.NONE);\n+        TYPE.setTokenized(false);\n+        TYPE.setOmitNorms(true);\n+        // The field is marked as stored, but storage on disk might be skipped\n+        TYPE.setStored(true);\n+        TYPE.freeze();\n+    }\n+\n+    public SyntheticIdField(BytesRef bytes) {\n+        super(NAME, bytes, TYPE);\n+    }\n+\n+    @Override\n+    public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n+        assert false : \"this should never be called\";\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void setTokenStream(TokenStream tokenStream) {\n+        assert false : \"this should never be called\";\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static boolean hasSyntheticIdAttributes(Map<String, String> attributes) {\n+        if (attributes != null) {\n+            var attributeValue = attributes.get(SyntheticIdField.ENABLED_ATTRIBUTE_KEY);\n+            if (attributeValue != null) {\n+                return SyntheticIdField.ENABLED_ATTRIBUTE_VALUE.equals(attributeValue);\n+            }\n+        }\n+        return false;\n+    }\n+}\ndiff --git a/server/src/main/java/org/elasticsearch/index/mapper/TsidExtractingIdFieldMapper.java b/server/src/main/java/org/elasticsearch/index/mapper/TsidExtractingIdFieldMapper.java\nindex bb8b0d9ec775c..1bb7001b29890 100644\n--- a/server/src/main/java/org/elasticsearch/index/mapper/TsidExtractingIdFieldMapper.java\n+++ b/server/src/main/java/org/elasticsearch/index/mapper/TsidExtractingIdFieldMapper.java\n@@ -10,7 +10,6 @@\n package org.elasticsearch.index.mapper;\n \n import org.apache.lucene.document.Field;\n-import org.apache.lucene.document.StringField;\n import org.apache.lucene.index.IndexableField;\n import org.apache.lucene.util.BytesRef;\n import org.elasticsearch.cluster.routing.IndexRouting;\n@@ -91,11 +90,20 @@ public static BytesRef createField(DocumentParserContext context, RoutingHashBui\n                 )\n             );\n         }\n+        assert id != null;\n         context.id(id);\n \n-        BytesRef uidEncoded = Uid.encodeId(context.id());\n-        context.doc().add(new StringField(NAME, uidEncoded, Field.Store.YES));\n-        return uidEncoded;\n+        final Field idField;\n+        if (context.indexSettings().useTsdbSyntheticId()) {\n+            idField = syntheticIdField(context.id());\n+        } else {\n+            idField = standardIdField(context.id());\n+        }\n+        assert NAME.equals(idField.name()) : idField.name();\n+        assert idField.binaryValue() != null;\n+\n+        context.doc().add(idField);\n+        return idField.binaryValue();\n     }\n \n     public static String createId(int routingHash, BytesRef tsid, long timestamp) {\ndiff --git a/server/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/server/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat\nindex bdb1b75be4843..ce7ea69c10d9e 100644\n--- a/server/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat\n+++ b/server/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat\n@@ -1,3 +1,4 @@\n org.elasticsearch.index.codec.bloomfilter.ES85BloomFilterPostingsFormat\n org.elasticsearch.index.codec.bloomfilter.ES87BloomFilterPostingsFormat\n org.elasticsearch.index.codec.postings.ES812PostingsFormat\n+org.elasticsearch.index.codec.tsdb.TSDBSyntheticIdPostingsFormat\ndiff --git a/server/src/test/java/org/elasticsearch/common/lucene/uid/VersionLookupTests.java b/server/src/test/java/org/elasticsearch/common/lucene/uid/VersionLookupTests.java\nindex e6c0742cb9979..2e69987f29180 100644\n--- a/server/src/test/java/org/elasticsearch/common/lucene/uid/VersionLookupTests.java\n+++ b/server/src/test/java/org/elasticsearch/common/lucene/uid/VersionLookupTests.java\n@@ -158,7 +158,8 @@ public void testLoadTimestampRange() throws Exception {\n     public void testLoadTimestampRangeWithDeleteTombstone() throws Exception {\n         Directory dir = newDirectory();\n         IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Lucene.STANDARD_ANALYZER).setMergePolicy(NoMergePolicy.INSTANCE));\n-        writer.addDocument(ParsedDocument.deleteTombstone(randomFrom(SeqNoFieldMapper.SeqNoIndexOptions.values()), \"_id\").docs().get(0));\n+        var randomSeqNoIndexOptions = randomFrom(SeqNoFieldMapper.SeqNoIndexOptions.values());\n+        writer.addDocument(ParsedDocument.deleteTombstone(randomSeqNoIndexOptions, false, \"_id\").docs().get(0));\n         DirectoryReader reader = DirectoryReader.open(writer);\n         LeafReaderContext segment = reader.leaves().get(0);\n         PerThreadIDVersionAndSeqNoLookup lookup = new PerThreadIDVersionAndSeqNoLookup(segment.reader(), true);\ndiff --git a/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java\nindex 79f6710bbe7a7..a553dc6578807 100644\n--- a/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java\n+++ b/server/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java\n@@ -4558,7 +4558,7 @@ public void testOnCloseStats() throws IOException {\n     public void testSupplyTombstoneDoc() throws Exception {\n         IndexShard shard = newStartedShard();\n         String id = randomRealisticUnicodeOfLengthBetween(1, 10);\n-        ParsedDocument deleteTombstone = ParsedDocument.deleteTombstone(shard.indexSettings.seqNoIndexOptions(), id);\n+        ParsedDocument deleteTombstone = ParsedDocument.deleteTombstone(shard.indexSettings.seqNoIndexOptions(), randomBoolean(), id);\n         assertThat(deleteTombstone.docs(), hasSize(1));\n         LuceneDocument deleteDoc = deleteTombstone.docs().get(0);\n         assertThat(\ndiff --git a/test/framework/src/main/java/org/elasticsearch/action/admin/indices/diskusage/AnalyzeIndexDiskUsageTestUtils.java b/test/framework/src/main/java/org/elasticsearch/action/admin/indices/diskusage/AnalyzeIndexDiskUsageTestUtils.java\nnew file mode 100644\nindex 0000000000000..5c433aeacc5ec\n--- /dev/null\n+++ b/test/framework/src/main/java/org/elasticsearch/action/admin/indices/diskusage/AnalyzeIndexDiskUsageTestUtils.java\n@@ -0,0 +1,40 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the \"Elastic License\n+ * 2.0\", the \"GNU Affero General Public License v3.0 only\", and the \"Server Side\n+ * Public License v 1\"; you may not use this file except in compliance with, at\n+ * your election, the \"Elastic License 2.0\", the \"GNU Affero General Public\n+ * License v3.0 only\", or the \"Server Side Public License, v 1\".\n+ */\n+\n+package org.elasticsearch.action.admin.indices.diskusage;\n+\n+import org.elasticsearch.core.Nullable;\n+\n+public class AnalyzeIndexDiskUsageTestUtils {\n+\n+    private AnalyzeIndexDiskUsageTestUtils() {}\n+\n+    @Nullable\n+    public static IndexDiskUsageStats getIndexStats(final AnalyzeIndexDiskUsageResponse diskUsageResponse, final String indexName) {\n+        var stats = diskUsageResponse.getStats();\n+        if (stats != null) {\n+            return stats.get(indexName);\n+        }\n+        return null;\n+    }\n+\n+    @Nullable\n+    public static IndexDiskUsageStats.PerFieldDiskUsage getPerFieldDiskUsage(\n+        final IndexDiskUsageStats indexDiskUsageStats,\n+        final String fieldName\n+    ) {\n+        if (indexDiskUsageStats != null) {\n+            var fields = indexDiskUsageStats.getFields();\n+            if (fields != null) {\n+                return fields.get(fieldName);\n+            }\n+        }\n+        return null;\n+    }\n+}\n",
  "additions": 1120,
  "deletions": 12,
  "changed_files": 16,
  "url": "https://github.com/elastic/elasticsearch/pull/136810",
  "mined_at": "2025-10-25T13:34:35.981895"
}