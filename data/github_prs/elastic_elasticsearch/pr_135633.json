{
  "id": 135633,
  "repository": "elastic/elasticsearch",
  "title": "ESQL: Fetch min transport version we're going to target when resolving fields",
  "body": "Relates https://github.com/elastic/elasticsearch/issues/131108\r\n\r\nFetch min transport version we're going to target when resolving fields.",
  "state": "closed",
  "merged": true,
  "merged_at": "2025-10-07T15:59:21+00:00",
  "created_at": "2025-09-29T14:32:09+00:00",
  "updated_at": "2025-10-24T10:01:40+00:00",
  "author": "nik9000",
  "reviewers": [
    "ivancea",
    "dnhatn",
    "idegtiarenko",
    "nik9000"
  ],
  "base_sha": "7cd76e133b486d3bb164b847a78c585c1daf1907",
  "head_sha": "9220454d12982761d7e8eacde9ef5176d25f68cc",
  "review_comments": [
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-09-29T14:42:45+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-09-29T15:43:58+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-09-29T15:44:14+00:00"
    },
    {
      "user": "dnhatn",
      "state": "APPROVED",
      "body": "I left one comment, but this looks good. Thank you for taking care of this, Nik!",
      "submitted_at": "2025-09-29T17:39:53+00:00"
    },
    {
      "user": "idegtiarenko",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-09-30T06:51:04+00:00"
    },
    {
      "user": "idegtiarenko",
      "state": "CHANGES_REQUESTED",
      "body": "I am not comfortable with copying such amount of code that is currently changing.\r\nLets try exploring alternative approach.",
      "submitted_at": "2025-09-30T06:51:41+00:00"
    },
    {
      "user": "dnhatn",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-09-30T14:56:29+00:00"
    },
    {
      "user": "ivancea",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-02T13:28:30+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-03T12:28:33+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-03T14:34:46+00:00"
    },
    {
      "user": "nik9000",
      "state": "COMMENTED",
      "body": "",
      "submitted_at": "2025-10-03T14:38:15+00:00"
    }
  ],
  "pr_comments": [
    {
      "user": "elasticsearchmachine",
      "body": "Pinging @elastic/es-analytical-engine (Team:Analytics)",
      "created_at": "2025-09-29T14:32:34+00:00"
    },
    {
      "user": "dnhatn",
      "body": "If you prefer to delay copying this code, we can add minTransportVersion to the existing field-caps response instead of the new resolve response.",
      "created_at": "2025-09-30T14:56:17+00:00"
    },
    {
      "user": "nik9000",
      "body": "> If you prefer to delay copying this code, we can add minTransportVersion to the existing field-caps response instead of the new resolve response.\r\n\r\nWe talked about that and it didn't feel worth it. Only ESQL wants it and we're worried folks will see it and think \"oh, this is how I get that version!\" But the request is expensive. I'll see if I can work through the copy and paste today.",
      "created_at": "2025-10-01T12:19:09+00:00"
    },
    {
      "user": "nik9000",
      "body": "@idegtiarenko and I've talked over slack and zoom and stuff. He's on holiday today so I'll summarize the conversation here.\r\n\r\nHe's convinced we need *something*, that does this, though not convinced this is the right way to do it. I'm more convinced than he is, but that's how conversations go. Either way, we can use this for now, and either keep using it for a long time (my guess) or modify it later (@idegtiarenko's guess). But, for now, this gives us something to fix a lot of the problems we have. So I'll try to get this in today or Monday.",
      "created_at": "2025-10-03T12:32:14+00:00"
    },
    {
      "user": "nik9000",
      "body": "I have an approval from Nhat and a passing build. I'll get this in. I think Github is confused because it's making me \"bypass rules\". Or maybe our rule configuration is confused. I dunno, but I think merging is good here.",
      "created_at": "2025-10-07T15:59:05+00:00"
    },
    {
      "user": "elasticsearchmachine",
      "body": "## üíî Backport failed\n| Status | Branch | Result |\n|:------:|:------:|:------:|\n| ‚ùå |  9.2  | Commit could not be cherrypicked due to conflicts |\n\nYou can use [sqren/backport](https://github.com/sqren/backport) to manually backport by running `backport --upstream elastic/elasticsearch --pr 135633`",
      "created_at": "2025-10-07T16:00:39+00:00"
    },
    {
      "user": "idegtiarenko",
      "body": "Was this backported?",
      "created_at": "2025-10-24T09:59:03+00:00"
    }
  ],
  "files_changed": [
    {
      "filename": "server/src/main/java/org/elasticsearch/action/fieldcaps/FieldCapabilitiesRequest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "patch": "@@ -288,7 +288,7 @@ public Map<String, Object> runtimeFields() {\n         return this.runtimeFields;\n     }\n \n-    Long nowInMillis() {\n+    public Long nowInMillis() {\n         return nowInMillis;\n     }\n "
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/action/fieldcaps/RequestDispatcher.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "patch": "@@ -54,7 +54,7 @@\n /**\n  * Dispatches child field-caps requests to old/new data nodes in the local cluster that have shards of the requesting indices.\n  */\n-final class RequestDispatcher {\n+public final class RequestDispatcher {\n     static final Logger LOGGER = LogManager.getLogger(RequestDispatcher.class);\n \n     private final TransportService transportService;\n@@ -74,7 +74,7 @@ final class RequestDispatcher {\n     private final AtomicInteger executionRound = new AtomicInteger();\n     private final Map<String, IndexSelector> indexSelectors;\n \n-    RequestDispatcher(\n+    public RequestDispatcher(\n         ClusterService clusterService,\n         TransportService transportService,\n         ProjectResolver projectResolver,\n@@ -127,7 +127,7 @@ final class RequestDispatcher {\n         }\n     }\n \n-    void execute() {\n+    public void execute() {\n         executor.execute(new AbstractRunnable() {\n             @Override\n             public void onFailure(Exception e) {"
    },
    {
      "filename": "server/src/main/java/org/elasticsearch/action/fieldcaps/TransportFieldCapabilitiesAction.java",
      "status": "modified",
      "additions": 18,
      "deletions": 47,
      "changes": 65,
      "patch": "@@ -129,40 +129,19 @@ public TransportFieldCapabilitiesAction(\n \n     @Override\n     protected void doExecute(Task task, FieldCapabilitiesRequest request, final ActionListener<FieldCapabilitiesResponse> listener) {\n-        executeRequest(\n-            task,\n-            request,\n-            (transportService, conn, fieldCapabilitiesRequest, responseHandler) -> transportService.sendRequest(\n-                conn,\n-                REMOTE_TYPE.name(),\n-                fieldCapabilitiesRequest,\n-                TransportRequestOptions.EMPTY,\n-                responseHandler\n-            ),\n-            listener\n-        );\n+        executeRequest(task, request, listener);\n     }\n \n-    public void executeRequest(\n-        Task task,\n-        FieldCapabilitiesRequest request,\n-        LinkedRequestExecutor linkedRequestExecutor,\n-        ActionListener<FieldCapabilitiesResponse> listener\n-    ) {\n+    public void executeRequest(Task task, FieldCapabilitiesRequest request, ActionListener<FieldCapabilitiesResponse> listener) {\n         // workaround for https://github.com/elastic/elasticsearch/issues/97916 - TODO remove this when we can\n-        searchCoordinationExecutor.execute(ActionRunnable.wrap(listener, l -> doExecuteForked(task, request, linkedRequestExecutor, l)));\n+        searchCoordinationExecutor.execute(ActionRunnable.wrap(listener, l -> doExecuteForked(task, request, l)));\n     }\n \n-    private void doExecuteForked(\n-        Task task,\n-        FieldCapabilitiesRequest request,\n-        LinkedRequestExecutor linkedRequestExecutor,\n-        ActionListener<FieldCapabilitiesResponse> listener\n-    ) {\n+    private void doExecuteForked(Task task, FieldCapabilitiesRequest request, ActionListener<FieldCapabilitiesResponse> listener) {\n         if (ccsCheckCompatibility) {\n             checkCCSVersionCompatibility(request);\n         }\n-        final Executor singleThreadedExecutor = buildSingleThreadedExecutor();\n+        final Executor singleThreadedExecutor = buildSingleThreadedExecutor(searchCoordinationExecutor, LOGGER);\n         assert task instanceof CancellableTask;\n         final CancellableTask fieldCapTask = (CancellableTask) task;\n         // retrieve the initial timestamp in case the action is a cross cluster search\n@@ -322,10 +301,11 @@ private void doExecuteForked(\n                         true,\n                         ActionListener.releaseAfter(remoteListener, refs.acquire())\n                     ).delegateFailure(\n-                        (responseListener, conn) -> linkedRequestExecutor.executeRemoteRequest(\n-                            transportService,\n+                        (responseListener, conn) -> transportService.sendRequest(\n                             conn,\n+                            REMOTE_TYPE.name(),\n                             remoteRequest,\n+                            TransportRequestOptions.EMPTY,\n                             new ActionListenerResponseHandler<>(responseListener, FieldCapabilitiesResponse::new, singleThreadedExecutor)\n                         )\n                     )\n@@ -339,7 +319,7 @@ private void doExecuteForked(\n         }\n     }\n \n-    private Executor buildSingleThreadedExecutor() {\n+    public static Executor buildSingleThreadedExecutor(Executor searchCoordinationExecutor, Logger logger) {\n         final ThrottledTaskRunner throttledTaskRunner = new ThrottledTaskRunner(\"field_caps\", 1, searchCoordinationExecutor);\n         return r -> throttledTaskRunner.enqueueTask(new ActionListener<>() {\n             @Override\n@@ -362,16 +342,7 @@ public void onFailure(Exception e) {\n         });\n     }\n \n-    public interface LinkedRequestExecutor {\n-        void executeRemoteRequest(\n-            TransportService transportService,\n-            Transport.Connection conn,\n-            FieldCapabilitiesRequest remoteRequest,\n-            ActionListenerResponseHandler<FieldCapabilitiesResponse> responseHandler\n-        );\n-    }\n-\n-    private static void checkIndexBlocks(ProjectState projectState, String[] concreteIndices) {\n+    public static void checkIndexBlocks(ProjectState projectState, String[] concreteIndices) {\n         var blocks = projectState.blocks();\n         var projectId = projectState.projectId();\n         if (blocks.global(projectId).isEmpty() && blocks.indices(projectId).isEmpty()) {\n@@ -421,7 +392,7 @@ private static void mergeIndexResponses(\n         }\n     }\n \n-    private static FieldCapabilitiesRequest prepareRemoteRequest(\n+    public static FieldCapabilitiesRequest prepareRemoteRequest(\n         String clusterAlias,\n         FieldCapabilitiesRequest request,\n         OriginalIndices originalIndices,\n@@ -615,10 +586,10 @@ private static void innerMerge(\n      * This collector can contain a failure for an index even if one of its shards was successful. When building the final\n      * list, these failures will be skipped because they have no affect on the final response.\n      */\n-    private static final class FailureCollector {\n+    public static final class FailureCollector {\n         private final Map<String, Exception> failuresByIndex = new HashMap<>();\n \n-        List<FieldCapabilitiesFailure> build(Set<String> successfulIndices) {\n+        public List<FieldCapabilitiesFailure> build(Set<String> successfulIndices) {\n             Map<Tuple<String, String>, FieldCapabilitiesFailure> indexFailures = new HashMap<>();\n             for (Map.Entry<String, Exception> failure : failuresByIndex.entrySet()) {\n                 String index = failure.getKey();\n@@ -647,15 +618,15 @@ List<FieldCapabilitiesFailure> build(Set<String> successfulIndices) {\n             return new ArrayList<>(indexFailures.values());\n         }\n \n-        void collect(String index, Exception e) {\n+        public void collect(String index, Exception e) {\n             failuresByIndex.putIfAbsent(index, e);\n         }\n \n-        void clear() {\n+        public void clear() {\n             failuresByIndex.clear();\n         }\n \n-        boolean isEmpty() {\n+        public boolean isEmpty() {\n             return failuresByIndex.isEmpty();\n         }\n     }\n@@ -718,8 +689,8 @@ public void messageReceived(FieldCapabilitiesNodeRequest request, TransportChann\n         }\n     }\n \n-    private static class ForkingOnFailureActionListener<Response> extends AbstractThreadedActionListener<Response> {\n-        ForkingOnFailureActionListener(Executor executor, boolean forceExecution, ActionListener<Response> delegate) {\n+    public static class ForkingOnFailureActionListener<Response> extends AbstractThreadedActionListener<Response> {\n+        public ForkingOnFailureActionListener(Executor executor, boolean forceExecution, ActionListener<Response> delegate) {\n             super(executor, forceExecution, delegate);\n         }\n "
    },
    {
      "filename": "server/src/main/resources/transport/definitions/referable/esql_resolve_fields_response_created.csv",
      "status": "added",
      "additions": 1,
      "deletions": 0,
      "changes": 1,
      "patch": "@@ -0,0 +1 @@\n+9189000,9185001"
    },
    {
      "filename": "server/src/main/resources/transport/upper_bounds/9.2.csv",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "patch": "@@ -1 +1 @@\n-initial_9.2.0,9185000\n+esql_resolve_fields_response_created,9185001"
    },
    {
      "filename": "server/src/main/resources/transport/upper_bounds/9.3.csv",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "patch": "@@ -1 +1 @@\n-timestamp_range_telemetry,9188000\n+esql_resolve_fields_response_created,9189000"
    },
    {
      "filename": "x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java",
      "status": "modified",
      "additions": 0,
      "deletions": 6,
      "changes": 6,
      "patch": "@@ -130,9 +130,6 @@ protected boolean fetchDenseVectorAggMetricDoubleIfFns() throws IOException {\n \n     protected boolean supportsNodeAssignment() throws IOException {\n         if (supportsNodeAssignment == null) {\n-            for (NodeInfo i : allNodeToInfo().values()) {\n-                logger.error(\"NOCOMMIT {}\", i);\n-            }\n             supportsNodeAssignment = allNodeToInfo().values()\n                 .stream()\n                 .allMatch(i -> (i.roles.contains(\"index\") && i.roles.contains(\"search\")) || (i.roles.contains(\"data\")));\n@@ -542,10 +539,8 @@ private boolean syntheticSourceByDefault() {\n     }\n \n     private Map<String, NodeInfo> expectedIndices() throws IOException {\n-        logger.error(\"ADFADF NOCOMMIT\");\n         Map<String, NodeInfo> result = new TreeMap<>();\n         if (supportsNodeAssignment()) {\n-            logger.error(\"supports {}\", allNodeToInfo());\n             for (Map.Entry<String, NodeInfo> e : allNodeToInfo().entrySet()) {\n                 String name = indexMode + \"_\" + e.getKey();\n                 if (e.getValue().cluster != null) {\n@@ -554,7 +549,6 @@ private Map<String, NodeInfo> expectedIndices() throws IOException {\n                 result.put(name, e.getValue());\n             }\n         } else {\n-            logger.error(\"one per {}\", allNodeToInfo());\n             for (Map.Entry<String, NodeInfo> e : allNodeToInfo().entrySet()) {\n                 String name = indexMode.toString();\n                 if (e.getValue().cluster != null) {"
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsAction.java",
      "status": "modified",
      "additions": 310,
      "deletions": 13,
      "changes": 323,
      "patch": "@@ -6,59 +6,356 @@\n  */\n package org.elasticsearch.xpack.esql.action;\n \n+import org.elasticsearch.ElasticsearchTimeoutException;\n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.ActionListenerResponseHandler;\n+import org.elasticsearch.action.ActionRunnable;\n import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.OriginalIndices;\n import org.elasticsearch.action.RemoteClusterActionType;\n+import org.elasticsearch.action.fieldcaps.FieldCapabilitiesFailure;\n+import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesRequest;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n+import org.elasticsearch.action.fieldcaps.IndexFieldCapabilities;\n+import org.elasticsearch.action.fieldcaps.RequestDispatcher;\n import org.elasticsearch.action.fieldcaps.TransportFieldCapabilitiesAction;\n import org.elasticsearch.action.support.ActionFilters;\n import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.RefCountingRunnable;\n+import org.elasticsearch.action.support.SubscribableListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ProjectState;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.project.ProjectResolver;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.core.TimeValue;\n+import org.elasticsearch.indices.IndicesService;\n import org.elasticsearch.injection.guice.Inject;\n+import org.elasticsearch.logging.LogManager;\n+import org.elasticsearch.logging.Logger;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.tasks.CancellableTask;\n import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.RemoteClusterAware;\n import org.elasticsearch.transport.Transport;\n import org.elasticsearch.transport.TransportRequestOptions;\n import org.elasticsearch.transport.TransportService;\n \n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.BiConsumer;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.TransportSearchHelper.checkCCSVersionCompatibility;\n+\n /**\n  * A fork of the field-caps API for ES|QL. This fork allows us to gradually introduce features and optimizations to this internal\n  * API without risking breaking the external field-caps API. For now, this API delegates to the field-caps API, but gradually,\n  * we will decouple this API completely from the field-caps.\n  */\n-public class EsqlResolveFieldsAction extends HandledTransportAction<FieldCapabilitiesRequest, FieldCapabilitiesResponse> {\n+public class EsqlResolveFieldsAction extends HandledTransportAction<FieldCapabilitiesRequest, EsqlResolveFieldsResponse> {\n     public static final String NAME = \"indices:data/read/esql/resolve_fields\";\n-    public static final ActionType<FieldCapabilitiesResponse> TYPE = new ActionType<>(NAME);\n+    public static final ActionType<EsqlResolveFieldsResponse> TYPE = new ActionType<>(NAME);\n     public static final RemoteClusterActionType<FieldCapabilitiesResponse> RESOLVE_REMOTE_TYPE = new RemoteClusterActionType<>(\n         NAME,\n         FieldCapabilitiesResponse::new\n     );\n+    private static final Logger LOGGER = LogManager.getLogger(EsqlResolveFieldsAction.class);\n \n-    private final TransportFieldCapabilitiesAction fieldCapsAction;\n+    private final Executor searchCoordinationExecutor;\n+    private final TransportService transportService;\n+    private final ClusterService clusterService;\n+    private final ProjectResolver projectResolver;\n+    private final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    private final IndicesService indicesService;\n+    private final boolean ccsCheckCompatibility;\n+    private final ThreadPool threadPool;\n+    private final TimeValue forceConnectTimeoutSecs;\n \n     @Inject\n     public EsqlResolveFieldsAction(\n         TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n         ActionFilters actionFilters,\n-        TransportFieldCapabilitiesAction fieldCapsAction\n+        IndicesService indicesService,\n+        ProjectResolver projectResolver,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n     ) {\n         // TODO replace DIRECT_EXECUTOR_SERVICE when removing workaround for https://github.com/elastic/elasticsearch/issues/97916\n         super(NAME, transportService, actionFilters, FieldCapabilitiesRequest::new, EsExecutors.DIRECT_EXECUTOR_SERVICE);\n-        this.fieldCapsAction = fieldCapsAction;\n+        this.searchCoordinationExecutor = threadPool.executor(ThreadPool.Names.SEARCH_COORDINATION);\n+        this.transportService = transportService;\n+        this.clusterService = clusterService;\n+        this.projectResolver = projectResolver;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.indicesService = indicesService;\n+        this.ccsCheckCompatibility = SearchService.CCS_VERSION_CHECK_SETTING.get(clusterService.getSettings());\n+        this.threadPool = threadPool;\n+        this.forceConnectTimeoutSecs = clusterService.getSettings().getAsTime(\"search.ccs.force_connect_timeout\", null);\n     }\n \n     @Override\n-    protected void doExecute(Task task, FieldCapabilitiesRequest request, final ActionListener<FieldCapabilitiesResponse> listener) {\n-        fieldCapsAction.executeRequest(task, request, this::executeLinkedRequest, listener);\n+    protected void doExecute(Task task, FieldCapabilitiesRequest request, final ActionListener<EsqlResolveFieldsResponse> listener) {\n+        executeRequest(task, request, listener);\n     }\n \n-    void executeLinkedRequest(\n-        TransportService transportService,\n-        Transport.Connection conn,\n-        FieldCapabilitiesRequest request,\n-        ActionListenerResponseHandler<FieldCapabilitiesResponse> responseHandler\n+    public void executeRequest(Task task, FieldCapabilitiesRequest request, ActionListener<EsqlResolveFieldsResponse> listener) {\n+        // workaround for https://github.com/elastic/elasticsearch/issues/97916 - TODO remove this when we can\n+        searchCoordinationExecutor.execute(ActionRunnable.wrap(listener, l -> doExecuteForked(task, request, l)));\n+    }\n+\n+    private void doExecuteForked(Task task, FieldCapabilitiesRequest request, ActionListener<EsqlResolveFieldsResponse> listener) {\n+        if (request.isMergeResults()) {\n+            throw new IllegalArgumentException(\"merging results not supported\");\n+        }\n+        if (ccsCheckCompatibility) {\n+            checkCCSVersionCompatibility(request);\n+        }\n+        final Executor singleThreadedExecutor = TransportFieldCapabilitiesAction.buildSingleThreadedExecutor(\n+            searchCoordinationExecutor,\n+            LOGGER\n+        );\n+        assert task instanceof CancellableTask;\n+        final CancellableTask fieldCapTask = (CancellableTask) task;\n+        // retrieve the initial timestamp in case the action is a cross cluster search\n+        long nowInMillis = request.nowInMillis() == null ? System.currentTimeMillis() : request.nowInMillis();\n+        ClusterState clusterState = clusterService.state();\n+        ProjectState projectState = projectResolver.getProjectState(clusterState);\n+        AtomicReference<TransportVersion> minTransportVersion = new AtomicReference<>(clusterState.getMinTransportVersion());\n+        final Map<String, OriginalIndices> remoteClusterIndices = transportService.getRemoteClusterService()\n+            .groupIndices(request.indicesOptions(), request.indices(), request.returnLocalAll());\n+        final OriginalIndices localIndices = remoteClusterIndices.remove(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY);\n+        final String[] concreteIndices;\n+        if (localIndices == null) {\n+            // in the case we have one or more remote indices but no local we don't expand to all local indices and just do remote indices\n+            concreteIndices = Strings.EMPTY_ARRAY;\n+        } else {\n+            concreteIndices = indexNameExpressionResolver.concreteIndexNames(projectState.metadata(), localIndices);\n+        }\n+\n+        if (concreteIndices.length == 0 && remoteClusterIndices.isEmpty()) {\n+            // No indices at all!\n+            listener.onResponse(\n+                new EsqlResolveFieldsResponse(\n+                    new FieldCapabilitiesResponse(new String[0], Collections.emptyMap()),\n+                    minTransportVersion.get()\n+                )\n+            );\n+            return;\n+        }\n+\n+        TransportFieldCapabilitiesAction.checkIndexBlocks(projectState, concreteIndices);\n+        final TransportFieldCapabilitiesAction.FailureCollector indexFailures = new TransportFieldCapabilitiesAction.FailureCollector();\n+        final Map<String, FieldCapabilitiesIndexResponse> indexResponses = new HashMap<>();\n+        // This map is used to share the index response for indices which have the same index mapping hash to reduce the memory usage.\n+        final Map<String, FieldCapabilitiesIndexResponse> indexMappingHashToResponses = new HashMap<>();\n+        final Runnable releaseResourcesOnCancel = () -> {\n+            LOGGER.trace(\"clear index responses on cancellation\");\n+            indexFailures.clear();\n+            indexResponses.clear();\n+            indexMappingHashToResponses.clear();\n+        };\n+        final Consumer<FieldCapabilitiesIndexResponse> handleIndexResponse = resp -> {\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+                return;\n+            }\n+            if (resp.canMatch() && resp.getIndexMappingHash() != null) {\n+                FieldCapabilitiesIndexResponse curr = indexMappingHashToResponses.putIfAbsent(resp.getIndexMappingHash(), resp);\n+                if (curr != null) {\n+                    resp = new FieldCapabilitiesIndexResponse(\n+                        resp.getIndexName(),\n+                        curr.getIndexMappingHash(),\n+                        curr.get(),\n+                        true,\n+                        curr.getIndexMode()\n+                    );\n+                }\n+            }\n+            if (request.includeEmptyFields()) {\n+                indexResponses.putIfAbsent(resp.getIndexName(), resp);\n+            } else {\n+                indexResponses.merge(resp.getIndexName(), resp, (a, b) -> {\n+                    if (a.get().equals(b.get())) {\n+                        return a;\n+                    }\n+                    Map<String, IndexFieldCapabilities> mergedCaps = new HashMap<>(a.get());\n+                    mergedCaps.putAll(b.get());\n+                    return new FieldCapabilitiesIndexResponse(\n+                        a.getIndexName(),\n+                        a.getIndexMappingHash(),\n+                        mergedCaps,\n+                        true,\n+                        a.getIndexMode()\n+                    );\n+                });\n+            }\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+            }\n+        };\n+        final BiConsumer<String, Exception> handleIndexFailure = (index, error) -> {\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+                return;\n+            }\n+            indexFailures.collect(index, error);\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+            }\n+        };\n+        final var finishedOrCancelled = new AtomicBoolean();\n+        fieldCapTask.addListener(() -> {\n+            if (finishedOrCancelled.compareAndSet(false, true)) {\n+                singleThreadedExecutor.execute(releaseResourcesOnCancel);\n+                LOGGER.trace(\"clear index responses on cancellation submitted\");\n+            }\n+        });\n+        try (RefCountingRunnable refs = new RefCountingRunnable(() -> {\n+            finishedOrCancelled.set(true);\n+            if (fieldCapTask.notifyIfCancelled(listener)) {\n+                releaseResourcesOnCancel.run();\n+            } else {\n+                finishHim(\n+                    indexResponses,\n+                    indexFailures,\n+                    listener.map(caps -> new EsqlResolveFieldsResponse(caps, minTransportVersion.get()))\n+                );\n+            }\n+        })) {\n+            // local cluster\n+            final RequestDispatcher requestDispatcher = new RequestDispatcher(\n+                clusterService,\n+                transportService,\n+                projectResolver,\n+                indicesService.getCoordinatorRewriteContextProvider(() -> nowInMillis),\n+                task,\n+                request,\n+                localIndices,\n+                nowInMillis,\n+                concreteIndices,\n+                singleThreadedExecutor,\n+                handleIndexResponse,\n+                handleIndexFailure,\n+                refs.acquire()::close\n+            );\n+            requestDispatcher.execute();\n+\n+            // this is the cross cluster part of this API - we force the other cluster to not merge the results but instead\n+            // send us back all individual index results.\n+            for (Map.Entry<String, OriginalIndices> remoteIndices : remoteClusterIndices.entrySet()) {\n+                String clusterAlias = remoteIndices.getKey();\n+                OriginalIndices originalIndices = remoteIndices.getValue();\n+                FieldCapabilitiesRequest remoteRequest = TransportFieldCapabilitiesAction.prepareRemoteRequest(\n+                    clusterAlias,\n+                    request,\n+                    originalIndices,\n+                    nowInMillis\n+                );\n+                ActionListener<EsqlResolveFieldsResponse> remoteListener = ActionListener.wrap(response -> {\n+                    for (FieldCapabilitiesIndexResponse resp : response.caps().getIndexResponses()) {\n+                        String indexName = RemoteClusterAware.buildRemoteIndexName(clusterAlias, resp.getIndexName());\n+                        handleIndexResponse.accept(\n+                            new FieldCapabilitiesIndexResponse(\n+                                indexName,\n+                                resp.getIndexMappingHash(),\n+                                resp.get(),\n+                                resp.canMatch(),\n+                                resp.getIndexMode()\n+                            )\n+                        );\n+                    }\n+                    for (FieldCapabilitiesFailure failure : response.caps().getFailures()) {\n+                        Exception ex = failure.getException();\n+                        for (String index : failure.getIndices()) {\n+                            handleIndexFailure.accept(RemoteClusterAware.buildRemoteIndexName(clusterAlias, index), ex);\n+                        }\n+                    }\n+                    minTransportVersion.accumulateAndGet(response.minTransportVersion(), (lhs, rhs) -> {\n+                        if (lhs == null || rhs == null) {\n+                            return null;\n+                        }\n+                        return TransportVersion.min(lhs, rhs);\n+                    });\n+                }, ex -> {\n+                    for (String index : originalIndices.indices()) {\n+                        handleIndexFailure.accept(RemoteClusterAware.buildRemoteIndexName(clusterAlias, index), ex);\n+                    }\n+                });\n+\n+                SubscribableListener<Transport.Connection> connectionListener = new SubscribableListener<>();\n+                if (forceConnectTimeoutSecs != null) {\n+                    connectionListener.addTimeout(forceConnectTimeoutSecs, threadPool, singleThreadedExecutor);\n+                }\n+\n+                connectionListener.addListener(\n+                    // The underlying transport service may call onFailure with a thread pool other than search_coordinator.\n+                    // This fork is a workaround to ensure that the merging of field-caps always occurs on the search_coordinator.\n+                    // TODO: remove this workaround after we fixed https://github.com/elastic/elasticsearch/issues/107439\n+                    new TransportFieldCapabilitiesAction.ForkingOnFailureActionListener<>(\n+                        singleThreadedExecutor,\n+                        true,\n+                        ActionListener.releaseAfter(remoteListener, refs.acquire())\n+                    ).delegateFailure(\n+                        (responseListener, conn) -> transportService.sendRequest(\n+                            conn,\n+                            RESOLVE_REMOTE_TYPE.name(),\n+                            remoteRequest,\n+                            TransportRequestOptions.EMPTY,\n+                            new ActionListenerResponseHandler<>(responseListener, EsqlResolveFieldsResponse::new, singleThreadedExecutor)\n+                        )\n+                    )\n+                );\n+\n+                boolean ensureConnected = forceConnectTimeoutSecs != null\n+                    || transportService.getRemoteClusterService().isSkipUnavailable(clusterAlias).orElse(true) == false;\n+                transportService.getRemoteClusterService()\n+                    .maybeEnsureConnectedAndGetConnection(clusterAlias, ensureConnected, connectionListener);\n+            }\n+        }\n+    }\n+\n+    private static void finishHim(\n+        Map<String, FieldCapabilitiesIndexResponse> indexResponses,\n+        TransportFieldCapabilitiesAction.FailureCollector indexFailures,\n+        ActionListener<FieldCapabilitiesResponse> listener\n     ) {\n-        transportService.sendRequest(conn, RESOLVE_REMOTE_TYPE.name(), request, TransportRequestOptions.EMPTY, responseHandler);\n+        List<FieldCapabilitiesFailure> failures = indexFailures.build(indexResponses.keySet());\n+        if (indexResponses.isEmpty() == false) {\n+            listener.onResponse(new FieldCapabilitiesResponse(new ArrayList<>(indexResponses.values()), failures));\n+        } else {\n+            // we have no responses at all, maybe because of errors\n+            if (indexFailures.isEmpty() == false) {\n+                /*\n+                 * Under no circumstances are we to pass timeout errors originating from SubscribableListener as top-level errors.\n+                 * Instead, they should always be passed through the response object, as part of \"failures\".\n+                 */\n+                if (failures.stream()\n+                    .anyMatch(\n+                        failure -> failure.getException() instanceof IllegalStateException ise\n+                            && ise.getCause() instanceof ElasticsearchTimeoutException\n+                    )) {\n+                    listener.onResponse(new FieldCapabilitiesResponse(Collections.emptyList(), failures));\n+                } else {\n+                    // throw back the first exception\n+                    listener.onFailure(failures.get(0).getException());\n+                }\n+            } else {\n+                listener.onResponse(new FieldCapabilitiesResponse(Collections.emptyList(), Collections.emptyList()));\n+            }\n+        }\n     }\n }"
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java",
      "status": "added",
      "additions": 68,
      "deletions": 0,
      "changes": 68,
      "patch": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+\n+package org.elasticsearch.xpack.esql.action;\n+\n+import org.elasticsearch.TransportVersion;\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.core.Nullable;\n+\n+import java.io.IOException;\n+\n+public class EsqlResolveFieldsResponse extends ActionResponse {\n+    private static final TransportVersion RESOLVE_FIELDS_RESPONSE_CREATED_TV = TransportVersion.fromName(\n+        \"esql_resolve_fields_response_created\"\n+    );\n+\n+    private final FieldCapabilitiesResponse caps;\n+    private final TransportVersion minTransportVersion;\n+\n+    public EsqlResolveFieldsResponse(FieldCapabilitiesResponse caps, TransportVersion minTransportVersion) {\n+        this.caps = caps;\n+        this.minTransportVersion = minTransportVersion;\n+    }\n+\n+    public EsqlResolveFieldsResponse(StreamInput in) throws IOException {\n+        caps = new FieldCapabilitiesResponse(in);\n+        if (in.getTransportVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) && in.readBoolean()) {\n+            minTransportVersion = TransportVersion.readVersion(in);\n+        } else {\n+            minTransportVersion = null;\n+        }\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        caps.writeTo(out);\n+        if (out.getTransportVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV)) {\n+            out.writeBoolean(minTransportVersion != null);\n+            if (minTransportVersion != null) {\n+                TransportVersion.writeVersion(minTransportVersion, out);\n+            }\n+        }\n+    }\n+\n+    public FieldCapabilitiesResponse caps() {\n+        return caps;\n+    }\n+\n+    /**\n+     * The minimum {@link TransportVersion} of all clusters against which we resolved\n+     * indices.\n+     * <p>\n+     *     If this is {@code null} then one of the nodes is before {@link #RESOLVE_FIELDS_RESPONSE_CREATED_TV} but\n+     *     we have no idea how early it is. Could be back in {@code 8.19.0}.\n+     * </p>\n+     */\n+    @Nullable\n+    public TransportVersion minTransportVersion() {\n+        return minTransportVersion;\n+    }\n+}"
    },
    {
      "filename": "x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java",
      "status": "modified",
      "additions": 10,
      "deletions": 5,
      "changes": 15,
      "patch": "@@ -18,6 +18,8 @@\n import org.elasticsearch.index.IndexMode;\n import org.elasticsearch.index.mapper.TimeSeriesParams;\n import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.logging.LogManager;\n+import org.elasticsearch.logging.Logger;\n import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsAction;\n import org.elasticsearch.xpack.esql.core.expression.MetadataAttribute;\n@@ -51,6 +53,8 @@\n import static org.elasticsearch.xpack.esql.core.type.DataType.UNSUPPORTED;\n \n public class IndexResolver {\n+    private static Logger LOGGER = LogManager.getLogger(IndexResolver.class);\n+\n     public static final Set<String> ALL_FIELDS = Set.of(\"*\");\n     public static final Set<String> INDEX_METADATA_FIELD = Set.of(MetadataAttribute.INDEX);\n     public static final String UNMAPPED = \"unmapped\";\n@@ -91,11 +95,12 @@ public void resolveAsMergedMapping(\n         client.execute(\n             EsqlResolveFieldsAction.TYPE,\n             createFieldCapsRequest(indexWildcard, fieldNames, requestFilter, includeAllDimensions),\n-            listener.delegateFailureAndWrap(\n-                (l, response) -> l.onResponse(\n-                    mergedMappings(indexWildcard, new FieldsInfo(response, supportsAggregateMetricDouble, supportsDenseVector))\n-                )\n-            )\n+            listener.delegateFailureAndWrap((l, response) -> {\n+                LOGGER.debug(\"minimum transport version {}\", response.minTransportVersion());\n+                l.onResponse(\n+                    mergedMappings(indexWildcard, new FieldsInfo(response.caps(), supportsAggregateMetricDouble, supportsDenseVector))\n+                );\n+            })\n         );\n     }\n "
    },
    {
      "filename": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/enrich/EnrichPolicyResolverTests.java",
      "status": "modified",
      "additions": 5,
      "deletions": 1,
      "changes": 6,
      "patch": "@@ -42,6 +42,7 @@\n import org.elasticsearch.xpack.core.enrich.EnrichMetadata;\n import org.elasticsearch.xpack.core.enrich.EnrichPolicy;\n import org.elasticsearch.xpack.esql.action.EsqlExecutionInfo;\n+import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse;\n import org.elasticsearch.xpack.esql.analysis.EnrichResolution;\n import org.elasticsearch.xpack.esql.plan.logical.Enrich;\n import org.elasticsearch.xpack.esql.session.IndexResolver;\n@@ -509,7 +510,10 @@ protected <Request extends ActionRequest, Response extends ActionResponse> void\n             } else {\n                 response = new FieldCapabilitiesResponse(List.of(), List.of());\n             }\n-            threadPool().executor(ThreadPool.Names.SEARCH_COORDINATION).execute(ActionRunnable.supply(listener, () -> (Response) response));\n+            threadPool().executor(ThreadPool.Names.SEARCH_COORDINATION)\n+                .execute(\n+                    ActionRunnable.supply(listener, () -> (Response) new EsqlResolveFieldsResponse(response, TransportVersion.current()))\n+                );\n         }\n     }\n }"
    },
    {
      "filename": "x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/telemetry/PlanExecutorMetricsTests.java",
      "status": "modified",
      "additions": 11,
      "deletions": 4,
      "changes": 15,
      "patch": "@@ -7,6 +7,7 @@\n \n package org.elasticsearch.xpack.esql.telemetry;\n \n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.OriginalIndices;\n import org.elasticsearch.action.fieldcaps.FieldCapabilities;\n@@ -33,6 +34,7 @@\n import org.elasticsearch.xpack.esql.action.EsqlExecutionInfo;\n import org.elasticsearch.xpack.esql.action.EsqlQueryRequest;\n import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsAction;\n+import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse;\n import org.elasticsearch.xpack.esql.analysis.EnrichResolution;\n import org.elasticsearch.xpack.esql.core.expression.FoldContext;\n import org.elasticsearch.xpack.esql.enrich.EnrichPolicyResolver;\n@@ -134,19 +136,24 @@ public void testFailedMetric() {\n         when(fieldCapabilitiesResponse.get()).thenReturn(fields(indices));\n         doAnswer((Answer<Void>) invocation -> {\n             @SuppressWarnings(\"unchecked\")\n-            ActionListener<FieldCapabilitiesResponse> listener = (ActionListener<FieldCapabilitiesResponse>) invocation.getArguments()[2];\n+            ActionListener<EsqlResolveFieldsResponse> listener = (ActionListener<EsqlResolveFieldsResponse>) invocation.getArguments()[2];\n             // simulate a valid field_caps response so we can parse and correctly analyze de query\n-            listener.onResponse(fieldCapabilitiesResponse);\n+            listener.onResponse(new EsqlResolveFieldsResponse(fieldCapabilitiesResponse, TransportVersion.current()));\n             return null;\n         }).when(qlClient).execute(eq(EsqlResolveFieldsAction.TYPE), any(), any());\n \n         Client esqlClient = mock(Client.class);\n         IndexResolver indexResolver = new IndexResolver(esqlClient);\n         doAnswer((Answer<Void>) invocation -> {\n             @SuppressWarnings(\"unchecked\")\n-            ActionListener<FieldCapabilitiesResponse> listener = (ActionListener<FieldCapabilitiesResponse>) invocation.getArguments()[2];\n+            ActionListener<EsqlResolveFieldsResponse> listener = (ActionListener<EsqlResolveFieldsResponse>) invocation.getArguments()[2];\n             // simulate a valid field_caps response so we can parse and correctly analyze de query\n-            listener.onResponse(new FieldCapabilitiesResponse(indexFieldCapabilities(indices), List.of()));\n+            listener.onResponse(\n+                new EsqlResolveFieldsResponse(\n+                    new FieldCapabilitiesResponse(indexFieldCapabilities(indices), List.of()),\n+                    TransportVersion.current()\n+                )\n+            );\n             return null;\n         }).when(esqlClient).execute(eq(EsqlResolveFieldsAction.TYPE), any(), any());\n "
    }
  ],
  "diff": "diff --git a/server/src/main/java/org/elasticsearch/action/fieldcaps/FieldCapabilitiesRequest.java b/server/src/main/java/org/elasticsearch/action/fieldcaps/FieldCapabilitiesRequest.java\nindex c4c974ad6b668..4ed4e24110cd9 100644\n--- a/server/src/main/java/org/elasticsearch/action/fieldcaps/FieldCapabilitiesRequest.java\n+++ b/server/src/main/java/org/elasticsearch/action/fieldcaps/FieldCapabilitiesRequest.java\n@@ -288,7 +288,7 @@ public Map<String, Object> runtimeFields() {\n         return this.runtimeFields;\n     }\n \n-    Long nowInMillis() {\n+    public Long nowInMillis() {\n         return nowInMillis;\n     }\n \ndiff --git a/server/src/main/java/org/elasticsearch/action/fieldcaps/RequestDispatcher.java b/server/src/main/java/org/elasticsearch/action/fieldcaps/RequestDispatcher.java\nindex c56fd985c9e2b..3ad45526be217 100644\n--- a/server/src/main/java/org/elasticsearch/action/fieldcaps/RequestDispatcher.java\n+++ b/server/src/main/java/org/elasticsearch/action/fieldcaps/RequestDispatcher.java\n@@ -54,7 +54,7 @@\n /**\n  * Dispatches child field-caps requests to old/new data nodes in the local cluster that have shards of the requesting indices.\n  */\n-final class RequestDispatcher {\n+public final class RequestDispatcher {\n     static final Logger LOGGER = LogManager.getLogger(RequestDispatcher.class);\n \n     private final TransportService transportService;\n@@ -74,7 +74,7 @@ final class RequestDispatcher {\n     private final AtomicInteger executionRound = new AtomicInteger();\n     private final Map<String, IndexSelector> indexSelectors;\n \n-    RequestDispatcher(\n+    public RequestDispatcher(\n         ClusterService clusterService,\n         TransportService transportService,\n         ProjectResolver projectResolver,\n@@ -127,7 +127,7 @@ final class RequestDispatcher {\n         }\n     }\n \n-    void execute() {\n+    public void execute() {\n         executor.execute(new AbstractRunnable() {\n             @Override\n             public void onFailure(Exception e) {\ndiff --git a/server/src/main/java/org/elasticsearch/action/fieldcaps/TransportFieldCapabilitiesAction.java b/server/src/main/java/org/elasticsearch/action/fieldcaps/TransportFieldCapabilitiesAction.java\nindex 74201f106f7d6..91c5c1a212dd9 100644\n--- a/server/src/main/java/org/elasticsearch/action/fieldcaps/TransportFieldCapabilitiesAction.java\n+++ b/server/src/main/java/org/elasticsearch/action/fieldcaps/TransportFieldCapabilitiesAction.java\n@@ -129,40 +129,19 @@ public TransportFieldCapabilitiesAction(\n \n     @Override\n     protected void doExecute(Task task, FieldCapabilitiesRequest request, final ActionListener<FieldCapabilitiesResponse> listener) {\n-        executeRequest(\n-            task,\n-            request,\n-            (transportService, conn, fieldCapabilitiesRequest, responseHandler) -> transportService.sendRequest(\n-                conn,\n-                REMOTE_TYPE.name(),\n-                fieldCapabilitiesRequest,\n-                TransportRequestOptions.EMPTY,\n-                responseHandler\n-            ),\n-            listener\n-        );\n+        executeRequest(task, request, listener);\n     }\n \n-    public void executeRequest(\n-        Task task,\n-        FieldCapabilitiesRequest request,\n-        LinkedRequestExecutor linkedRequestExecutor,\n-        ActionListener<FieldCapabilitiesResponse> listener\n-    ) {\n+    public void executeRequest(Task task, FieldCapabilitiesRequest request, ActionListener<FieldCapabilitiesResponse> listener) {\n         // workaround for https://github.com/elastic/elasticsearch/issues/97916 - TODO remove this when we can\n-        searchCoordinationExecutor.execute(ActionRunnable.wrap(listener, l -> doExecuteForked(task, request, linkedRequestExecutor, l)));\n+        searchCoordinationExecutor.execute(ActionRunnable.wrap(listener, l -> doExecuteForked(task, request, l)));\n     }\n \n-    private void doExecuteForked(\n-        Task task,\n-        FieldCapabilitiesRequest request,\n-        LinkedRequestExecutor linkedRequestExecutor,\n-        ActionListener<FieldCapabilitiesResponse> listener\n-    ) {\n+    private void doExecuteForked(Task task, FieldCapabilitiesRequest request, ActionListener<FieldCapabilitiesResponse> listener) {\n         if (ccsCheckCompatibility) {\n             checkCCSVersionCompatibility(request);\n         }\n-        final Executor singleThreadedExecutor = buildSingleThreadedExecutor();\n+        final Executor singleThreadedExecutor = buildSingleThreadedExecutor(searchCoordinationExecutor, LOGGER);\n         assert task instanceof CancellableTask;\n         final CancellableTask fieldCapTask = (CancellableTask) task;\n         // retrieve the initial timestamp in case the action is a cross cluster search\n@@ -322,10 +301,11 @@ private void doExecuteForked(\n                         true,\n                         ActionListener.releaseAfter(remoteListener, refs.acquire())\n                     ).delegateFailure(\n-                        (responseListener, conn) -> linkedRequestExecutor.executeRemoteRequest(\n-                            transportService,\n+                        (responseListener, conn) -> transportService.sendRequest(\n                             conn,\n+                            REMOTE_TYPE.name(),\n                             remoteRequest,\n+                            TransportRequestOptions.EMPTY,\n                             new ActionListenerResponseHandler<>(responseListener, FieldCapabilitiesResponse::new, singleThreadedExecutor)\n                         )\n                     )\n@@ -339,7 +319,7 @@ private void doExecuteForked(\n         }\n     }\n \n-    private Executor buildSingleThreadedExecutor() {\n+    public static Executor buildSingleThreadedExecutor(Executor searchCoordinationExecutor, Logger logger) {\n         final ThrottledTaskRunner throttledTaskRunner = new ThrottledTaskRunner(\"field_caps\", 1, searchCoordinationExecutor);\n         return r -> throttledTaskRunner.enqueueTask(new ActionListener<>() {\n             @Override\n@@ -362,16 +342,7 @@ public void onFailure(Exception e) {\n         });\n     }\n \n-    public interface LinkedRequestExecutor {\n-        void executeRemoteRequest(\n-            TransportService transportService,\n-            Transport.Connection conn,\n-            FieldCapabilitiesRequest remoteRequest,\n-            ActionListenerResponseHandler<FieldCapabilitiesResponse> responseHandler\n-        );\n-    }\n-\n-    private static void checkIndexBlocks(ProjectState projectState, String[] concreteIndices) {\n+    public static void checkIndexBlocks(ProjectState projectState, String[] concreteIndices) {\n         var blocks = projectState.blocks();\n         var projectId = projectState.projectId();\n         if (blocks.global(projectId).isEmpty() && blocks.indices(projectId).isEmpty()) {\n@@ -421,7 +392,7 @@ private static void mergeIndexResponses(\n         }\n     }\n \n-    private static FieldCapabilitiesRequest prepareRemoteRequest(\n+    public static FieldCapabilitiesRequest prepareRemoteRequest(\n         String clusterAlias,\n         FieldCapabilitiesRequest request,\n         OriginalIndices originalIndices,\n@@ -615,10 +586,10 @@ private static void innerMerge(\n      * This collector can contain a failure for an index even if one of its shards was successful. When building the final\n      * list, these failures will be skipped because they have no affect on the final response.\n      */\n-    private static final class FailureCollector {\n+    public static final class FailureCollector {\n         private final Map<String, Exception> failuresByIndex = new HashMap<>();\n \n-        List<FieldCapabilitiesFailure> build(Set<String> successfulIndices) {\n+        public List<FieldCapabilitiesFailure> build(Set<String> successfulIndices) {\n             Map<Tuple<String, String>, FieldCapabilitiesFailure> indexFailures = new HashMap<>();\n             for (Map.Entry<String, Exception> failure : failuresByIndex.entrySet()) {\n                 String index = failure.getKey();\n@@ -647,15 +618,15 @@ List<FieldCapabilitiesFailure> build(Set<String> successfulIndices) {\n             return new ArrayList<>(indexFailures.values());\n         }\n \n-        void collect(String index, Exception e) {\n+        public void collect(String index, Exception e) {\n             failuresByIndex.putIfAbsent(index, e);\n         }\n \n-        void clear() {\n+        public void clear() {\n             failuresByIndex.clear();\n         }\n \n-        boolean isEmpty() {\n+        public boolean isEmpty() {\n             return failuresByIndex.isEmpty();\n         }\n     }\n@@ -718,8 +689,8 @@ public void messageReceived(FieldCapabilitiesNodeRequest request, TransportChann\n         }\n     }\n \n-    private static class ForkingOnFailureActionListener<Response> extends AbstractThreadedActionListener<Response> {\n-        ForkingOnFailureActionListener(Executor executor, boolean forceExecution, ActionListener<Response> delegate) {\n+    public static class ForkingOnFailureActionListener<Response> extends AbstractThreadedActionListener<Response> {\n+        public ForkingOnFailureActionListener(Executor executor, boolean forceExecution, ActionListener<Response> delegate) {\n             super(executor, forceExecution, delegate);\n         }\n \ndiff --git a/server/src/main/resources/transport/definitions/referable/esql_resolve_fields_response_created.csv b/server/src/main/resources/transport/definitions/referable/esql_resolve_fields_response_created.csv\nnew file mode 100644\nindex 0000000000000..eb3f0c4f01615\n--- /dev/null\n+++ b/server/src/main/resources/transport/definitions/referable/esql_resolve_fields_response_created.csv\n@@ -0,0 +1 @@\n+9189000,9185001\ndiff --git a/server/src/main/resources/transport/upper_bounds/9.2.csv b/server/src/main/resources/transport/upper_bounds/9.2.csv\nindex 2147eab66c207..341a3050b4a97 100644\n--- a/server/src/main/resources/transport/upper_bounds/9.2.csv\n+++ b/server/src/main/resources/transport/upper_bounds/9.2.csv\n@@ -1 +1 @@\n-initial_9.2.0,9185000\n+esql_resolve_fields_response_created,9185001\ndiff --git a/server/src/main/resources/transport/upper_bounds/9.3.csv b/server/src/main/resources/transport/upper_bounds/9.3.csv\nindex fe8be2e1f55a8..b947ec1f1d1ce 100644\n--- a/server/src/main/resources/transport/upper_bounds/9.3.csv\n+++ b/server/src/main/resources/transport/upper_bounds/9.3.csv\n@@ -1 +1 @@\n-timestamp_range_telemetry,9188000\n+esql_resolve_fields_response_created,9189000\ndiff --git a/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java b/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java\nindex d8596e6943bbc..1e77a62711990 100644\n--- a/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java\n+++ b/x-pack/plugin/esql/qa/server/src/main/java/org/elasticsearch/xpack/esql/qa/rest/AllSupportedFieldsTestCase.java\n@@ -130,9 +130,6 @@ protected boolean fetchDenseVectorAggMetricDoubleIfFns() throws IOException {\n \n     protected boolean supportsNodeAssignment() throws IOException {\n         if (supportsNodeAssignment == null) {\n-            for (NodeInfo i : allNodeToInfo().values()) {\n-                logger.error(\"NOCOMMIT {}\", i);\n-            }\n             supportsNodeAssignment = allNodeToInfo().values()\n                 .stream()\n                 .allMatch(i -> (i.roles.contains(\"index\") && i.roles.contains(\"search\")) || (i.roles.contains(\"data\")));\n@@ -542,10 +539,8 @@ private boolean syntheticSourceByDefault() {\n     }\n \n     private Map<String, NodeInfo> expectedIndices() throws IOException {\n-        logger.error(\"ADFADF NOCOMMIT\");\n         Map<String, NodeInfo> result = new TreeMap<>();\n         if (supportsNodeAssignment()) {\n-            logger.error(\"supports {}\", allNodeToInfo());\n             for (Map.Entry<String, NodeInfo> e : allNodeToInfo().entrySet()) {\n                 String name = indexMode + \"_\" + e.getKey();\n                 if (e.getValue().cluster != null) {\n@@ -554,7 +549,6 @@ private Map<String, NodeInfo> expectedIndices() throws IOException {\n                 result.put(name, e.getValue());\n             }\n         } else {\n-            logger.error(\"one per {}\", allNodeToInfo());\n             for (Map.Entry<String, NodeInfo> e : allNodeToInfo().entrySet()) {\n                 String name = indexMode.toString();\n                 if (e.getValue().cluster != null) {\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsAction.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsAction.java\nindex ff137a024b5bf..47574a6530837 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsAction.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsAction.java\n@@ -6,59 +6,356 @@\n  */\n package org.elasticsearch.xpack.esql.action;\n \n+import org.elasticsearch.ElasticsearchTimeoutException;\n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.ActionListenerResponseHandler;\n+import org.elasticsearch.action.ActionRunnable;\n import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.OriginalIndices;\n import org.elasticsearch.action.RemoteClusterActionType;\n+import org.elasticsearch.action.fieldcaps.FieldCapabilitiesFailure;\n+import org.elasticsearch.action.fieldcaps.FieldCapabilitiesIndexResponse;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesRequest;\n import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n+import org.elasticsearch.action.fieldcaps.IndexFieldCapabilities;\n+import org.elasticsearch.action.fieldcaps.RequestDispatcher;\n import org.elasticsearch.action.fieldcaps.TransportFieldCapabilitiesAction;\n import org.elasticsearch.action.support.ActionFilters;\n import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.RefCountingRunnable;\n+import org.elasticsearch.action.support.SubscribableListener;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ProjectState;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.project.ProjectResolver;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.util.concurrent.EsExecutors;\n+import org.elasticsearch.core.TimeValue;\n+import org.elasticsearch.indices.IndicesService;\n import org.elasticsearch.injection.guice.Inject;\n+import org.elasticsearch.logging.LogManager;\n+import org.elasticsearch.logging.Logger;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.tasks.CancellableTask;\n import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.RemoteClusterAware;\n import org.elasticsearch.transport.Transport;\n import org.elasticsearch.transport.TransportRequestOptions;\n import org.elasticsearch.transport.TransportService;\n \n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.BiConsumer;\n+import java.util.function.Consumer;\n+\n+import static org.elasticsearch.action.search.TransportSearchHelper.checkCCSVersionCompatibility;\n+\n /**\n  * A fork of the field-caps API for ES|QL. This fork allows us to gradually introduce features and optimizations to this internal\n  * API without risking breaking the external field-caps API. For now, this API delegates to the field-caps API, but gradually,\n  * we will decouple this API completely from the field-caps.\n  */\n-public class EsqlResolveFieldsAction extends HandledTransportAction<FieldCapabilitiesRequest, FieldCapabilitiesResponse> {\n+public class EsqlResolveFieldsAction extends HandledTransportAction<FieldCapabilitiesRequest, EsqlResolveFieldsResponse> {\n     public static final String NAME = \"indices:data/read/esql/resolve_fields\";\n-    public static final ActionType<FieldCapabilitiesResponse> TYPE = new ActionType<>(NAME);\n+    public static final ActionType<EsqlResolveFieldsResponse> TYPE = new ActionType<>(NAME);\n     public static final RemoteClusterActionType<FieldCapabilitiesResponse> RESOLVE_REMOTE_TYPE = new RemoteClusterActionType<>(\n         NAME,\n         FieldCapabilitiesResponse::new\n     );\n+    private static final Logger LOGGER = LogManager.getLogger(EsqlResolveFieldsAction.class);\n \n-    private final TransportFieldCapabilitiesAction fieldCapsAction;\n+    private final Executor searchCoordinationExecutor;\n+    private final TransportService transportService;\n+    private final ClusterService clusterService;\n+    private final ProjectResolver projectResolver;\n+    private final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    private final IndicesService indicesService;\n+    private final boolean ccsCheckCompatibility;\n+    private final ThreadPool threadPool;\n+    private final TimeValue forceConnectTimeoutSecs;\n \n     @Inject\n     public EsqlResolveFieldsAction(\n         TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n         ActionFilters actionFilters,\n-        TransportFieldCapabilitiesAction fieldCapsAction\n+        IndicesService indicesService,\n+        ProjectResolver projectResolver,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n     ) {\n         // TODO replace DIRECT_EXECUTOR_SERVICE when removing workaround for https://github.com/elastic/elasticsearch/issues/97916\n         super(NAME, transportService, actionFilters, FieldCapabilitiesRequest::new, EsExecutors.DIRECT_EXECUTOR_SERVICE);\n-        this.fieldCapsAction = fieldCapsAction;\n+        this.searchCoordinationExecutor = threadPool.executor(ThreadPool.Names.SEARCH_COORDINATION);\n+        this.transportService = transportService;\n+        this.clusterService = clusterService;\n+        this.projectResolver = projectResolver;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.indicesService = indicesService;\n+        this.ccsCheckCompatibility = SearchService.CCS_VERSION_CHECK_SETTING.get(clusterService.getSettings());\n+        this.threadPool = threadPool;\n+        this.forceConnectTimeoutSecs = clusterService.getSettings().getAsTime(\"search.ccs.force_connect_timeout\", null);\n     }\n \n     @Override\n-    protected void doExecute(Task task, FieldCapabilitiesRequest request, final ActionListener<FieldCapabilitiesResponse> listener) {\n-        fieldCapsAction.executeRequest(task, request, this::executeLinkedRequest, listener);\n+    protected void doExecute(Task task, FieldCapabilitiesRequest request, final ActionListener<EsqlResolveFieldsResponse> listener) {\n+        executeRequest(task, request, listener);\n     }\n \n-    void executeLinkedRequest(\n-        TransportService transportService,\n-        Transport.Connection conn,\n-        FieldCapabilitiesRequest request,\n-        ActionListenerResponseHandler<FieldCapabilitiesResponse> responseHandler\n+    public void executeRequest(Task task, FieldCapabilitiesRequest request, ActionListener<EsqlResolveFieldsResponse> listener) {\n+        // workaround for https://github.com/elastic/elasticsearch/issues/97916 - TODO remove this when we can\n+        searchCoordinationExecutor.execute(ActionRunnable.wrap(listener, l -> doExecuteForked(task, request, l)));\n+    }\n+\n+    private void doExecuteForked(Task task, FieldCapabilitiesRequest request, ActionListener<EsqlResolveFieldsResponse> listener) {\n+        if (request.isMergeResults()) {\n+            throw new IllegalArgumentException(\"merging results not supported\");\n+        }\n+        if (ccsCheckCompatibility) {\n+            checkCCSVersionCompatibility(request);\n+        }\n+        final Executor singleThreadedExecutor = TransportFieldCapabilitiesAction.buildSingleThreadedExecutor(\n+            searchCoordinationExecutor,\n+            LOGGER\n+        );\n+        assert task instanceof CancellableTask;\n+        final CancellableTask fieldCapTask = (CancellableTask) task;\n+        // retrieve the initial timestamp in case the action is a cross cluster search\n+        long nowInMillis = request.nowInMillis() == null ? System.currentTimeMillis() : request.nowInMillis();\n+        ClusterState clusterState = clusterService.state();\n+        ProjectState projectState = projectResolver.getProjectState(clusterState);\n+        AtomicReference<TransportVersion> minTransportVersion = new AtomicReference<>(clusterState.getMinTransportVersion());\n+        final Map<String, OriginalIndices> remoteClusterIndices = transportService.getRemoteClusterService()\n+            .groupIndices(request.indicesOptions(), request.indices(), request.returnLocalAll());\n+        final OriginalIndices localIndices = remoteClusterIndices.remove(RemoteClusterAware.LOCAL_CLUSTER_GROUP_KEY);\n+        final String[] concreteIndices;\n+        if (localIndices == null) {\n+            // in the case we have one or more remote indices but no local we don't expand to all local indices and just do remote indices\n+            concreteIndices = Strings.EMPTY_ARRAY;\n+        } else {\n+            concreteIndices = indexNameExpressionResolver.concreteIndexNames(projectState.metadata(), localIndices);\n+        }\n+\n+        if (concreteIndices.length == 0 && remoteClusterIndices.isEmpty()) {\n+            // No indices at all!\n+            listener.onResponse(\n+                new EsqlResolveFieldsResponse(\n+                    new FieldCapabilitiesResponse(new String[0], Collections.emptyMap()),\n+                    minTransportVersion.get()\n+                )\n+            );\n+            return;\n+        }\n+\n+        TransportFieldCapabilitiesAction.checkIndexBlocks(projectState, concreteIndices);\n+        final TransportFieldCapabilitiesAction.FailureCollector indexFailures = new TransportFieldCapabilitiesAction.FailureCollector();\n+        final Map<String, FieldCapabilitiesIndexResponse> indexResponses = new HashMap<>();\n+        // This map is used to share the index response for indices which have the same index mapping hash to reduce the memory usage.\n+        final Map<String, FieldCapabilitiesIndexResponse> indexMappingHashToResponses = new HashMap<>();\n+        final Runnable releaseResourcesOnCancel = () -> {\n+            LOGGER.trace(\"clear index responses on cancellation\");\n+            indexFailures.clear();\n+            indexResponses.clear();\n+            indexMappingHashToResponses.clear();\n+        };\n+        final Consumer<FieldCapabilitiesIndexResponse> handleIndexResponse = resp -> {\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+                return;\n+            }\n+            if (resp.canMatch() && resp.getIndexMappingHash() != null) {\n+                FieldCapabilitiesIndexResponse curr = indexMappingHashToResponses.putIfAbsent(resp.getIndexMappingHash(), resp);\n+                if (curr != null) {\n+                    resp = new FieldCapabilitiesIndexResponse(\n+                        resp.getIndexName(),\n+                        curr.getIndexMappingHash(),\n+                        curr.get(),\n+                        true,\n+                        curr.getIndexMode()\n+                    );\n+                }\n+            }\n+            if (request.includeEmptyFields()) {\n+                indexResponses.putIfAbsent(resp.getIndexName(), resp);\n+            } else {\n+                indexResponses.merge(resp.getIndexName(), resp, (a, b) -> {\n+                    if (a.get().equals(b.get())) {\n+                        return a;\n+                    }\n+                    Map<String, IndexFieldCapabilities> mergedCaps = new HashMap<>(a.get());\n+                    mergedCaps.putAll(b.get());\n+                    return new FieldCapabilitiesIndexResponse(\n+                        a.getIndexName(),\n+                        a.getIndexMappingHash(),\n+                        mergedCaps,\n+                        true,\n+                        a.getIndexMode()\n+                    );\n+                });\n+            }\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+            }\n+        };\n+        final BiConsumer<String, Exception> handleIndexFailure = (index, error) -> {\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+                return;\n+            }\n+            indexFailures.collect(index, error);\n+            if (fieldCapTask.isCancelled()) {\n+                releaseResourcesOnCancel.run();\n+            }\n+        };\n+        final var finishedOrCancelled = new AtomicBoolean();\n+        fieldCapTask.addListener(() -> {\n+            if (finishedOrCancelled.compareAndSet(false, true)) {\n+                singleThreadedExecutor.execute(releaseResourcesOnCancel);\n+                LOGGER.trace(\"clear index responses on cancellation submitted\");\n+            }\n+        });\n+        try (RefCountingRunnable refs = new RefCountingRunnable(() -> {\n+            finishedOrCancelled.set(true);\n+            if (fieldCapTask.notifyIfCancelled(listener)) {\n+                releaseResourcesOnCancel.run();\n+            } else {\n+                finishHim(\n+                    indexResponses,\n+                    indexFailures,\n+                    listener.map(caps -> new EsqlResolveFieldsResponse(caps, minTransportVersion.get()))\n+                );\n+            }\n+        })) {\n+            // local cluster\n+            final RequestDispatcher requestDispatcher = new RequestDispatcher(\n+                clusterService,\n+                transportService,\n+                projectResolver,\n+                indicesService.getCoordinatorRewriteContextProvider(() -> nowInMillis),\n+                task,\n+                request,\n+                localIndices,\n+                nowInMillis,\n+                concreteIndices,\n+                singleThreadedExecutor,\n+                handleIndexResponse,\n+                handleIndexFailure,\n+                refs.acquire()::close\n+            );\n+            requestDispatcher.execute();\n+\n+            // this is the cross cluster part of this API - we force the other cluster to not merge the results but instead\n+            // send us back all individual index results.\n+            for (Map.Entry<String, OriginalIndices> remoteIndices : remoteClusterIndices.entrySet()) {\n+                String clusterAlias = remoteIndices.getKey();\n+                OriginalIndices originalIndices = remoteIndices.getValue();\n+                FieldCapabilitiesRequest remoteRequest = TransportFieldCapabilitiesAction.prepareRemoteRequest(\n+                    clusterAlias,\n+                    request,\n+                    originalIndices,\n+                    nowInMillis\n+                );\n+                ActionListener<EsqlResolveFieldsResponse> remoteListener = ActionListener.wrap(response -> {\n+                    for (FieldCapabilitiesIndexResponse resp : response.caps().getIndexResponses()) {\n+                        String indexName = RemoteClusterAware.buildRemoteIndexName(clusterAlias, resp.getIndexName());\n+                        handleIndexResponse.accept(\n+                            new FieldCapabilitiesIndexResponse(\n+                                indexName,\n+                                resp.getIndexMappingHash(),\n+                                resp.get(),\n+                                resp.canMatch(),\n+                                resp.getIndexMode()\n+                            )\n+                        );\n+                    }\n+                    for (FieldCapabilitiesFailure failure : response.caps().getFailures()) {\n+                        Exception ex = failure.getException();\n+                        for (String index : failure.getIndices()) {\n+                            handleIndexFailure.accept(RemoteClusterAware.buildRemoteIndexName(clusterAlias, index), ex);\n+                        }\n+                    }\n+                    minTransportVersion.accumulateAndGet(response.minTransportVersion(), (lhs, rhs) -> {\n+                        if (lhs == null || rhs == null) {\n+                            return null;\n+                        }\n+                        return TransportVersion.min(lhs, rhs);\n+                    });\n+                }, ex -> {\n+                    for (String index : originalIndices.indices()) {\n+                        handleIndexFailure.accept(RemoteClusterAware.buildRemoteIndexName(clusterAlias, index), ex);\n+                    }\n+                });\n+\n+                SubscribableListener<Transport.Connection> connectionListener = new SubscribableListener<>();\n+                if (forceConnectTimeoutSecs != null) {\n+                    connectionListener.addTimeout(forceConnectTimeoutSecs, threadPool, singleThreadedExecutor);\n+                }\n+\n+                connectionListener.addListener(\n+                    // The underlying transport service may call onFailure with a thread pool other than search_coordinator.\n+                    // This fork is a workaround to ensure that the merging of field-caps always occurs on the search_coordinator.\n+                    // TODO: remove this workaround after we fixed https://github.com/elastic/elasticsearch/issues/107439\n+                    new TransportFieldCapabilitiesAction.ForkingOnFailureActionListener<>(\n+                        singleThreadedExecutor,\n+                        true,\n+                        ActionListener.releaseAfter(remoteListener, refs.acquire())\n+                    ).delegateFailure(\n+                        (responseListener, conn) -> transportService.sendRequest(\n+                            conn,\n+                            RESOLVE_REMOTE_TYPE.name(),\n+                            remoteRequest,\n+                            TransportRequestOptions.EMPTY,\n+                            new ActionListenerResponseHandler<>(responseListener, EsqlResolveFieldsResponse::new, singleThreadedExecutor)\n+                        )\n+                    )\n+                );\n+\n+                boolean ensureConnected = forceConnectTimeoutSecs != null\n+                    || transportService.getRemoteClusterService().isSkipUnavailable(clusterAlias).orElse(true) == false;\n+                transportService.getRemoteClusterService()\n+                    .maybeEnsureConnectedAndGetConnection(clusterAlias, ensureConnected, connectionListener);\n+            }\n+        }\n+    }\n+\n+    private static void finishHim(\n+        Map<String, FieldCapabilitiesIndexResponse> indexResponses,\n+        TransportFieldCapabilitiesAction.FailureCollector indexFailures,\n+        ActionListener<FieldCapabilitiesResponse> listener\n     ) {\n-        transportService.sendRequest(conn, RESOLVE_REMOTE_TYPE.name(), request, TransportRequestOptions.EMPTY, responseHandler);\n+        List<FieldCapabilitiesFailure> failures = indexFailures.build(indexResponses.keySet());\n+        if (indexResponses.isEmpty() == false) {\n+            listener.onResponse(new FieldCapabilitiesResponse(new ArrayList<>(indexResponses.values()), failures));\n+        } else {\n+            // we have no responses at all, maybe because of errors\n+            if (indexFailures.isEmpty() == false) {\n+                /*\n+                 * Under no circumstances are we to pass timeout errors originating from SubscribableListener as top-level errors.\n+                 * Instead, they should always be passed through the response object, as part of \"failures\".\n+                 */\n+                if (failures.stream()\n+                    .anyMatch(\n+                        failure -> failure.getException() instanceof IllegalStateException ise\n+                            && ise.getCause() instanceof ElasticsearchTimeoutException\n+                    )) {\n+                    listener.onResponse(new FieldCapabilitiesResponse(Collections.emptyList(), failures));\n+                } else {\n+                    // throw back the first exception\n+                    listener.onFailure(failures.get(0).getException());\n+                }\n+            } else {\n+                listener.onResponse(new FieldCapabilitiesResponse(Collections.emptyList(), Collections.emptyList()));\n+            }\n+        }\n     }\n }\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java\nnew file mode 100644\nindex 0000000000000..365b2b976e2f1\n--- /dev/null\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/action/EsqlResolveFieldsResponse.java\n@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+\n+package org.elasticsearch.xpack.esql.action;\n+\n+import org.elasticsearch.TransportVersion;\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.action.fieldcaps.FieldCapabilitiesResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.core.Nullable;\n+\n+import java.io.IOException;\n+\n+public class EsqlResolveFieldsResponse extends ActionResponse {\n+    private static final TransportVersion RESOLVE_FIELDS_RESPONSE_CREATED_TV = TransportVersion.fromName(\n+        \"esql_resolve_fields_response_created\"\n+    );\n+\n+    private final FieldCapabilitiesResponse caps;\n+    private final TransportVersion minTransportVersion;\n+\n+    public EsqlResolveFieldsResponse(FieldCapabilitiesResponse caps, TransportVersion minTransportVersion) {\n+        this.caps = caps;\n+        this.minTransportVersion = minTransportVersion;\n+    }\n+\n+    public EsqlResolveFieldsResponse(StreamInput in) throws IOException {\n+        caps = new FieldCapabilitiesResponse(in);\n+        if (in.getTransportVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV) && in.readBoolean()) {\n+            minTransportVersion = TransportVersion.readVersion(in);\n+        } else {\n+            minTransportVersion = null;\n+        }\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        caps.writeTo(out);\n+        if (out.getTransportVersion().supports(RESOLVE_FIELDS_RESPONSE_CREATED_TV)) {\n+            out.writeBoolean(minTransportVersion != null);\n+            if (minTransportVersion != null) {\n+                TransportVersion.writeVersion(minTransportVersion, out);\n+            }\n+        }\n+    }\n+\n+    public FieldCapabilitiesResponse caps() {\n+        return caps;\n+    }\n+\n+    /**\n+     * The minimum {@link TransportVersion} of all clusters against which we resolved\n+     * indices.\n+     * <p>\n+     *     If this is {@code null} then one of the nodes is before {@link #RESOLVE_FIELDS_RESPONSE_CREATED_TV} but\n+     *     we have no idea how early it is. Could be back in {@code 8.19.0}.\n+     * </p>\n+     */\n+    @Nullable\n+    public TransportVersion minTransportVersion() {\n+        return minTransportVersion;\n+    }\n+}\ndiff --git a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java\nindex ff042c5e7d870..5cbc380d6d164 100644\n--- a/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java\n+++ b/x-pack/plugin/esql/src/main/java/org/elasticsearch/xpack/esql/session/IndexResolver.java\n@@ -18,6 +18,8 @@\n import org.elasticsearch.index.IndexMode;\n import org.elasticsearch.index.mapper.TimeSeriesParams;\n import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.logging.LogManager;\n+import org.elasticsearch.logging.Logger;\n import org.elasticsearch.threadpool.ThreadPool;\n import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsAction;\n import org.elasticsearch.xpack.esql.core.expression.MetadataAttribute;\n@@ -51,6 +53,8 @@\n import static org.elasticsearch.xpack.esql.core.type.DataType.UNSUPPORTED;\n \n public class IndexResolver {\n+    private static Logger LOGGER = LogManager.getLogger(IndexResolver.class);\n+\n     public static final Set<String> ALL_FIELDS = Set.of(\"*\");\n     public static final Set<String> INDEX_METADATA_FIELD = Set.of(MetadataAttribute.INDEX);\n     public static final String UNMAPPED = \"unmapped\";\n@@ -91,11 +95,12 @@ public void resolveAsMergedMapping(\n         client.execute(\n             EsqlResolveFieldsAction.TYPE,\n             createFieldCapsRequest(indexWildcard, fieldNames, requestFilter, includeAllDimensions),\n-            listener.delegateFailureAndWrap(\n-                (l, response) -> l.onResponse(\n-                    mergedMappings(indexWildcard, new FieldsInfo(response, supportsAggregateMetricDouble, supportsDenseVector))\n-                )\n-            )\n+            listener.delegateFailureAndWrap((l, response) -> {\n+                LOGGER.debug(\"minimum transport version {}\", response.minTransportVersion());\n+                l.onResponse(\n+                    mergedMappings(indexWildcard, new FieldsInfo(response.caps(), supportsAggregateMetricDouble, supportsDenseVector))\n+                );\n+            })\n         );\n     }\n \ndiff --git a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/enrich/EnrichPolicyResolverTests.java b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/enrich/EnrichPolicyResolverTests.java\nindex a0c5c99f82a60..9cb735b955d09 100644\n--- a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/enrich/EnrichPolicyResolverTests.java\n+++ b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/enrich/EnrichPolicyResolverTests.java\n@@ -42,6 +42,7 @@\n import org.elasticsearch.xpack.core.enrich.EnrichMetadata;\n import org.elasticsearch.xpack.core.enrich.EnrichPolicy;\n import org.elasticsearch.xpack.esql.action.EsqlExecutionInfo;\n+import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse;\n import org.elasticsearch.xpack.esql.analysis.EnrichResolution;\n import org.elasticsearch.xpack.esql.plan.logical.Enrich;\n import org.elasticsearch.xpack.esql.session.IndexResolver;\n@@ -509,7 +510,10 @@ protected <Request extends ActionRequest, Response extends ActionResponse> void\n             } else {\n                 response = new FieldCapabilitiesResponse(List.of(), List.of());\n             }\n-            threadPool().executor(ThreadPool.Names.SEARCH_COORDINATION).execute(ActionRunnable.supply(listener, () -> (Response) response));\n+            threadPool().executor(ThreadPool.Names.SEARCH_COORDINATION)\n+                .execute(\n+                    ActionRunnable.supply(listener, () -> (Response) new EsqlResolveFieldsResponse(response, TransportVersion.current()))\n+                );\n         }\n     }\n }\ndiff --git a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/telemetry/PlanExecutorMetricsTests.java b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/telemetry/PlanExecutorMetricsTests.java\nindex c24ae185c5f25..7a2565aaad9ce 100644\n--- a/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/telemetry/PlanExecutorMetricsTests.java\n+++ b/x-pack/plugin/esql/src/test/java/org/elasticsearch/xpack/esql/telemetry/PlanExecutorMetricsTests.java\n@@ -7,6 +7,7 @@\n \n package org.elasticsearch.xpack.esql.telemetry;\n \n+import org.elasticsearch.TransportVersion;\n import org.elasticsearch.action.ActionListener;\n import org.elasticsearch.action.OriginalIndices;\n import org.elasticsearch.action.fieldcaps.FieldCapabilities;\n@@ -33,6 +34,7 @@\n import org.elasticsearch.xpack.esql.action.EsqlExecutionInfo;\n import org.elasticsearch.xpack.esql.action.EsqlQueryRequest;\n import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsAction;\n+import org.elasticsearch.xpack.esql.action.EsqlResolveFieldsResponse;\n import org.elasticsearch.xpack.esql.analysis.EnrichResolution;\n import org.elasticsearch.xpack.esql.core.expression.FoldContext;\n import org.elasticsearch.xpack.esql.enrich.EnrichPolicyResolver;\n@@ -134,9 +136,9 @@ public void testFailedMetric() {\n         when(fieldCapabilitiesResponse.get()).thenReturn(fields(indices));\n         doAnswer((Answer<Void>) invocation -> {\n             @SuppressWarnings(\"unchecked\")\n-            ActionListener<FieldCapabilitiesResponse> listener = (ActionListener<FieldCapabilitiesResponse>) invocation.getArguments()[2];\n+            ActionListener<EsqlResolveFieldsResponse> listener = (ActionListener<EsqlResolveFieldsResponse>) invocation.getArguments()[2];\n             // simulate a valid field_caps response so we can parse and correctly analyze de query\n-            listener.onResponse(fieldCapabilitiesResponse);\n+            listener.onResponse(new EsqlResolveFieldsResponse(fieldCapabilitiesResponse, TransportVersion.current()));\n             return null;\n         }).when(qlClient).execute(eq(EsqlResolveFieldsAction.TYPE), any(), any());\n \n@@ -144,9 +146,14 @@ public void testFailedMetric() {\n         IndexResolver indexResolver = new IndexResolver(esqlClient);\n         doAnswer((Answer<Void>) invocation -> {\n             @SuppressWarnings(\"unchecked\")\n-            ActionListener<FieldCapabilitiesResponse> listener = (ActionListener<FieldCapabilitiesResponse>) invocation.getArguments()[2];\n+            ActionListener<EsqlResolveFieldsResponse> listener = (ActionListener<EsqlResolveFieldsResponse>) invocation.getArguments()[2];\n             // simulate a valid field_caps response so we can parse and correctly analyze de query\n-            listener.onResponse(new FieldCapabilitiesResponse(indexFieldCapabilities(indices), List.of()));\n+            listener.onResponse(\n+                new EsqlResolveFieldsResponse(\n+                    new FieldCapabilitiesResponse(indexFieldCapabilities(indices), List.of()),\n+                    TransportVersion.current()\n+                )\n+            );\n             return null;\n         }).when(esqlClient).execute(eq(EsqlResolveFieldsAction.TYPE), any(), any());\n \n",
  "additions": 429,
  "deletions": 82,
  "changed_files": 12,
  "url": "https://github.com/elastic/elasticsearch/pull/135633",
  "mined_at": "2025-10-25T13:32:08.011367"
}