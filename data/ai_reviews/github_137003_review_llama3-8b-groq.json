{
  "source_item_id": 137003,
  "source_type": "github",
  "generator_model_id": "llama3-8b-groq",
  "generation_timestamp": "2025-10-26T13:36:43.440408",
  "input_code_language": "java",
  "input_code_snippet": "/*\n * Copyright Elasticsearch B.V., and/or licensed to Elasticsearch B.V.\n * under one or more license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright\n * ownership. Elasticsearch B.V. licenses this file to you under\n * the Apache License, Version 2.0 (the \"License\"); you may\n * not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\n * This file is based on a modification of https://github.com/open-telemetry/opentelemetry-java which is licensed under the Apache 2.0 License.\n */\n\npackage org.elasticsearch.exponentialhistogram;\n\nimport org.apache.lucene.util.BitUtil;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.RamUsageEstimator;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStream;\n\nimport static org.elasticsearch.exponentialhistogram.ExponentialHistogram.MAX_SCALE;\nimport static org.elasticsearch.exponentialhistogram.ExponentialHistogram.MIN_SCALE;\n\n/**\n * Encodes the data of an exponential histogram in a compact format as byte array.\n * Note that some data of exponential histograms is stored outside of this array as separate doc values for better compression,\n * e.g. the zero threshold. This data is therefore not part of this encoding.\n **/\nfinal class CompressedHistogramData {\n\n    /*\n     The encoding has the following format:\n        - 1 byte: scale + flags\n                  scale has a range of less than 64, so we can use the two remaining bits for flags\n                  Currently the only flag is HAS_NEGATIVE_BUCKETS_FLAG, which indicates that there are negative buckets\n                  If this flag is not set, we can skip encoding the length of the negative buckets and therefore save a\n                  bit of space for this typical case.\n        - VInt: (optional) length of encoded negative buckets in bytes, if HAS_NEGATIVE_BUCKETS_FLAG is set\n        - byte[]: encoded negative buckets, details on the encoding below\n        - byte[]: encoded positive buckets, details on the encoding below\n     There is no end marker for the encoded buckets, therefore the total length of the encoded data needs to be known when decoding\n\n     The following scheme is used to encode the negative and positive buckets:\n        - if there are no buckets, the result is an empty array (byte[0])\n        - write the index of the first bucket as ZigZag-VLong\n        - write the count of the first bucket as ZigZag-VLong\n        - for each remaining (non-empty) bucket:\n           - if there was no empty bucket right before this bucket (the index of the bucket is exactly previousBucketIndex+1),\n             write the count for the bucket as ZigZag-VLong\n           - Otherwise there is at least one empty bucket between this one and the previous one.\n             We compute the number of empty buckets as n=currentBucketIndex-previousIndex-1 and then write -n out as\n             ZigZag-VLong followed by the count for the bucket as ZigZag-VLong. The negation is performed to allow to\n             distinguish whether a value represents a bucket count (positive number) or the number of empty buckets (negative number)\n             when decoding.\n\n     While this encoding is designed for sparse histograms, it compresses well for dense histograms too.\n     For fully dense histograms it effectively results in encoding the index of the first bucket, followed by just an array of counts.\n     For sparse histograms it corresponds to an interleaved encoding of the bucket indices with delta compression and the bucket counts.\n     Even mostly sparse histograms that have some dense regions profit from this encoding.\n     */\n\n    static final long SHALLOW_SIZE = RamUsageEstimator.shallowSizeOfInstance(CompressedHistogramData.class);\n\n    private static final int SCALE_OFFSET = 11;\n    private static final int HAS_NEGATIVE_BUCKETS_FLAG = 1 << 6; // = 64\n    private static final int SCALE_MASK = 0x3F; // = 63\n    static {\n        // protection against changes to MIN_SCALE and MAX_SCALE messing with our encoding\n        assert MIN_SCALE + SCALE_OFFSET >= 0;\n        assert MAX_SCALE + SCALE_OFFSET <= SCALE_MASK;\n    }\n\n    private int scale;\n\n    private byte[] encodedData;\n    private int negativeBucketsStart;\n    private int negativeBucketsLength;\n    private int positiveBucketsLength;\n\n    void decode(BytesRef data) {\n        this.encodedData = data.bytes;\n        AccessibleByteArrayStreamInput input = new AccessibleByteArrayStreamInput(data.bytes, data.offset, data.length);\n\n        int scaleWithFlags = input.read();\n        this.scale = (scaleWithFlags & SCALE_MASK) - SCALE_OFFSET;\n        boolean hasNegativeBuckets = (scaleWithFlags & HAS_NEGATIVE_BUCKETS_FLAG) != 0;\n\n        negativeBucketsLength = 0;\n        if (hasNegativeBuckets) {\n            negativeBucketsLength = (int) input.readVLong();\n        }\n\n        negativeBucketsStart = input.getPosition();\n        input.skip(negativeBucketsLength);\n        positiveBucketsLength = input.available();\n    }\n\n    static void write(OutputStream output, int scale, BucketIterator negativeBuckets, BucketIterator positiveBuckets) throws IOException {\n        assert scale >= MIN_SCALE && scale <= MAX_SCALE : \"scale must be in range [\" + MIN_SCALE + \", \" + MAX_SCALE + \"]\";\n        boolean hasNegativeBuckets = negativeBuckets.hasNext();\n        int scaleWithFlags = (scale + SCALE_OFFSET);\n        assert scaleWithFlags >= 0 && scaleWithFlags <= SCALE_MASK;\n        if (hasNegativeBuckets) {\n            scaleWithFlags |= HAS_NEGATIVE_BUCKETS_FLAG;\n        }\n        output.write((byte) scaleWithFlags);\n        if (hasNegativeBuckets) {\n            ByteArrayOutputStream temp = new ByteArrayOutputStream();\n            BucketsDecoder.serializeBuckets(temp, negativeBuckets);\n            byte[] data = temp.toByteArray();\n            writeVLong(data.length, output);\n            output.write(data);\n        }\n        BucketsDecoder.serializeBuckets(output, positiveBuckets);\n    }\n\n    int scale() {\n        return scale;\n    }\n\n    BucketsDecoder negativeBucketsDecoder() {\n        return new BucketsDecoder(negativeBucketsStart, negativeBucketsLength);\n    }\n\n    BucketsDecoder positiveBucketsDecoder() {\n        return new BucketsDecoder(negativeBucketsStart + negativeBucketsLength, positiveBucketsLength);\n    }\n\n    final class BucketsDecoder {\n\n        private long currentIndex;\n        /**\n         * The count for the bucket this iterator is currently pointing at.\n         * A value of {@code -1} is used to represent that the end has been reached.\n         */\n        private long currentCount;\n\n        private final AccessibleByteArrayStreamInput bucketsStreamInput;\n\n        private BucketsDecoder(int encodedBucketsStartOffset, int length) {\n            if (length > 0) {\n                bucketsStreamInput = new AccessibleByteArrayStreamInput(encodedData, encodedBucketsStartOffset, length);\n                currentIndex = bucketsStreamInput.readZLong() - 1;\n                currentCount = 0;\n                advance();\n            } else {\n                bucketsStreamInput = null;\n                // no data means we are iterating over an empty set of buckets\n                markEndReached();\n            }\n        }\n\n        private BucketsDecoder(BucketsDecoder toCopy) {\n            if (toCopy.bucketsStreamInput != null) {\n                int position = toCopy.bucketsStreamInput.getPosition();\n                bucketsStreamInput = new AccessibleByteArrayStreamInput(\n               ",
  "raw_llm_output": "{\n  \"bugs_found\": [\n      {\n         \"explanation\": \"The decode method does not handle the case where the input data is null.\",\n         \"suggested_fix\": \"if (data == null) { throw new NullPointerException(\\\"Input data cannot be null\\\"); }\"\n      },\n      {\n         \"explanation\": \"The write method does not check if the output stream is null before writing to it.\",\n         \"suggested_fix\": \"if (output == null) { throw new NullPointerException(\\\"Output stream cannot be null\\\"); }\"\n      },\n      {\n         \"explanation\": \"The BucketsDecoder class does not handle the case where the buckets stream input is null.\",\n         \"suggested_fix\": \"if (bucketsStreamInput == null) { throw new NullPointerException(\\\"Buckets stream input cannot be null\\\"); }\"\n      },\n      {\n         \"explanation\": \"The BucketsDecoder class does not handle the case where the current index is out of bounds.\",\n         \"suggested_fix\": \"if (currentIndex < 0) { throw new IndexOutOfBoundsException(\\\"Current index is out of bounds\\\"); }\"\n      },\n      {\n         \"explanation\": \"The BucketsDecoder class does not handle the case where the current count is out of bounds.\",\n         \"suggested_fix\": \"if (currentCount < -1 || currentCount > Integer.MAX_VALUE) { throw new IndexOutOfBoundsException(\\\"Current count is out of bounds\\\"); }\"\n      }\n   ],\n   \"no_bugs_reasoning\": \"\"\n}",
  "parsed_llm_output": {
    "bugs_found": [
      {
        "explanation": "The decode method does not handle the case where the input data is null.",
        "suggested_fix": "if (data == null) { throw new NullPointerException(\"Input data cannot be null\"); }"
      },
      {
        "explanation": "The write method does not check if the output stream is null before writing to it.",
        "suggested_fix": "if (output == null) { throw new NullPointerException(\"Output stream cannot be null\"); }"
      },
      {
        "explanation": "The BucketsDecoder class does not handle the case where the buckets stream input is null.",
        "suggested_fix": "if (bucketsStreamInput == null) { throw new NullPointerException(\"Buckets stream input cannot be null\"); }"
      },
      {
        "explanation": "The BucketsDecoder class does not handle the case where the current index is out of bounds.",
        "suggested_fix": "if (currentIndex < 0) { throw new IndexOutOfBoundsException(\"Current index is out of bounds\"); }"
      },
      {
        "explanation": "The BucketsDecoder class does not handle the case where the current count is out of bounds.",
        "suggested_fix": "if (currentCount < -1 || currentCount > Integer.MAX_VALUE) { throw new IndexOutOfBoundsException(\"Current count is out of bounds\"); }"
      }
    ],
    "no_bugs_reasoning": ""
  },
  "error": null
}