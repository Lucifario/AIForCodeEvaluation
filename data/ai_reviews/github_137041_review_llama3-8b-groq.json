{
  "source_item_id": 137041,
  "source_type": "github",
  "generator_model_id": "llama3-8b-groq",
  "generation_timestamp": "2025-10-26T13:33:41.484368",
  "input_code_language": "java",
  "input_code_snippet": "/*\n * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n * or more contributor license agreements. Licensed under the Elastic License\n * 2.0; you may not use this file except in compliance with the Elastic License\n * 2.0.\n */\n\npackage org.elasticsearch.xpack.esql.optimizer.rules.logical;\n\nimport org.elasticsearch.xpack.esql.optimizer.LogicalOptimizerContext;\nimport org.elasticsearch.xpack.esql.plan.logical.CardinalityPreserving;\nimport org.elasticsearch.xpack.esql.plan.logical.Enrich;\nimport org.elasticsearch.xpack.esql.plan.logical.ExecutesOn;\nimport org.elasticsearch.xpack.esql.plan.logical.Limit;\nimport org.elasticsearch.xpack.esql.plan.logical.LogicalPlan;\nimport org.elasticsearch.xpack.esql.plan.logical.PipelineBreaker;\n\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.IdentityHashMap;\nimport java.util.Set;\n\n/**\n * Locate any LIMIT that is \"visible\" under remote ENRICH, and make a copy of it above the ENRICH. The original limit is marked as local.\n * This allows the correct semantics of the remote application of limit. Enrich itself does not change the cardinality,\n * but the limit needs to be taken twice, locally on the node and again on the coordinator.\n * This runs only on LogicalPlanOptimizer not on local one.\n */\npublic final class HoistRemoteEnrichLimit extends OptimizerRules.ParameterizedOptimizerRule<Enrich, LogicalOptimizerContext>\n    implements\n        OptimizerRules.CoordinatorOnly {\n\n    public HoistRemoteEnrichLimit() {\n        super(OptimizerRules.TransformDirection.UP);\n    }\n\n    @Override\n    protected LogicalPlan rule(Enrich en, LogicalOptimizerContext ctx) {\n        if (en.mode() == Enrich.Mode.REMOTE) {\n            // Since limits are combinable, we will just assemble the set of candidates and create one combined lowest limit above the\n            // Enrich.\n            Set<Limit> seenLimits = Collections.newSetFromMap(new IdentityHashMap<>());\n            en.child().forEachDownMayReturnEarly((p, stop) -> {\n                if (p instanceof Limit l && l.local() == false) {\n                    // Local limits can be ignored here as they are always duplicates that have another limit upstairs\n                    seenLimits.add(l);\n                    return;\n                }\n                if ((p instanceof CardinalityPreserving) == false // can change the number of rows, so we can't just pull a limit from\n                                                                  // under it\n                    // this will fail the verifier anyway, so no need to continue\n                    || (p instanceof ExecutesOn ex && ex.executesOn() == ExecutesOn.ExecuteLocation.COORDINATOR)\n                // This is essentially another remote enrich - let it take care of its own limits\n                    || (p instanceof Enrich e && e.mode() == Enrich.Mode.REMOTE)\n                // If it's a pipeline breaker, this part will be on the coordinator anyway, which likely will fail the verifier\n                // In any case, duplicating anything below there is pointless.\n                    || p instanceof PipelineBreaker) {\n                    stop.set(true);\n                }\n            });\n\n            if (seenLimits.isEmpty()) {\n                return en;\n            }\n            // Mark original limits as local\n            LogicalPlan transformLimits = en.transformDown(Limit.class, l -> seenLimits.contains(l) ? l.withLocal(true) : l);\n            // Shouldn't actually throw because we checked seenLimits is not empty\n            Limit lowestLimit = seenLimits.stream().min(Comparator.comparing(l -> (int) l.limit().fold(ctx.foldCtx()))).orElseThrow();\n            // Insert new lowest limit on top of the Enrich, and mark it as duplicated since we don't want it to be pushed down\n            return new Limit(lowestLimit.source(), lowestLimit.limit(), transformLimits, true, false);\n        }\n        return en;\n    }\n}\n",
  "raw_llm_output": "{\n  \"bugs_found\": [\n      {\n         \"explanation\": \"The rule does not handle the case where there are multiple remote enriches in the pipeline. It only considers the first one it encounters.\",\n         \"suggested_fix\": \"Use a recursive approach to handle multiple remote enriches, or use a data structure to keep track of the remote enriches encountered so far.\"\n      },\n      {\n         \"explanation\": \"The rule does not handle the case where a limit is duplicated, i.e., there are multiple limits with the same value. It will only consider the first one it encounters.\",\n         \"suggested_fix\": \"Use a data structure to keep track of the unique limits encountered so far, and only consider the minimum limit value.\"\n      },\n      {\n         \"explanation\": \"The rule does not handle the case where a pipeline breaker is present in the pipeline. It will not consider the limits below the pipeline breaker.\",\n         \"suggested_fix\": \"Add a check to handle pipeline breakers, and consider the limits below them if they are not on the coordinator.\"\n      },\n      {\n         \"explanation\": \"The rule does not handle the case where a limit is not combinable, i.e., it is not a remote enrich. It will not consider the limits below it.\",\n         \"suggested_fix\": \"Add a check to handle non-combinable limits, and consider the limits below them if they are combinable.\"\n      },\n      {\n         \"explanation\": \"The rule does not handle the case where the coordinator is not the correct location for the limit. It will not consider the limits below the executes on coordinator.\",\n         \"suggested_fix\": \"Add a check to handle the coordinator location, and consider the limits below it if it is the correct location.\"\n      }\n   ],\n   \"no_bugs_reasoning\": \"\"\n}",
  "parsed_llm_output": {
    "bugs_found": [
      {
        "explanation": "The rule does not handle the case where there are multiple remote enriches in the pipeline. It only considers the first one it encounters.",
        "suggested_fix": "Use a recursive approach to handle multiple remote enriches, or use a data structure to keep track of the remote enriches encountered so far."
      },
      {
        "explanation": "The rule does not handle the case where a limit is duplicated, i.e., there are multiple limits with the same value. It will only consider the first one it encounters.",
        "suggested_fix": "Use a data structure to keep track of the unique limits encountered so far, and only consider the minimum limit value."
      },
      {
        "explanation": "The rule does not handle the case where a pipeline breaker is present in the pipeline. It will not consider the limits below the pipeline breaker.",
        "suggested_fix": "Add a check to handle pipeline breakers, and consider the limits below them if they are not on the coordinator."
      },
      {
        "explanation": "The rule does not handle the case where a limit is not combinable, i.e., it is not a remote enrich. It will not consider the limits below it.",
        "suggested_fix": "Add a check to handle non-combinable limits, and consider the limits below them if they are combinable."
      },
      {
        "explanation": "The rule does not handle the case where the coordinator is not the correct location for the limit. It will not consider the limits below the executes on coordinator.",
        "suggested_fix": "Add a check to handle the coordinator location, and consider the limits below it if it is the correct location."
      }
    ],
    "no_bugs_reasoning": ""
  },
  "error": null
}